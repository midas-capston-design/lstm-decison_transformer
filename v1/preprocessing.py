#!/usr/bin/env python3
"""
ì§€ìê¸° ê¸°ë°˜ ì‹¤ë‚´ ìœ„ì¹˜ ì¶”ì • - ë°ì´í„° ì „ì²˜ë¦¬
"""
import pandas as pd
import numpy as np
import os
from pathlib import Path
import pickle
from tqdm import tqdm
from collections import defaultdict

# ì„¤ì •
WINDOW_SIZE = 100  # ìƒ˜í”Œ
GRID_SIZE = 0.45   # m
SENSOR_COLS = ['MagX', 'MagY', 'MagZ', 'Pitch', 'Roll', 'Yaw']

print("="*70)
print("ğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
print("="*70)

# ============================================================================
# 1ë‹¨ê³„: ë…¸ë“œ ì •ë³´ ë¡œë“œ
# ============================================================================
print("\n[1/6] ë…¸ë“œ ì •ë³´ ë¡œë“œ...")
nodes_df = pd.read_csv('nodes_final.csv')
node_positions = {row['id']: (row['x_m'], row['y_m'])
                  for _, row in nodes_df.iterrows()}

print(f"  ì´ {len(node_positions)}ê°œ ë…¸ë“œ ë¡œë“œ")

# ê±´ë¬¼ ë²”ìœ„ ê³„ì‚°
x_coords = [pos[0] for pos in node_positions.values()]
y_coords = [pos[1] for pos in node_positions.values()]
x_min, x_max = min(x_coords), max(x_coords)
y_min, y_max = min(y_coords), max(y_coords)

print(f"  ê±´ë¬¼ ë²”ìœ„: x=[{x_min}, {x_max}], y=[{y_min}, {y_max}]")

# ============================================================================
# 2ë‹¨ê³„: ê²½ë¡œ ê³„íš í•¨ìˆ˜
# ============================================================================

def plan_simple_route(start_node, end_node, node_positions):
    """
    ê°„ë‹¨í•œ ê²½ë¡œ ê³„íš: ë‘ ë…¸ë“œ ì‚¬ì´ë¥¼ ì§ì„  ë³´ê°„
    ë‚˜ì¤‘ì— RightAngle ì •ë³´ë¡œ ê°œì„  ê°€ëŠ¥
    """
    start_pos = node_positions[start_node]
    end_pos = node_positions[end_node]

    # ë§¨í•´íŠ¼ ê±°ë¦¬ ê¸°ë°˜ ê²½ë¡œ (ë‹¨ìˆœí™”)
    # ì‹¤ì œë¡œëŠ” ë³µë„ êµ¬ì¡°ë¥¼ ë”°ë¼ê°€ì•¼ í•˜ì§€ë§Œ, ì¼ë‹¨ ì§ì„ ìœ¼ë¡œ
    return [start_pos, end_pos]


def calculate_marker_coordinates(start_pos, end_pos, num_markers):
    """
    ì‹œì‘ì ì—ì„œ ëì ê¹Œì§€ num_markers ê°œì˜ ì¢Œí‘œ ìƒì„± (0.45m ê°„ê²©)
    """
    coords = []

    # ì´ ê±°ë¦¬
    dx = end_pos[0] - start_pos[0]
    dy = end_pos[1] - start_pos[1]
    total_distance = np.sqrt(dx**2 + dy**2)

    # ê° ë§ˆì»¤ì˜ ì¢Œí‘œ ê³„ì‚°
    for i in range(num_markers):
        # ì§„í–‰ë¥  (0.0 ~ 1.0)
        progress = i / (num_markers - 1) if num_markers > 1 else 0

        x = start_pos[0] + dx * progress
        y = start_pos[1] + dy * progress
        coords.append((x, y))

    return coords


# ============================================================================
# 3ë‹¨ê³„: ê·¸ë¦¬ë“œ ë§¤í•‘
# ============================================================================

def coord_to_grid(x, y):
    """ì ˆëŒ€ ì¢Œí‘œë¥¼ ê·¸ë¦¬ë“œ IDë¡œ ë³€í™˜"""
    grid_x = int(round((x - x_min) / GRID_SIZE))
    grid_y = int(round((y - y_min) / GRID_SIZE))

    # ê·¸ë¦¬ë“œ ë²”ìœ„
    num_x_grids = int(np.ceil((x_max - x_min) / GRID_SIZE)) + 1
    num_y_grids = int(np.ceil((y_max - y_min) / GRID_SIZE)) + 1

    # ë²”ìœ„ ì²´í¬
    grid_x = max(0, min(grid_x, num_x_grids - 1))
    grid_y = max(0, min(grid_y, num_y_grids - 1))

    grid_id = grid_y * num_x_grids + grid_x
    return grid_id, (grid_x, grid_y)


# ============================================================================
# 4ë‹¨ê³„: íŒŒì¼ë³„ ì²˜ë¦¬
# ============================================================================

def process_file(filepath):
    """
    í•˜ë‚˜ì˜ ê²½ë¡œ íŒŒì¼ ì²˜ë¦¬

    Returns:
        sequences: (N, 100, 6) - Nê°œì˜ ì‹œí€€ìŠ¤
        labels: (N,) - ê·¸ë¦¬ë“œ ID
        coords: (N, 2) - ì ˆëŒ€ ì¢Œí‘œ (x, y)
    """
    filename = os.path.basename(filepath)
    parts = filename.replace('.csv', '').split('_')

    if len(parts) != 3:
        return None

    start_node = int(parts[0])
    end_node = int(parts[1])
    trial = parts[2]

    # ë…¸ë“œ ì¡´ì¬ í™•ì¸
    if start_node not in node_positions or end_node not in node_positions:
        return None

    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(filepath)

    # Highlighted ë§ˆì»¤ ì¸ë±ìŠ¤
    highlighted_indices = df[df['Highlighted'] == True].index.tolist()
    num_markers = len(highlighted_indices)

    if num_markers == 0:
        return None

    # ë§ˆì»¤ ì¢Œí‘œ ê³„ì‚°
    start_pos = node_positions[start_node]
    end_pos = node_positions[end_node]
    marker_coords = calculate_marker_coordinates(start_pos, end_pos, num_markers)

    # ì‹œí€€ìŠ¤ ë° ë¼ë²¨ ìƒì„±
    sequences = []
    labels = []
    coords_list = []

    for marker_idx, (hl_idx, (x, y)) in enumerate(zip(highlighted_indices, marker_coords)):
        # ë§ˆì»¤ ì§ì „ 100 ìƒ˜í”Œ
        start_idx = max(0, hl_idx - WINDOW_SIZE)
        end_idx = hl_idx

        # ì„¼ì„œ ë°ì´í„° ì¶”ì¶œ
        seq = df.iloc[start_idx:end_idx][SENSOR_COLS].values

        # íŒ¨ë”© (100 ìƒ˜í”Œ ë¯¸ë§Œì¸ ê²½ìš°)
        if len(seq) < WINDOW_SIZE:
            # ì•ë¶€ë¶„ì„ ë³µì‚¬í•´ì„œ íŒ¨ë”©
            pad_len = WINDOW_SIZE - len(seq)
            if len(seq) > 0:
                seq = np.vstack([np.tile(seq[0], (pad_len, 1)), seq])
            else:
                continue  # ë°ì´í„°ê°€ ì•„ì˜ˆ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°

        # ê·¸ë¦¬ë“œ ID ê³„ì‚°
        grid_id, grid_xy = coord_to_grid(x, y)

        sequences.append(seq)
        labels.append(grid_id)
        coords_list.append((x, y))

    if len(sequences) == 0:
        return None

    return {
        'sequences': np.array(sequences),
        'labels': np.array(labels),
        'coords': np.array(coords_list),
        'route': f"{start_node}â†’{end_node}",
        'trial': trial
    }


# ============================================================================
# 5ë‹¨ê³„: ì „ì²´ ë°ì´í„°ì…‹ ìƒì„±
# ============================================================================

print("\n[2/6] ë°ì´í„° íŒŒì¼ ì²˜ë¦¬...")

data_dir = Path('law_data')
files = sorted(list(data_dir.glob('*.csv')))

all_data = []
grid_stats = defaultdict(int)

for filepath in tqdm(files, desc="Processing files"):
    result = process_file(filepath)
    if result is not None:
        all_data.append(result)

        # ê·¸ë¦¬ë“œ í†µê³„
        for label in result['labels']:
            grid_stats[label] += 1

print(f"\n  ì²˜ë¦¬ëœ íŒŒì¼: {len(all_data)}/{len(files)}")
print(f"  ê³ ìœ  ê·¸ë¦¬ë“œ ì…€: {len(grid_stats)}")
print(f"  ì´ ìƒ˜í”Œ ìˆ˜: {sum(len(d['sequences']) for d in all_data)}")

# ============================================================================
# 6ë‹¨ê³„: ë°ì´í„° í†µí•© ë° ì •ê·œí™”
# ============================================================================

print("\n[3/6] ë°ì´í„° í†µí•© ë° ì •ê·œí™”...")

# ëª¨ë“  ì‹œí€€ìŠ¤ í†µí•©
all_sequences = np.vstack([d['sequences'] for d in all_data])
all_labels = np.concatenate([d['labels'] for d in all_data])
all_coords = np.vstack([d['coords'] for d in all_data])

print(f"  í†µí•© ë°ì´í„° shape: {all_sequences.shape}")
print(f"  ë¼ë²¨ shape: {all_labels.shape}")

# ì •ê·œí™” íŒŒë¼ë¯¸í„° ê³„ì‚°
mean = all_sequences.mean(axis=(0, 1))
std = all_sequences.std(axis=(0, 1))

print(f"\n  ì •ê·œí™” íŒŒë¼ë¯¸í„°:")
for i, col in enumerate(SENSOR_COLS):
    print(f"    {col}: mean={mean[i]:.4f}, std={std[i]:.4f}")

# ì •ê·œí™” ì ìš©
all_sequences_norm = (all_sequences - mean) / (std + 1e-8)

# ============================================================================
# 7ë‹¨ê³„: Train/Val/Test ë¶„í• 
# ============================================================================

print("\n[4/6] Train/Val/Test ë¶„í• ...")

# ê²½ë¡œë³„ë¡œ ê·¸ë£¹í™”
route_groups = defaultdict(list)
for i, data in enumerate(all_data):
    route_groups[data['route']].append(i)

# ê²½ë¡œë¥¼ 70/15/15ë¡œ ë¶„í• 
routes = list(route_groups.keys())
np.random.seed(42)
np.random.shuffle(routes)

n_routes = len(routes)
n_train = int(0.7 * n_routes)
n_val = int(0.15 * n_routes)

train_routes = routes[:n_train]
val_routes = routes[n_train:n_train+n_val]
test_routes = routes[n_train+n_val:]

print(f"  Train ê²½ë¡œ: {len(train_routes)}")
print(f"  Val ê²½ë¡œ: {len(val_routes)}")
print(f"  Test ê²½ë¡œ: {len(test_routes)}")

# ì¸ë±ìŠ¤ ìˆ˜ì§‘
train_indices = []
val_indices = []
test_indices = []

idx_offset = 0
for data in all_data:
    route = data['route']
    n_samples = len(data['sequences'])
    indices = list(range(idx_offset, idx_offset + n_samples))

    if route in train_routes:
        train_indices.extend(indices)
    elif route in val_routes:
        val_indices.extend(indices)
    else:
        test_indices.extend(indices)

    idx_offset += n_samples

print(f"\n  Train ìƒ˜í”Œ: {len(train_indices)}")
print(f"  Val ìƒ˜í”Œ: {len(val_indices)}")
print(f"  Test ìƒ˜í”Œ: {len(test_indices)}")

# ============================================================================
# 8ë‹¨ê³„: ë°ì´í„°ì…‹ ì €ì¥
# ============================================================================

print("\n[5/6] ë°ì´í„°ì…‹ ì €ì¥...")

# ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
output_dir = Path('processed_data')
output_dir.mkdir(exist_ok=True)

# NumPy í˜•ì‹ìœ¼ë¡œ ì €ì¥
np.save(output_dir / 'X_train.npy', all_sequences_norm[train_indices])
np.save(output_dir / 'y_train.npy', all_labels[train_indices])
np.save(output_dir / 'coords_train.npy', all_coords[train_indices])

np.save(output_dir / 'X_val.npy', all_sequences_norm[val_indices])
np.save(output_dir / 'y_val.npy', all_labels[val_indices])
np.save(output_dir / 'coords_val.npy', all_coords[val_indices])

np.save(output_dir / 'X_test.npy', all_sequences_norm[test_indices])
np.save(output_dir / 'y_test.npy', all_labels[test_indices])
np.save(output_dir / 'coords_test.npy', all_coords[test_indices])

# ë©”íƒ€ë°ì´í„° ì €ì¥
metadata = {
    'window_size': WINDOW_SIZE,
    'grid_size': GRID_SIZE,
    'sensor_cols': SENSOR_COLS,
    'mean': mean,
    'std': std,
    'num_classes': len(grid_stats),
    'grid_stats': dict(grid_stats),
    'x_range': (x_min, x_max),
    'y_range': (y_min, y_max),
}

with open(output_dir / 'metadata.pkl', 'wb') as f:
    pickle.dump(metadata, f)

print(f"  ì €ì¥ ì™„ë£Œ: {output_dir}")

# ============================================================================
# 9ë‹¨ê³„: ê²€ì¦
# ============================================================================

print("\n[6/6] ë°ì´í„°ì…‹ ê²€ì¦...")

# ë¡œë“œ í…ŒìŠ¤íŠ¸
X_train = np.load(output_dir / 'X_train.npy')
y_train = np.load(output_dir / 'y_train.npy')

print(f"\n  âœ… X_train shape: {X_train.shape}")
print(f"  âœ… y_train shape: {y_train.shape}")
print(f"  âœ… í´ë˜ìŠ¤ ìˆ˜: {len(np.unique(y_train))}")
print(f"  âœ… ê°’ ë²”ìœ„ í™•ì¸:")
print(f"     X_train: [{X_train.min():.2f}, {X_train.max():.2f}]")
print(f"     y_train: [{y_train.min()}, {y_train.max()}]")

# í´ë˜ìŠ¤ ë¶„í¬
unique, counts = np.unique(y_train, return_counts=True)
print(f"\n  ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬ (ìƒìœ„ 10ê°œ):")
top_classes = sorted(zip(unique, counts), key=lambda x: x[1], reverse=True)[:10]
for cls, cnt in top_classes:
    print(f"     ê·¸ë¦¬ë“œ {cls}: {cnt}ê°œ ìƒ˜í”Œ")

print("\n" + "="*70)
print("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
print("="*70)
print(f"""
ë‹¤ìŒ íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:
  - processed_data/X_train.npy  ({X_train.shape})
  - processed_data/X_val.npy
  - processed_data/X_test.npy
  - processed_data/y_train.npy
  - processed_data/y_val.npy
  - processed_data/y_test.npy
  - processed_data/metadata.pkl

ì´ì œ LSTM ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!
""")
