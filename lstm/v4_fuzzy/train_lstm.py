#!/usr/bin/env python3
"""
ì§€ìê¸° ê¸°ë°˜ ì‹¤ë‚´ ìœ„ì¹˜ ì¶”ì • - LSTM ëª¨ë¸ í•™ìŠµ
"""
import numpy as np
import pickle
from pathlib import Path
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
import matplotlib.pyplot as plt

print("="*70)
print("ğŸš€ LSTM ëª¨ë¸ í•™ìŠµ ì‹œì‘ (v4 + Fuzzy)")
print("="*70)

# ============================================================================
# 1. ë°ì´í„° ë¡œë“œ
# ============================================================================
print("\n[1/5] ë°ì´í„° ë¡œë“œ...")

data_dir = Path('processed_data_v4_fuzzy')

X_train = np.load(data_dir / 'X_train.npy')
y_train = np.load(data_dir / 'y_train.npy')

X_val = np.load(data_dir / 'X_val.npy')
y_val = np.load(data_dir / 'y_val.npy')

X_test = np.load(data_dir / 'X_test.npy')
y_test = np.load(data_dir / 'y_test.npy')

with open(data_dir / 'metadata.pkl', 'rb') as f:
    metadata = pickle.load(f)

num_classes = metadata['num_classes']

print(f"  Train: {X_train.shape} â†’ {num_classes} í´ë˜ìŠ¤")
print(f"  Val:   {X_val.shape}")
print(f"  Test:  {X_test.shape}")
print(f"  í´ë˜ìŠ¤ ìˆ˜: {num_classes}")

# ============================================================================
# 2. ëª¨ë¸ ì •ì˜
# ============================================================================
print("\n[2/5] ëª¨ë¸ êµ¬ì¶•...")

def build_lstm_model(input_shape, num_classes):
    """
    4ì¸µ LSTM ëª¨ë¸

    Args:
        input_shape: (timesteps, features) = (100, 6)
        num_classes: ì¶œë ¥ í´ë˜ìŠ¤ ìˆ˜
    """
    model = keras.Sequential([
        # Input layer
        layers.Input(shape=input_shape),

        # LSTM Layer 1
        layers.LSTM(128, return_sequences=True, name='lstm_1'),
        layers.BatchNormalization(),
        layers.Dropout(0.3),

        # LSTM Layer 2
        layers.LSTM(256, return_sequences=True, name='lstm_2'),
        layers.BatchNormalization(),
        layers.Dropout(0.3),

        # LSTM Layer 3
        layers.LSTM(256, return_sequences=True, name='lstm_3'),
        layers.BatchNormalization(),
        layers.Dropout(0.3),

        # LSTM Layer 4
        layers.LSTM(128, return_sequences=False, name='lstm_4'),
        layers.BatchNormalization(),
        layers.Dropout(0.3),

        # Dense layers
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.4),

        layers.Dense(128, activation='relu'),
        layers.Dropout(0.4),

        # Output layer
        layers.Dense(num_classes, activation='softmax', name='output')
    ])

    return model

# ëª¨ë¸ ìƒì„±
input_shape = (X_train.shape[1], X_train.shape[2])  # (100, 6)
model = build_lstm_model(input_shape, num_classes)

# ëª¨ë¸ ì»´íŒŒì¼
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# ëª¨ë¸ ìš”ì•½
model.summary()

print(f"\n  ì´ íŒŒë¼ë¯¸í„°: {model.count_params():,}")

# ============================================================================
# 3. ì½œë°± ì„¤ì •
# ============================================================================
print("\n[3/5] í•™ìŠµ ì„¤ì •...")

# ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
model_dir = Path('models')
model_dir.mkdir(exist_ok=True)

callbacks = [
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
    ModelCheckpoint(
        filepath=str(model_dir / 'lstm_best.keras'),
        monitor='val_accuracy',
        save_best_only=True,
        mode='max',
        verbose=1
    ),

    # Early stopping
    EarlyStopping(
        monitor='val_accuracy',
        patience=15,
        mode='max',
        verbose=1,
        restore_best_weights=True
    ),

    # Learning rate ê°ì†Œ
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=1e-6,
        verbose=1
    )
]

print("  Callbacks:")
print("    - ModelCheckpoint: val_accuracy ìµœê³  ëª¨ë¸ ì €ì¥")
print("    - EarlyStopping: patience=15")
print("    - ReduceLROnPlateau: patience=5, factor=0.5")

# ============================================================================
# 4. ëª¨ë¸ í•™ìŠµ
# ============================================================================
print("\n[4/5] ëª¨ë¸ í•™ìŠµ...")

BATCH_SIZE = 128
EPOCHS = 100

history = model.fit(
    X_train, y_train,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    validation_data=(X_val, y_val),
    callbacks=callbacks,
    verbose=1
)

# ============================================================================
# 5. í•™ìŠµ ê²°ê³¼ ì‹œê°í™”
# ============================================================================
print("\n[5/5] í•™ìŠµ ê²°ê³¼ ì €ì¥...")

# í•™ìŠµ ê³¡ì„  ê·¸ë¦¬ê¸°
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Loss
ax1.plot(history.history['loss'], label='Train Loss')
ax1.plot(history.history['val_loss'], label='Val Loss')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss')
ax1.set_title('Training and Validation Loss')
ax1.legend()
ax1.grid(True)

# Accuracy
ax2.plot(history.history['accuracy'], label='Train Accuracy')
ax2.plot(history.history['val_accuracy'], label='Val Accuracy')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Accuracy')
ax2.set_title('Training and Validation Accuracy')
ax2.legend()
ax2.grid(True)

plt.tight_layout()
plt.savefig('training_history.png', dpi=150)
print("  í•™ìŠµ ê³¡ì„  ì €ì¥: training_history.png")

# ìµœì¢… í‰ê°€
print("\n[ìµœì¢… í‰ê°€]")
train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)
val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)

print(f"  Train - Loss: {train_loss:.4f}, Accuracy: {train_acc*100:.2f}%")
print(f"  Val   - Loss: {val_loss:.4f}, Accuracy: {val_acc*100:.2f}%")
print(f"  Test  - Loss: {test_loss:.4f}, Accuracy: {test_acc*100:.2f}%")

# í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥
with open(model_dir / 'history.pkl', 'wb') as f:
    pickle.dump(history.history, f)

# ìµœì¢… ëª¨ë¸ ì €ì¥
model.save(model_dir / 'lstm_final.keras')
print(f"\n  ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_dir}/")

print("\n" + "="*70)
print("âœ… í•™ìŠµ ì™„ë£Œ!")
print("="*70)
print(f"""
ğŸ“Š ìµœì¢… ê²°ê³¼:
  Train Accuracy: {train_acc*100:.2f}%
  Val Accuracy:   {val_acc*100:.2f}%
  Test Accuracy:  {test_acc*100:.2f}%

  ëª¨ë¸ ìœ„ì¹˜: {model_dir}/lstm_best.keras

ë‹¤ìŒ ë‹¨ê³„: ì„±ëŠ¥ í‰ê°€ ë° ìœ„ì¹˜ ì˜¤ì°¨ ë¶„ì„ ğŸ¯
""")
