#!/usr/bin/env python3
"""
ì§€ìê¸° ê¸°ë°˜ ì‹¤ë‚´ ìœ„ì¹˜ ì¶”ì • - Fuzzy ì •ê·œí™” ì „ì²˜ë¦¬
Fuzzy membership functionsë¥¼ ì‚¬ìš©í•œ ì •ê·œí™”
"""
import pandas as pd
import numpy as np
import os
from pathlib import Path
import pickle
from tqdm import tqdm
from collections import defaultdict

print("="*70)
print("ğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ v3 + Fuzzy ì •ê·œí™”")
print("="*70)

# ============================================================================
# Configuration
# ============================================================================
PROJECT_ROOT = Path(__file__).resolve().parents[2]
DATA_DIR = PROJECT_ROOT / 'law_data'
NODES_FILE = Path('../nodes_final.csv')
OUTPUT_DIR = Path('processed_data_v3_fuzzy')
OUTPUT_DIR.mkdir(exist_ok=True)

WINDOW_SIZE = 100  # ì‹œí€€ìŠ¤ ê¸¸ì´
STRIDE = 5         # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° stride
GRID_SIZE = 0.45   # ê·¸ë¦¬ë“œ í¬ê¸° (m)

# ë°ì´í„° ì¦ê°• ì„¤ì •
AUGMENT_RATIO = 0.3
MAG_NOISE_STD = 0.8
ORIENTATION_NOISE_STD = 1.5

# ============================================================================
# Fuzzy Normalization Functions
# ============================================================================

def triangular_membership(x, a, b, c):
    """
    ì‚¼ê°í˜• ë©¤ë²„ì‹­ í•¨ìˆ˜
    a: ì™¼ìª½ ë
    b: ì¤‘ì‹¬ (í”¼í¬)
    c: ì˜¤ë¥¸ìª½ ë
    """
    if x <= a or x >= c:
        return 0.0
    elif a < x <= b:
        return (x - a) / (b - a)
    else:  # b < x < c
        return (c - x) / (c - b)

def trapezoidal_membership(x, a, b, c, d):
    """
    ì‚¬ë‹¤ë¦¬ê¼´ ë©¤ë²„ì‹­ í•¨ìˆ˜
    a: ì™¼ìª½ ì‹œì‘
    b: ì™¼ìª½ í‰íƒ„ ì‹œì‘
    c: ì˜¤ë¥¸ìª½ í‰íƒ„ ë
    d: ì˜¤ë¥¸ìª½ ë
    """
    if x <= a or x >= d:
        return 0.0
    elif a < x <= b:
        return (x - a) / (b - a)
    elif b < x <= c:
        return 1.0
    else:  # c < x < d
        return (d - x) / (d - a)

def fuzzy_normalize(data, feature_name):
    """
    Fuzzy ì •ê·œí™”: ê° íŠ¹ì§•ì— ëŒ€í•´ Low/Medium/High ë©¤ë²„ì‹­ í•¨ìˆ˜ ì ìš©
    ì¶œë ¥: ê° ìƒ˜í”Œë§ˆë‹¤ (low, medium, high) membership ê°’
    """
    min_val = np.min(data)
    max_val = np.max(data)
    range_val = max_val - min_val

    # í¼ì§€ êµ¬ê°„ ì •ì˜
    q25 = np.percentile(data, 25)
    q50 = np.percentile(data, 50)
    q75 = np.percentile(data, 75)

    # Low: ì‚¼ê°í˜• (min, min, q50)
    # Medium: ì‚¼ê°í˜• (q25, q50, q75)
    # High: ì‚¼ê°í˜• (q50, max, max)

    n_samples = len(data)
    fuzzy_features = np.zeros((n_samples, 3))

    for i, val in enumerate(data):
        # Low membership
        if val <= min_val:
            fuzzy_features[i, 0] = 1.0
        elif val <= q50:
            fuzzy_features[i, 0] = (q50 - val) / (q50 - min_val)
        else:
            fuzzy_features[i, 0] = 0.0

        # Medium membership (ì‚¼ê°í˜•)
        fuzzy_features[i, 1] = triangular_membership(val, q25, q50, q75)

        # High membership
        if val >= max_val:
            fuzzy_features[i, 2] = 1.0
        elif val >= q50:
            fuzzy_features[i, 2] = (val - q50) / (max_val - q50)
        else:
            fuzzy_features[i, 2] = 0.0

    return fuzzy_features

def standard_normalize(data, mean, std):
    """í‘œì¤€ ì •ê·œí™” (ë¹„êµìš©)"""
    return (data - mean) / (std + 1e-8)

# ============================================================================
# 1. ë…¸ë“œ ì •ë³´ ë¡œë“œ
# ============================================================================
print("\n[1/6] ë…¸ë“œ ì •ë³´ ë¡œë“œ...")
nodes_df = pd.read_csv(NODES_FILE)
print(f"  ì´ {len(nodes_df)}ê°œ ë…¸ë“œ")

# ============================================================================
# 2. ëª¨ë“  CSV íŒŒì¼ ë¡œë“œ ë° ê²½ë¡œ ë¶„ì„
# ============================================================================
print("\n[2/6] CSV íŒŒì¼ ë¡œë“œ ë° ë¶„ì„...")

all_trajectories = []
route_info = defaultdict(list)

csv_files = sorted(DATA_DIR.glob('*.csv'))
print(f"  ì´ {len(csv_files)}ê°œ íŒŒì¼")

for csv_file in tqdm(csv_files, desc="  íŒŒì¼ ë¡œë“œ"):
    try:
        df = pd.read_csv(csv_file)

        # íŒŒì¼ëª… íŒŒì‹±: start_end_trial.csv
        parts = csv_file.stem.split('_')
        start_node = int(parts[0])
        end_node = int(parts[1])
        trial = int(parts[2])

        all_trajectories.append({
            'file': csv_file.name,
            'start_node': start_node,
            'end_node': end_node,
            'trial': trial,
            'df': df
        })

        route_info[(start_node, end_node)].append(csv_file.name)

    except Exception as e:
        print(f"    ì˜¤ë¥˜ ({csv_file.name}): {e}")

print(f"  ë¡œë“œ ì™„ë£Œ: {len(all_trajectories)}ê°œ ê¶¤ì ")
print(f"  ê³ ìœ  ê²½ë¡œ: {len(route_info)}ê°œ")

# ============================================================================
# 3. ë§ˆì»¤ ê¸°ë°˜ ì ˆëŒ€ ì¢Œí‘œ ê³„ì‚° ë° ì‹œí€€ìŠ¤ ìƒì„±
# ============================================================================
print("\n[3/6] ë§ˆì»¤ ê¸°ë°˜ ì‹œí€€ìŠ¤ ìƒì„±...")

sequences = []
all_labels = []

for traj in tqdm(all_trajectories, desc="  ì‹œí€€ìŠ¤ ìƒì„±"):
    df = traj['df']
    start_node = traj['start_node']
    end_node = traj['end_node']

    # Highlighted ë§ˆì»¤ ì¶”ì¶œ
    marker_indices = df[df['Highlighted'] == True].index.tolist()

    if len(marker_indices) < 2:
        continue

    # ì‹œì‘/ë ë…¸ë“œ ì¢Œí‘œ
    start_coord = nodes_df[nodes_df['Node'] == start_node][['X', 'Y']].values[0]
    end_coord = nodes_df[nodes_df['Node'] == end_node][['X', 'Y']].values[0]

    # ê²½ë¡œ ë²¡í„°
    path_vector = end_coord - start_coord
    path_length = np.linalg.norm(path_vector)

    # ë§ˆì»¤ ê°„ê²© 0.45m
    for i in range(len(marker_indices) - 1):
        marker_idx_A = marker_indices[i]
        marker_idx_B = marker_indices[i + 1]

        # ë§ˆì»¤ Aì˜ ì ˆëŒ€ ì¢Œí‘œ ê³„ì‚°
        progress = (i * GRID_SIZE) / path_length  # ê²½ë¡œ ì§„í–‰ë¥ 
        marker_pos = start_coord + progress * path_vector

        # ë§ˆì»¤ Aì™€ B ì‚¬ì´ì˜ ëª¨ë“  ìƒ˜í”Œì— ëŒ€í•´ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°
        for center_idx in range(marker_idx_A, marker_idx_B, STRIDE):
            start_idx = center_idx - WINDOW_SIZE
            end_idx = center_idx

            if start_idx < 0:
                continue

            # ìœˆë„ìš° ì¶”ì¶œ
            window = df.iloc[start_idx:end_idx]

            if len(window) != WINDOW_SIZE:
                continue

            # íŠ¹ì§• ì¶”ì¶œ: MagX, MagY, MagZ, Pitch, Roll, Yaw
            features = window[['MagX', 'MagY', 'MagZ', 'Pitch', 'Roll', 'Yaw']].values

            sequences.append(features)
            all_labels.append(marker_pos)

sequences = np.array(sequences)  # (N, 100, 6)
all_labels = np.array(all_labels)  # (N, 2)

print(f"  ìƒì„±ëœ ì‹œí€€ìŠ¤: {len(sequences):,}ê°œ")

# ============================================================================
# 4. Fuzzy ì •ê·œí™” ì ìš©
# ============================================================================
print("\n[4/6] Fuzzy ì •ê·œí™” ì ìš©...")

# ê° íŠ¹ì§•ì— ëŒ€í•œ í†µê³„
feature_names = ['MagX', 'MagY', 'MagZ', 'Pitch', 'Roll', 'Yaw']
fuzzy_sequences = []

for i, fname in enumerate(feature_names):
    print(f"  {fname} ì •ê·œí™” ì¤‘...")

    # ëª¨ë“  ìƒ˜í”Œì˜ í•´ë‹¹ íŠ¹ì§• ì¶”ì¶œ
    feature_data = sequences[:, :, i].flatten()

    # Fuzzy ì •ê·œí™” (Low, Medium, High)
    fuzzy_feat = fuzzy_normalize(feature_data, fname)

    # ì›ë˜ shapeìœ¼ë¡œ ë³µì›
    fuzzy_feat = fuzzy_feat.reshape(len(sequences), WINDOW_SIZE, 3)
    fuzzy_sequences.append(fuzzy_feat)

# ëª¨ë“  íŠ¹ì§• í•©ì¹˜ê¸°: (N, 100, 6*3) = (N, 100, 18)
fuzzy_sequences = np.concatenate(fuzzy_sequences, axis=2)

print(f"  Fuzzy ì •ê·œí™” ì™„ë£Œ: {fuzzy_sequences.shape}")

# ============================================================================
# 5. ê·¸ë¦¬ë“œ ë§¤í•‘ ë° ë¼ë²¨ ìƒì„±
# ============================================================================
print("\n[5/6] ê·¸ë¦¬ë“œ ë§¤í•‘...")

# ì¢Œí‘œë¥¼ ê·¸ë¦¬ë“œ IDë¡œ ë³€í™˜
def coord_to_grid_id(x, y, grid_size=GRID_SIZE):
    grid_x = int(np.round(x / grid_size))
    grid_y = int(np.round(y / grid_size))
    return f"{grid_x}_{grid_y}"

grid_ids = [coord_to_grid_id(x, y) for x, y in all_labels]
unique_grids = sorted(set(grid_ids))
grid_to_idx = {grid: idx for idx, grid in enumerate(unique_grids)}
labels = np.array([grid_to_idx[gid] for gid in grid_ids])

print(f"  ê³ ìœ  ê·¸ë¦¬ë“œ: {len(unique_grids)}ê°œ")

# í´ë˜ìŠ¤ í•„í„°ë§ (10ê°œ ë¯¸ë§Œ ìƒ˜í”Œ)
from collections import Counter
class_counts = Counter(labels)
valid_classes = [cls for cls, count in class_counts.items() if count >= 10]

print(f"  í•„í„°ë§ ì „ í´ë˜ìŠ¤: {len(unique_grids)}ê°œ")
print(f"  í•„í„°ë§ í›„ í´ë˜ìŠ¤: {len(valid_classes)}ê°œ")

# í•„í„°ë§
valid_indices = [i for i, lbl in enumerate(labels) if lbl in valid_classes]
fuzzy_sequences = fuzzy_sequences[valid_indices]
labels = labels[valid_indices]

# í´ë˜ìŠ¤ ID ì¬ë§¤í•‘
old_to_new = {old: new for new, old in enumerate(sorted(valid_classes))}
labels = np.array([old_to_new[lbl] for lbl in labels])

# ============================================================================
# 6. Train/Val/Test ë¶„í•  ë° ì €ì¥
# ============================================================================
print("\n[6/6] ë°ì´í„° ë¶„í•  ë° ì €ì¥...")

# ê²½ë¡œ ê¸°ë°˜ ë¶„í• 
route_files = defaultdict(list)
for i, traj in enumerate(all_trajectories):
    route_key = (traj['start_node'], traj['end_node'])
    route_files[route_key].append(traj['file'])

unique_routes = list(route_files.keys())
np.random.shuffle(unique_routes)

n_routes = len(unique_routes)
n_train = int(0.7 * n_routes)
n_val = int(0.15 * n_routes)

train_routes = set(unique_routes[:n_train])
val_routes = set(unique_routes[n_train:n_train + n_val])
test_routes = set(unique_routes[n_train + n_val:])

# ìƒ˜í”Œ ë¶„í• 
train_indices = []
val_indices = []
test_indices = []

for i, traj in enumerate(all_trajectories):
    if i >= len(valid_indices):
        continue
    route_key = (traj['start_node'], traj['end_node'])

    if route_key in train_routes:
        train_indices.append(valid_indices[i])
    elif route_key in val_routes:
        val_indices.append(valid_indices[i])
    else:
        test_indices.append(valid_indices[i])

# ì‹¤ì œ ë¶„í• 
X_train = fuzzy_sequences[train_indices]
y_train = labels[train_indices]

X_val = fuzzy_sequences[val_indices]
y_val = labels[val_indices]

X_test = fuzzy_sequences[test_indices]
y_test = labels[test_indices]

print(f"  Train: {len(X_train):,} ìƒ˜í”Œ ({len(train_routes)} ê²½ë¡œ)")
print(f"  Val:   {len(X_val):,} ìƒ˜í”Œ ({len(val_routes)} ê²½ë¡œ)")
print(f"  Test:  {len(X_test):,} ìƒ˜í”Œ ({len(test_routes)} ê²½ë¡œ)")

# ì €ì¥
np.save(OUTPUT_DIR / 'X_train.npy', X_train)
np.save(OUTPUT_DIR / 'y_train.npy', y_train)
np.save(OUTPUT_DIR / 'X_val.npy', X_val)
np.save(OUTPUT_DIR / 'y_val.npy', y_val)
np.save(OUTPUT_DIR / 'X_test.npy', X_test)
np.save(OUTPUT_DIR / 'y_test.npy', y_test)

# ë©”íƒ€ë°ì´í„° ì €ì¥
metadata = {
    'num_classes': len(valid_classes),
    'num_features': 18,  # 6 features * 3 fuzzy values
    'window_size': WINDOW_SIZE,
    'grid_size': GRID_SIZE,
    'fuzzy_normalization': True,
    'grid_to_idx': grid_to_idx,
    'feature_names': feature_names,
}

with open(OUTPUT_DIR / 'metadata.pkl', 'wb') as f:
    pickle.dump(metadata, f)

print(f"\nâœ… Fuzzy ì •ê·œí™” ì „ì²˜ë¦¬ ì™„ë£Œ!")
print(f"  ì¶œë ¥ ë””ë ‰í† ë¦¬: {OUTPUT_DIR}")
print(f"  ì…ë ¥ shape: (batch, {WINDOW_SIZE}, 18)")
print(f"  ì¶œë ¥ í´ë˜ìŠ¤: {len(valid_classes)}ê°œ")
