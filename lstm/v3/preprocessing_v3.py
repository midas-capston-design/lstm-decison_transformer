#!/usr/bin/env python3
"""
ì§€ìê¸° ê¸°ë°˜ ì‹¤ë‚´ ìœ„ì¹˜ ì¶”ì • - ë°ì´í„° ì „ì²˜ë¦¬ v3
- Stride ì¶•ì†Œ (10 â†’ 5)
- ë³´ìˆ˜ì  ë°ì´í„° ì¦ê°• (ì„¼ì„œ ë…¸ì´ì¦ˆ ìˆ˜ì¤€)
"""
import pandas as pd
import numpy as np
import os
from pathlib import Path
import pickle
from tqdm import tqdm
from collections import defaultdict

# ì„¤ì •
WINDOW_SIZE = 100  # ìƒ˜í”Œ
STRIDE = 5         # Sliding window stride (5ìƒ˜í”Œ = 0.1ì´ˆ ê°„ê²©)
GRID_SIZE = 0.45   # m
SENSOR_COLS = ['MagX', 'MagY', 'MagZ', 'Pitch', 'Roll', 'Yaw']

# ë°ì´í„° ì¦ê°• ì„¤ì •
AUGMENT_PROB = 0.3  # 30% ìƒ˜í”Œë§Œ ì¦ê°•
MAG_NOISE_STD = 0.8  # ìê¸°ì¥ ì„¼ì„œ ë…¸ì´ì¦ˆ (Î¼T)
ORIENTATION_NOISE_STD = 1.5  # ë°©í–¥ ì„¼ì„œ ë…¸ì´ì¦ˆ (ë„)

print("="*70)
print("ğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ v3 ì‹œì‘ (Stride=5 + ë°ì´í„° ì¦ê°•)")
print("="*70)
print(f"  Window: {WINDOW_SIZE} ìƒ˜í”Œ (2ì´ˆ @ 50Hz)")
print(f"  Stride: {STRIDE} ìƒ˜í”Œ (0.1ì´ˆ)")
print(f"  Grid: {GRID_SIZE}m")
print(f"  ì¦ê°• í™•ë¥ : {AUGMENT_PROB*100:.0f}%")
print(f"  ìê¸°ì¥ ë…¸ì´ì¦ˆ: std={MAG_NOISE_STD}Î¼T")
print(f"  ë°©í–¥ ë…¸ì´ì¦ˆ: std={ORIENTATION_NOISE_STD}Â°")

# ============================================================================
# ë°ì´í„° ì¦ê°• í•¨ìˆ˜
# ============================================================================

def augment_sequence(seq):
    """
    ì„¼ì„œ ë…¸ì´ì¦ˆ ìˆ˜ì¤€ì˜ ë³´ìˆ˜ì  ì¦ê°•

    Args:
        seq: (100, 6) numpy array [MagX, MagY, MagZ, Pitch, Roll, Yaw]

    Returns:
        augmented seq: (100, 6)
    """
    seq_aug = seq.copy()

    # ìê¸°ì¥ ì„¼ì„œ ë…¸ì´ì¦ˆ ì¶”ê°€ (MagX, MagY, MagZ)
    mag_noise = np.random.normal(0, MAG_NOISE_STD, (seq.shape[0], 3))
    seq_aug[:, 0:3] += mag_noise

    # ë°©í–¥ ì„¼ì„œ ë…¸ì´ì¦ˆ ì¶”ê°€ (Pitch, Roll, Yaw)
    orientation_noise = np.random.normal(0, ORIENTATION_NOISE_STD, (seq.shape[0], 3))
    seq_aug[:, 3:6] += orientation_noise

    return seq_aug

# ============================================================================
# 1ë‹¨ê³„: ë…¸ë“œ ì •ë³´ ë¡œë“œ
# ============================================================================
print("\n[1/7] ë…¸ë“œ ì •ë³´ ë¡œë“œ...")
nodes_df = pd.read_csv('nodes_final.csv')
node_positions = {row['id']: (row['x_m'], row['y_m'])
                  for _, row in nodes_df.iterrows()}

# ê±´ë¬¼ ë²”ìœ„ ê³„ì‚°
x_coords = [pos[0] for pos in node_positions.values()]
y_coords = [pos[1] for pos in node_positions.values()]
x_min, x_max = min(x_coords), max(x_coords)
y_min, y_max = min(y_coords), max(y_coords)

print(f"  ê±´ë¬¼ ë²”ìœ„: x=[{x_min}, {x_max}], y=[{y_min}, {y_max}]")

# ============================================================================
# 2ë‹¨ê³„: ê·¸ë¦¬ë“œ ë§¤í•‘ í•¨ìˆ˜
# ============================================================================

def coord_to_grid(x, y):
    """ì ˆëŒ€ ì¢Œí‘œë¥¼ ê·¸ë¦¬ë“œ IDë¡œ ë³€í™˜"""
    grid_x = int(round((x - x_min) / GRID_SIZE))
    grid_y = int(round((y - y_min) / GRID_SIZE))

    num_x_grids = int(np.ceil((x_max - x_min) / GRID_SIZE)) + 1
    num_y_grids = int(np.ceil((y_max - y_min) / GRID_SIZE)) + 1

    grid_x = max(0, min(grid_x, num_x_grids - 1))
    grid_y = max(0, min(grid_y, num_y_grids - 1))

    grid_id = grid_y * num_x_grids + grid_x
    return grid_id


def calculate_marker_coordinates(start_pos, end_pos, num_markers):
    """ì‹œì‘ì ì—ì„œ ëì ê¹Œì§€ num_markers ê°œì˜ ì¢Œí‘œ ìƒì„±"""
    coords = []
    dx = end_pos[0] - start_pos[0]
    dy = end_pos[1] - start_pos[1]

    for i in range(num_markers):
        progress = i / (num_markers - 1) if num_markers > 1 else 0
        x = start_pos[0] + dx * progress
        y = start_pos[1] + dy * progress
        coords.append((x, y))

    return coords


def interpolate_position(pos1, pos2, t1, t2, t_current):
    """
    ë‘ ë§ˆì»¤ ì‚¬ì´ì—ì„œ í˜„ì¬ ìƒ˜í”Œì˜ ìœ„ì¹˜ë¥¼ ì„ í˜• ë³´ê°„

    Args:
        pos1, pos2: ë§ˆì»¤ A, Bì˜ ì¢Œí‘œ
        t1, t2: ë§ˆì»¤ A, Bì˜ ì¸ë±ìŠ¤
        t_current: í˜„ì¬ ìƒ˜í”Œ ì¸ë±ìŠ¤
    """
    if t2 == t1:
        return pos1

    progress = (t_current - t1) / (t2 - t1)
    progress = max(0, min(1, progress))  # 0-1ë¡œ í´ë¦¬í•‘

    x = pos1[0] + (pos2[0] - pos1[0]) * progress
    y = pos1[1] + (pos2[1] - pos1[1]) * progress

    return (x, y)


# ============================================================================
# 3ë‹¨ê³„: íŒŒì¼ë³„ ì²˜ë¦¬ (ë§ˆì»¤ ì‚¬ì´ ë°ì´í„° í¬í•¨ + ì¦ê°•)
# ============================================================================

def process_file_v3(filepath):
    """
    í•˜ë‚˜ì˜ ê²½ë¡œ íŒŒì¼ ì²˜ë¦¬ (Stride=5 + ë°ì´í„° ì¦ê°•)

    Returns:
        sequences: (N, 100, 6)
        labels: (N,)
        coords: (N, 2)
    """
    filename = os.path.basename(filepath)
    parts = filename.replace('.csv', '').split('_')

    if len(parts) != 3:
        return None

    start_node = int(parts[0])
    end_node = int(parts[1])

    if start_node not in node_positions or end_node not in node_positions:
        return None

    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(filepath)

    # ì„¼ì„œ ì»¬ëŸ¼ í™•ì¸
    if not all(col in df.columns for col in SENSOR_COLS):
        return None

    # Highlighted ë§ˆì»¤ ì¸ë±ìŠ¤
    highlighted_indices = df[df['Highlighted'] == True].index.tolist()
    num_markers = len(highlighted_indices)

    if num_markers < 2:  # ìµœì†Œ 2ê°œ ë§ˆì»¤ í•„ìš”
        return None

    # ë§ˆì»¤ ì¢Œí‘œ ê³„ì‚°
    start_pos = node_positions[start_node]
    end_pos = node_positions[end_node]
    marker_coords = calculate_marker_coordinates(start_pos, end_pos, num_markers)

    sequences = []
    labels = []
    coords_list = []

    # ì—°ì†ëœ ë§ˆì»¤ ìŒ ìˆœíšŒ
    for i in range(len(highlighted_indices) - 1):
        marker_idx_A = highlighted_indices[i]
        marker_idx_B = highlighted_indices[i + 1]
        coord_A = marker_coords[i]
        coord_B = marker_coords[i + 1]

        # ë§ˆì»¤ Aì™€ B ì‚¬ì´ì˜ ëª¨ë“  ìƒ˜í”Œ ìˆœíšŒ (Sliding Window with STRIDE=5)
        for center_idx in range(marker_idx_A, marker_idx_B, STRIDE):
            # Window ë²”ìœ„
            start_idx = center_idx - WINDOW_SIZE
            end_idx = center_idx

            # ë²”ìœ„ ì²´í¬
            if start_idx < 0:
                continue
            if end_idx > len(df):
                break

            # ì„¼ì„œ ë°ì´í„° ì¶”ì¶œ
            seq = df.iloc[start_idx:end_idx][SENSOR_COLS].values

            # ê¸¸ì´ ì²´í¬
            if len(seq) != WINDOW_SIZE:
                continue

            # í˜„ì¬ ìƒ˜í”Œì˜ ìœ„ì¹˜ ë³´ê°„
            x, y = interpolate_position(coord_A, coord_B,
                                        marker_idx_A, marker_idx_B,
                                        center_idx)

            # ê·¸ë¦¬ë“œ ID
            grid_id = coord_to_grid(x, y)

            # ì›ë³¸ ë°ì´í„° ì¶”ê°€
            sequences.append(seq)
            labels.append(grid_id)
            coords_list.append((x, y))

            # ë°ì´í„° ì¦ê°• (í™•ë¥ ì )
            if np.random.random() < AUGMENT_PROB:
                seq_aug = augment_sequence(seq)
                sequences.append(seq_aug)
                labels.append(grid_id)
                coords_list.append((x, y))

    if len(sequences) == 0:
        return None

    return {
        'sequences': np.array(sequences),
        'labels': np.array(labels),
        'coords': np.array(coords_list),
        'route': f"{start_node}â†’{end_node}",
    }


# ============================================================================
# 4ë‹¨ê³„: ì „ì²´ ë°ì´í„°ì…‹ ìƒì„±
# ============================================================================

print("\n[2/7] ë°ì´í„° íŒŒì¼ ì²˜ë¦¬ (Stride=5 + ì¦ê°•)...")

PROJECT_ROOT = Path(__file__).resolve().parents[2]
data_dir = PROJECT_ROOT / 'law_data'
files = sorted(list(data_dir.glob('*.csv')))

all_data = []
grid_stats = defaultdict(int)

np.random.seed(42)  # ì¬í˜„ì„±

for filepath in tqdm(files, desc="Processing files"):
    result = process_file_v3(filepath)
    if result is not None:
        all_data.append(result)

        for label in result['labels']:
            grid_stats[label] += 1

total_samples = sum(len(d['sequences']) for d in all_data)

print(f"\n  ì²˜ë¦¬ëœ íŒŒì¼: {len(all_data)}/{len(files)}")
print(f"  ê³ ìœ  ê·¸ë¦¬ë“œ ì…€: {len(grid_stats)}")
print(f"  ì´ ìƒ˜í”Œ ìˆ˜: {total_samples:,}")
print(f"  ì¦ê°€ìœ¨: {total_samples / 12179:.1f}ë°° (v1 ëŒ€ë¹„)")

# ============================================================================
# 5ë‹¨ê³„: ë°ì´í„° í†µí•© ë° ì •ê·œí™”
# ============================================================================

print("\n[3/7] ë°ì´í„° í†µí•© ë° ì •ê·œí™”...")

all_sequences = np.vstack([d['sequences'] for d in all_data])
all_labels = np.concatenate([d['labels'] for d in all_data])
all_coords = np.vstack([d['coords'] for d in all_data])

print(f"  í†µí•© ë°ì´í„° shape: {all_sequences.shape}")
print(f"  ë¼ë²¨ shape: {all_labels.shape}")

# ì •ê·œí™”
mean = all_sequences.mean(axis=(0, 1))
std = all_sequences.std(axis=(0, 1))

print(f"\n  ì •ê·œí™” íŒŒë¼ë¯¸í„°:")
for i, col in enumerate(SENSOR_COLS):
    print(f"    {col}: mean={mean[i]:.4f}, std={std[i]:.4f}")

all_sequences_norm = (all_sequences - mean) / (std + 1e-8)

# ============================================================================
# 6ë‹¨ê³„: Train/Val/Test ë¶„í• 
# ============================================================================

print("\n[4/7] Train/Val/Test ë¶„í• ...")

# ê²½ë¡œë³„ ê·¸ë£¹í™”
route_groups = defaultdict(list)
for i, data in enumerate(all_data):
    route_groups[data['route']].append(i)

routes = list(route_groups.keys())
np.random.seed(42)
np.random.shuffle(routes)

n_routes = len(routes)
n_train = int(0.7 * n_routes)
n_val = int(0.15 * n_routes)

train_routes = routes[:n_train]
val_routes = routes[n_train:n_train+n_val]
test_routes = routes[n_train+n_val:]

print(f"  Train ê²½ë¡œ: {len(train_routes)}")
print(f"  Val ê²½ë¡œ: {len(val_routes)}")
print(f"  Test ê²½ë¡œ: {len(test_routes)}")

# ì¸ë±ìŠ¤ ìˆ˜ì§‘
train_indices = []
val_indices = []
test_indices = []

idx_offset = 0
for data in all_data:
    route = data['route']
    n_samples = len(data['sequences'])
    indices = list(range(idx_offset, idx_offset + n_samples))

    if route in train_routes:
        train_indices.extend(indices)
    elif route in val_routes:
        val_indices.extend(indices)
    else:
        test_indices.extend(indices)

    idx_offset += n_samples

print(f"\n  Train ìƒ˜í”Œ: {len(train_indices):,}")
print(f"  Val ìƒ˜í”Œ: {len(val_indices):,}")
print(f"  Test ìƒ˜í”Œ: {len(test_indices):,}")

# ============================================================================
# 7ë‹¨ê³„: í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„
# ============================================================================

print("\n[5/7] í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„...")

y_train = all_labels[train_indices]
unique, counts = np.unique(y_train, return_counts=True)

print(f"  Train í´ë˜ìŠ¤ ìˆ˜: {len(unique):,}")
print(f"  í´ë˜ìŠ¤ë‹¹ í‰ê·  ìƒ˜í”Œ: {counts.mean():.1f}ê°œ")
print(f"  í´ë˜ìŠ¤ë‹¹ ìµœì†Œ: {counts.min()}ê°œ")
print(f"  í´ë˜ìŠ¤ë‹¹ ìµœëŒ€: {counts.max()}ê°œ")

# ìƒ˜í”Œ ìˆ˜ë³„ ë¶„í¬
bins = [1, 5, 10, 20, 50, 100, 200]
print(f"\n  ìƒ˜í”Œ ìˆ˜ë³„ í´ë˜ìŠ¤ ë¶„í¬:")
for i in range(len(bins)-1):
    count = np.sum((counts >= bins[i]) & (counts < bins[i+1]))
    print(f"    {bins[i]:3d}-{bins[i+1]:3d}ê°œ: {count:4d} í´ë˜ìŠ¤")
count = np.sum(counts >= bins[-1])
print(f"    {bins[-1]:3d}+ê°œ:   {count:4d} í´ë˜ìŠ¤")

# ============================================================================
# 8ë‹¨ê³„: í´ë˜ìŠ¤ í•„í„°ë§ (ì„ íƒì )
# ============================================================================

print("\n[6/7] í´ë˜ìŠ¤ í•„í„°ë§...")

MIN_SAMPLES = 10  # ìµœì†Œ 10ê°œ ìƒ˜í”Œ ì´ìƒ í´ë˜ìŠ¤ë§Œ ìœ ì§€

# ì¶©ë¶„í•œ ìƒ˜í”Œì´ ìˆëŠ” í´ë˜ìŠ¤ë§Œ ì„ íƒ
valid_classes = unique[counts >= MIN_SAMPLES]
print(f"  í•„í„°ë§ ì „ í´ë˜ìŠ¤: {len(unique)}")
print(f"  í•„í„°ë§ í›„ í´ë˜ìŠ¤: {len(valid_classes)} (>= {MIN_SAMPLES} ìƒ˜í”Œ)")

# í•„í„°ë§ëœ ì¸ë±ìŠ¤
filtered_train_indices = [i for i in train_indices if all_labels[i] in valid_classes]
filtered_val_indices = [i for i in val_indices if all_labels[i] in valid_classes]
filtered_test_indices = [i for i in test_indices if all_labels[i] in valid_classes]

print(f"\n  í•„í„°ë§ í›„ ìƒ˜í”Œ:")
print(f"    Train: {len(filtered_train_indices):,} ({len(filtered_train_indices)/len(train_indices)*100:.1f}% ìœ ì§€)")
print(f"    Val: {len(filtered_val_indices):,} ({len(filtered_val_indices)/len(val_indices)*100:.1f}% ìœ ì§€)")
print(f"    Test: {len(filtered_test_indices):,} ({len(filtered_test_indices)/len(test_indices)*100:.1f}% ìœ ì§€)")

# í´ë˜ìŠ¤ ID ì¬ë§¤í•‘
class_mapping = {old_id: new_id for new_id, old_id in enumerate(valid_classes)}
all_labels_remapped = np.array([class_mapping.get(label, -1) for label in all_labels])

# ìµœì¢… í´ë˜ìŠ¤ ë¶„í¬
y_train_final = all_labels_remapped[filtered_train_indices]
unique_final, counts_final = np.unique(y_train_final, return_counts=True)

print(f"\n  ìµœì¢… í´ë˜ìŠ¤ ë¶„í¬:")
print(f"    í´ë˜ìŠ¤ ìˆ˜: {len(unique_final)}")
print(f"    í‰ê·  ìƒ˜í”Œ: {counts_final.mean():.1f}ê°œ")
print(f"    ìµœì†Œ ìƒ˜í”Œ: {counts_final.min()}ê°œ")
print(f"    ìµœëŒ€ ìƒ˜í”Œ: {counts_final.max()}ê°œ")

# ============================================================================
# 9ë‹¨ê³„: ë°ì´í„°ì…‹ ì €ì¥
# ============================================================================

print("\n[7/7] ë°ì´í„°ì…‹ ì €ì¥...")

output_dir = Path('processed_data_v3')
output_dir.mkdir(exist_ok=True)

np.save(output_dir / 'X_train.npy', all_sequences_norm[filtered_train_indices])
np.save(output_dir / 'y_train.npy', all_labels_remapped[filtered_train_indices])
np.save(output_dir / 'coords_train.npy', all_coords[filtered_train_indices])

np.save(output_dir / 'X_val.npy', all_sequences_norm[filtered_val_indices])
np.save(output_dir / 'y_val.npy', all_labels_remapped[filtered_val_indices])
np.save(output_dir / 'coords_val.npy', all_coords[filtered_val_indices])

np.save(output_dir / 'X_test.npy', all_sequences_norm[filtered_test_indices])
np.save(output_dir / 'y_test.npy', all_labels_remapped[filtered_test_indices])
np.save(output_dir / 'coords_test.npy', all_coords[filtered_test_indices])

metadata = {
    'window_size': WINDOW_SIZE,
    'stride': STRIDE,
    'grid_size': GRID_SIZE,
    'sensor_cols': SENSOR_COLS,
    'mean': mean,
    'std': std,
    'num_classes': len(valid_classes),
    'class_mapping': class_mapping,
    'valid_classes': valid_classes.tolist(),
    'grid_stats': dict(grid_stats),
    'x_range': (x_min, x_max),
    'y_range': (y_min, y_max),
    'augmentation': {
        'augment_prob': AUGMENT_PROB,
        'mag_noise_std': MAG_NOISE_STD,
        'orientation_noise_std': ORIENTATION_NOISE_STD,
    }
}

with open(output_dir / 'metadata.pkl', 'wb') as f:
    pickle.dump(metadata, f)

print(f"  ì €ì¥ ì™„ë£Œ: {output_dir}")

# ============================================================================
# 10ë‹¨ê³„: ê²€ì¦
# ============================================================================

X_train = np.load(output_dir / 'X_train.npy')
y_train = np.load(output_dir / 'y_train.npy')

print("\n" + "="*70)
print("âœ… ì „ì²˜ë¦¬ v3 ì™„ë£Œ!")
print("="*70)
print(f"""
ğŸ“Š ìµœì¢… ë°ì´í„°ì…‹:
  Train: {len(filtered_train_indices):,} ìƒ˜í”Œ
  Val:   {len(filtered_val_indices):,} ìƒ˜í”Œ
  Test:  {len(filtered_test_indices):,} ìƒ˜í”Œ
  ì´í•©:  {len(filtered_train_indices) + len(filtered_val_indices) + len(filtered_test_indices):,} ìƒ˜í”Œ

  ì¦ê°€ìœ¨: {total_samples / 12179:.1f}ë°° (v1 ëŒ€ë¹„)
  í•„í„°ë§ í›„: {(len(filtered_train_indices) + len(filtered_val_indices) + len(filtered_test_indices)) / 12179:.1f}ë°°

  í´ë˜ìŠ¤: {len(valid_classes):,}ê°œ
  í´ë˜ìŠ¤ë‹¹ í‰ê· : {counts_final.mean():.1f}ê°œ ìƒ˜í”Œ

  ì…ë ¥ shape: {X_train.shape}

ì €ì¥ ìœ„ì¹˜: {output_dir}/

ğŸ¯ v3 ê°œì„  íš¨ê³¼:
  âœ… Stride ì¶•ì†Œ (10 â†’ 5): ìƒ˜í”Œ 2ë°° ì¦ê°€
  âœ… ë°ì´í„° ì¦ê°• (30%): ì¶”ê°€ 1.3ë°° ì¦ê°€
  âœ… í´ë˜ìŠ¤ í•„í„°ë§: í•™ìŠµ ê°€ëŠ¥í•œ í´ë˜ìŠ¤ë§Œ ìœ ì§€
  âœ… ìµœì¢… í´ë˜ìŠ¤ë‹¹ í‰ê·  {counts_final.mean():.1f}ê°œ ìƒ˜í”Œ
  âœ… ì „ì²´ ë°ì´í„° í€„ë¦¬í‹° í–¥ìƒ

ì´ì œ LSTM ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€
""")
