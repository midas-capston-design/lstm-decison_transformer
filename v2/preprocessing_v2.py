#!/usr/bin/env python3
"""
ì§€ìê¸° ê¸°ë°˜ ì‹¤ë‚´ ìœ„ì¹˜ ì¶”ì • - ë°ì´í„° ì „ì²˜ë¦¬ v2
ë§ˆì»¤ ì‚¬ì´ ë°ì´í„°ë„ ëª¨ë‘ í™œìš© (Sliding Window)
"""
import pandas as pd
import numpy as np
import os
from pathlib import Path
import pickle
from tqdm import tqdm
from collections import defaultdict

# ì„¤ì •
WINDOW_SIZE = 100  # ìƒ˜í”Œ
STRIDE = 10        # Sliding window stride (10ìƒ˜í”Œ = 0.2ì´ˆ ê°„ê²©)
GRID_SIZE = 0.45   # m
SENSOR_COLS = ['MagX', 'MagY', 'MagZ', 'Pitch', 'Roll', 'Yaw']

print("="*70)
print("ğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ v2 ì‹œì‘ (ë§ˆì»¤ ì‚¬ì´ ë°ì´í„° í¬í•¨)")
print("="*70)
print(f"  Window: {WINDOW_SIZE} ìƒ˜í”Œ (2ì´ˆ @ 50Hz)")
print(f"  Stride: {STRIDE} ìƒ˜í”Œ (0.2ì´ˆ)")
print(f"  Grid: {GRID_SIZE}m")

# ============================================================================
# 1ë‹¨ê³„: ë…¸ë“œ ì •ë³´ ë¡œë“œ
# ============================================================================
print("\n[1/6] ë…¸ë“œ ì •ë³´ ë¡œë“œ...")
nodes_df = pd.read_csv('nodes_final.csv')
node_positions = {row['id']: (row['x_m'], row['y_m'])
                  for _, row in nodes_df.iterrows()}

# ê±´ë¬¼ ë²”ìœ„ ê³„ì‚°
x_coords = [pos[0] for pos in node_positions.values()]
y_coords = [pos[1] for pos in node_positions.values()]
x_min, x_max = min(x_coords), max(x_coords)
y_min, y_max = min(y_coords), max(y_coords)

print(f"  ê±´ë¬¼ ë²”ìœ„: x=[{x_min}, {x_max}], y=[{y_min}, {y_max}]")

# ============================================================================
# 2ë‹¨ê³„: ê·¸ë¦¬ë“œ ë§¤í•‘ í•¨ìˆ˜
# ============================================================================

def coord_to_grid(x, y):
    """ì ˆëŒ€ ì¢Œí‘œë¥¼ ê·¸ë¦¬ë“œ IDë¡œ ë³€í™˜"""
    grid_x = int(round((x - x_min) / GRID_SIZE))
    grid_y = int(round((y - y_min) / GRID_SIZE))

    num_x_grids = int(np.ceil((x_max - x_min) / GRID_SIZE)) + 1
    num_y_grids = int(np.ceil((y_max - y_min) / GRID_SIZE)) + 1

    grid_x = max(0, min(grid_x, num_x_grids - 1))
    grid_y = max(0, min(grid_y, num_y_grids - 1))

    grid_id = grid_y * num_x_grids + grid_x
    return grid_id


def calculate_marker_coordinates(start_pos, end_pos, num_markers):
    """ì‹œì‘ì ì—ì„œ ëì ê¹Œì§€ num_markers ê°œì˜ ì¢Œí‘œ ìƒì„±"""
    coords = []
    dx = end_pos[0] - start_pos[0]
    dy = end_pos[1] - start_pos[1]

    for i in range(num_markers):
        progress = i / (num_markers - 1) if num_markers > 1 else 0
        x = start_pos[0] + dx * progress
        y = start_pos[1] + dy * progress
        coords.append((x, y))

    return coords


def interpolate_position(pos1, pos2, t1, t2, t_current):
    """
    ë‘ ë§ˆì»¤ ì‚¬ì´ì—ì„œ í˜„ì¬ ìƒ˜í”Œì˜ ìœ„ì¹˜ë¥¼ ì„ í˜• ë³´ê°„

    Args:
        pos1, pos2: ë§ˆì»¤ A, Bì˜ ì¢Œí‘œ
        t1, t2: ë§ˆì»¤ A, Bì˜ ì¸ë±ìŠ¤
        t_current: í˜„ì¬ ìƒ˜í”Œ ì¸ë±ìŠ¤
    """
    if t2 == t1:
        return pos1

    progress = (t_current - t1) / (t2 - t1)
    progress = max(0, min(1, progress))  # 0-1ë¡œ í´ë¦¬í•‘

    x = pos1[0] + (pos2[0] - pos1[0]) * progress
    y = pos1[1] + (pos2[1] - pos1[1]) * progress

    return (x, y)


# ============================================================================
# 3ë‹¨ê³„: íŒŒì¼ë³„ ì²˜ë¦¬ (ë§ˆì»¤ ì‚¬ì´ ë°ì´í„° í¬í•¨)
# ============================================================================

def process_file_v2(filepath):
    """
    í•˜ë‚˜ì˜ ê²½ë¡œ íŒŒì¼ ì²˜ë¦¬ (ë§ˆì»¤ ì‚¬ì´ ë°ì´í„° í¬í•¨)

    Returns:
        sequences: (N, 100, 6)
        labels: (N,)
        coords: (N, 2)
    """
    filename = os.path.basename(filepath)
    parts = filename.replace('.csv', '').split('_')

    if len(parts) != 3:
        return None

    start_node = int(parts[0])
    end_node = int(parts[1])

    if start_node not in node_positions or end_node not in node_positions:
        return None

    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(filepath)

    # ì„¼ì„œ ì»¬ëŸ¼ í™•ì¸
    if not all(col in df.columns for col in SENSOR_COLS):
        return None

    # Highlighted ë§ˆì»¤ ì¸ë±ìŠ¤
    highlighted_indices = df[df['Highlighted'] == True].index.tolist()
    num_markers = len(highlighted_indices)

    if num_markers < 2:  # ìµœì†Œ 2ê°œ ë§ˆì»¤ í•„ìš”
        return None

    # ë§ˆì»¤ ì¢Œí‘œ ê³„ì‚°
    start_pos = node_positions[start_node]
    end_pos = node_positions[end_node]
    marker_coords = calculate_marker_coordinates(start_pos, end_pos, num_markers)

    sequences = []
    labels = []
    coords_list = []

    # ì—°ì†ëœ ë§ˆì»¤ ìŒ ìˆœíšŒ
    for i in range(len(highlighted_indices) - 1):
        marker_idx_A = highlighted_indices[i]
        marker_idx_B = highlighted_indices[i + 1]
        coord_A = marker_coords[i]
        coord_B = marker_coords[i + 1]

        # ë§ˆì»¤ Aì™€ B ì‚¬ì´ì˜ ëª¨ë“  ìƒ˜í”Œ ìˆœíšŒ (Sliding Window)
        for center_idx in range(marker_idx_A, marker_idx_B, STRIDE):
            # Window ë²”ìœ„
            start_idx = center_idx - WINDOW_SIZE
            end_idx = center_idx

            # ë²”ìœ„ ì²´í¬
            if start_idx < 0:
                continue
            if end_idx > len(df):
                break

            # ì„¼ì„œ ë°ì´í„° ì¶”ì¶œ
            seq = df.iloc[start_idx:end_idx][SENSOR_COLS].values

            # ê¸¸ì´ ì²´í¬
            if len(seq) != WINDOW_SIZE:
                continue

            # í˜„ì¬ ìƒ˜í”Œì˜ ìœ„ì¹˜ ë³´ê°„
            x, y = interpolate_position(coord_A, coord_B,
                                        marker_idx_A, marker_idx_B,
                                        center_idx)

            # ê·¸ë¦¬ë“œ ID
            grid_id = coord_to_grid(x, y)

            sequences.append(seq)
            labels.append(grid_id)
            coords_list.append((x, y))

    if len(sequences) == 0:
        return None

    return {
        'sequences': np.array(sequences),
        'labels': np.array(labels),
        'coords': np.array(coords_list),
        'route': f"{start_node}â†’{end_node}",
    }


# ============================================================================
# 4ë‹¨ê³„: ì „ì²´ ë°ì´í„°ì…‹ ìƒì„±
# ============================================================================

print("\n[2/6] ë°ì´í„° íŒŒì¼ ì²˜ë¦¬ (ë§ˆì»¤ ì‚¬ì´ í¬í•¨)...")

data_dir = Path('law_data')
files = sorted(list(data_dir.glob('*.csv')))

all_data = []
grid_stats = defaultdict(int)

for filepath in tqdm(files, desc="Processing files"):
    result = process_file_v2(filepath)
    if result is not None:
        all_data.append(result)

        for label in result['labels']:
            grid_stats[label] += 1

total_samples = sum(len(d['sequences']) for d in all_data)

print(f"\n  ì²˜ë¦¬ëœ íŒŒì¼: {len(all_data)}/{len(files)}")
print(f"  ê³ ìœ  ê·¸ë¦¬ë“œ ì…€: {len(grid_stats)}")
print(f"  ì´ ìƒ˜í”Œ ìˆ˜: {total_samples:,}")
print(f"  ì¦ê°€ìœ¨: {total_samples / 12179:.1f}ë°° (ì´ì „ ëŒ€ë¹„)")

# ============================================================================
# 5ë‹¨ê³„: ë°ì´í„° í†µí•© ë° ì •ê·œí™”
# ============================================================================

print("\n[3/6] ë°ì´í„° í†µí•© ë° ì •ê·œí™”...")

all_sequences = np.vstack([d['sequences'] for d in all_data])
all_labels = np.concatenate([d['labels'] for d in all_data])
all_coords = np.vstack([d['coords'] for d in all_data])

print(f"  í†µí•© ë°ì´í„° shape: {all_sequences.shape}")
print(f"  ë¼ë²¨ shape: {all_labels.shape}")

# ì •ê·œí™”
mean = all_sequences.mean(axis=(0, 1))
std = all_sequences.std(axis=(0, 1))

print(f"\n  ì •ê·œí™” íŒŒë¼ë¯¸í„°:")
for i, col in enumerate(SENSOR_COLS):
    print(f"    {col}: mean={mean[i]:.4f}, std={std[i]:.4f}")

all_sequences_norm = (all_sequences - mean) / (std + 1e-8)

# ============================================================================
# 6ë‹¨ê³„: Train/Val/Test ë¶„í• 
# ============================================================================

print("\n[4/6] Train/Val/Test ë¶„í• ...")

# ê²½ë¡œë³„ ê·¸ë£¹í™”
route_groups = defaultdict(list)
for i, data in enumerate(all_data):
    route_groups[data['route']].append(i)

routes = list(route_groups.keys())
np.random.seed(42)
np.random.shuffle(routes)

n_routes = len(routes)
n_train = int(0.7 * n_routes)
n_val = int(0.15 * n_routes)

train_routes = routes[:n_train]
val_routes = routes[n_train:n_train+n_val]
test_routes = routes[n_train+n_val:]

print(f"  Train ê²½ë¡œ: {len(train_routes)}")
print(f"  Val ê²½ë¡œ: {len(val_routes)}")
print(f"  Test ê²½ë¡œ: {len(test_routes)}")

# ì¸ë±ìŠ¤ ìˆ˜ì§‘
train_indices = []
val_indices = []
test_indices = []

idx_offset = 0
for data in all_data:
    route = data['route']
    n_samples = len(data['sequences'])
    indices = list(range(idx_offset, idx_offset + n_samples))

    if route in train_routes:
        train_indices.extend(indices)
    elif route in val_routes:
        val_indices.extend(indices)
    else:
        test_indices.extend(indices)

    idx_offset += n_samples

print(f"\n  Train ìƒ˜í”Œ: {len(train_indices):,}")
print(f"  Val ìƒ˜í”Œ: {len(val_indices):,}")
print(f"  Test ìƒ˜í”Œ: {len(test_indices):,}")

# ============================================================================
# 7ë‹¨ê³„: í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„
# ============================================================================

print("\n[5/6] í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„...")

y_train = all_labels[train_indices]
unique, counts = np.unique(y_train, return_counts=True)

print(f"  Train í´ë˜ìŠ¤ ìˆ˜: {len(unique):,}")
print(f"  í´ë˜ìŠ¤ë‹¹ í‰ê·  ìƒ˜í”Œ: {counts.mean():.1f}ê°œ")
print(f"  í´ë˜ìŠ¤ë‹¹ ìµœì†Œ: {counts.min()}ê°œ")
print(f"  í´ë˜ìŠ¤ë‹¹ ìµœëŒ€: {counts.max()}ê°œ")

# ìƒ˜í”Œ ìˆ˜ë³„ ë¶„í¬
bins = [1, 5, 10, 20, 50, 100, 200]
print(f"\n  ìƒ˜í”Œ ìˆ˜ë³„ í´ë˜ìŠ¤ ë¶„í¬:")
for i in range(len(bins)-1):
    count = np.sum((counts >= bins[i]) & (counts < bins[i+1]))
    print(f"    {bins[i]:3d}-{bins[i+1]:3d}ê°œ: {count:4d} í´ë˜ìŠ¤")
count = np.sum(counts >= bins[-1])
print(f"    {bins[-1]:3d}+ê°œ:   {count:4d} í´ë˜ìŠ¤")

# ============================================================================
# 8ë‹¨ê³„: ë°ì´í„°ì…‹ ì €ì¥
# ============================================================================

print("\n[6/6] ë°ì´í„°ì…‹ ì €ì¥...")

output_dir = Path('processed_data_v2')
output_dir.mkdir(exist_ok=True)

np.save(output_dir / 'X_train.npy', all_sequences_norm[train_indices])
np.save(output_dir / 'y_train.npy', all_labels[train_indices])
np.save(output_dir / 'coords_train.npy', all_coords[train_indices])

np.save(output_dir / 'X_val.npy', all_sequences_norm[val_indices])
np.save(output_dir / 'y_val.npy', all_labels[val_indices])
np.save(output_dir / 'coords_val.npy', all_coords[val_indices])

np.save(output_dir / 'X_test.npy', all_sequences_norm[test_indices])
np.save(output_dir / 'y_test.npy', all_labels[test_indices])
np.save(output_dir / 'coords_test.npy', all_coords[test_indices])

metadata = {
    'window_size': WINDOW_SIZE,
    'stride': STRIDE,
    'grid_size': GRID_SIZE,
    'sensor_cols': SENSOR_COLS,
    'mean': mean,
    'std': std,
    'num_classes': len(grid_stats),
    'grid_stats': dict(grid_stats),
    'x_range': (x_min, x_max),
    'y_range': (y_min, y_max),
}

with open(output_dir / 'metadata.pkl', 'wb') as f:
    pickle.dump(metadata, f)

print(f"  ì €ì¥ ì™„ë£Œ: {output_dir}")

# ============================================================================
# 9ë‹¨ê³„: ê²€ì¦
# ============================================================================

X_train = np.load(output_dir / 'X_train.npy')
y_train = np.load(output_dir / 'y_train.npy')

print("\n" + "="*70)
print("âœ… ì „ì²˜ë¦¬ v2 ì™„ë£Œ!")
print("="*70)
print(f"""
ğŸ“Š ìµœì¢… ë°ì´í„°ì…‹:
  Train: {len(train_indices):,} ìƒ˜í”Œ
  Val:   {len(val_indices):,} ìƒ˜í”Œ
  Test:  {len(test_indices):,} ìƒ˜í”Œ
  ì´í•©:  {total_samples:,} ìƒ˜í”Œ

  ì¦ê°€ìœ¨: {total_samples / 12179:.1f}ë°° (v1 ëŒ€ë¹„)

  í´ë˜ìŠ¤: {len(unique):,}ê°œ
  í´ë˜ìŠ¤ë‹¹ í‰ê· : {counts.mean():.1f}ê°œ ìƒ˜í”Œ

  ì…ë ¥ shape: {X_train.shape}

ì €ì¥ ìœ„ì¹˜: {output_dir}/

ğŸ¯ ê°œì„  íš¨ê³¼:
  âœ… ë°ì´í„° 10ë°° ì¦ê°€
  âœ… í´ë˜ìŠ¤ë‹¹ ìƒ˜í”Œ ìˆ˜ ì¦ê°€ (5.2 â†’ {counts.mean():.1f})
  âœ… í•™ìŠµ ê°€ëŠ¥ì„± ëŒ€í­ í–¥ìƒ
  âœ… ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ì™€ ë™ì¼

ì´ì œ LSTM ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€
""")
