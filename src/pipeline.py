#!/usr/bin/env python3
"""
í•˜ë‚˜ì˜ ìŠ¤í¬ë¦½íŠ¸ë¡œ ë°ì´í„° ì „ì²˜ë¦¬ + Hyena í•™ìŠµì„ ì²˜ë¦¬í•œë‹¤.

ì‚¬ìš© ì˜ˆ:
  - ë°ì´í„° ì¤€ë¹„:  python pipeline.py preprocess --law-dir law_data --nodes nodes_final.csv --output data
  - í•™ìŠµ:        python pipeline.py train --data-dir data --nodes nodes_final.csv --epochs 50
"""
from __future__ import annotations

import argparse
import csv
import json
import math
import random
from collections import defaultdict
from pathlib import Path
from typing import Dict, Iterable, List, Tuple

import numpy as np
import pywt  # Wavelet transform
import torch
import torch.nn.functional as F
from torch import nn
from torch.utils.data import DataLoader, Dataset

# -----------------------------
# ì „ì²˜ë¦¬ ê´€ë ¨ ìƒìˆ˜
# -----------------------------
WINDOW_SIZE = 250
STRIDE = 50
GRID_SIZE = 0.45  # ëª©í‘œ ì •í™•ë„ì— ë§ì¶¤ (0.9 â†’ 0.45)
STEP_DISTANCE = 0.45
BUTTON_COLUMNS = ("Highlighted", "RightAngle")
CONNECTION_THRESHOLD = 5.0
WRONG_CONNECTIONS = {frozenset((10, 28)), frozenset((24, 25))}
TRAIN_RATIO = 0.6
VAL_RATIO = 0.2
TEST_RATIO = 0.2
TARGET_THRESHOLD = 1.35
BASE_MAG = (-33.0, -15.0, -42.0)  # ì§€ìê¸° í‰ê· ê°’ (ì •ê·œí™” ê¸°ì¤€)

# ì¢Œí‘œ ì •ê·œí™” ë²”ìœ„ (nodes_final.csv ê¸°ì¤€)
COORD_BOUNDS = {
    "min_x": -85.5,
    "max_x": 0.0,
    "min_y": -9.0,
    "max_y": 9.0,
}

# -----------------------------
# ê³µí†µ ìœ í‹¸
# -----------------------------


def read_nodes(path: Path) -> Dict[int, Tuple[float, float]]:
    with path.open() as f:
        reader = csv.DictReader(f)
        return {int(row["id"]): (float(row["x_m"]), float(row["y_m"])) for row in reader}


def build_graph(nodes: Dict[int, Tuple[float, float]]) -> Dict[int, List[Tuple[int, float]]]:
    adj = {node: [] for node in nodes}
    node_items = list(nodes.items())
    for i, (id1, (x1, y1)) in enumerate(node_items):
        for id2, (x2, y2) in node_items[i + 1 :]:
            dist = math.hypot(x1 - x2, y1 - y2)
            if dist <= CONNECTION_THRESHOLD and frozenset((id1, id2)) not in WRONG_CONNECTIONS:
                adj[id1].append((id2, dist))
                adj[id2].append((id1, dist))
    return adj


def shortest_path(adj: Dict[int, List[Tuple[int, float]]], start: int, end: int) -> List[int] | None:
    import heapq

    if start == end:
        return [start]
    dist = {start: 0.0}
    prev = {}
    pq = [(0.0, start)]
    visited = set()
    while pq:
        d, node = heapq.heappop(pq)
        if node in visited:
            continue
        visited.add(node)
        if node == end:
            break
        for neigh, w in adj[node]:
            nd = d + w
            if neigh not in dist or nd < dist[neigh]:
                dist[neigh] = nd
                prev[neigh] = node
                heapq.heappush(pq, (nd, neigh))
    if end not in dist:
        return None
    path = [end]
    while path[-1] != start:
        path.append(prev[path[-1]])
    path.reverse()
    return path


def build_segments(path_nodes: List[int], positions: Dict[int, Tuple[float, float]]):
    segments = []
    total = 0.0
    for n1, n2 in zip(path_nodes, path_nodes[1:]):
        p1 = positions[n1]
        p2 = positions[n2]
        dist = math.hypot(p1[0] - p2[0], p1[1] - p2[1])
        segments.append((n1, n2, p1, p2, dist))
        total += dist
    return segments, total


def coordinate_at_distance(segments, distance):
    traversed = 0.0
    for n1, n2, p1, p2, dist in segments:
        if distance <= traversed + dist or (n1, n2) == segments[-1][:2]:
            seg_pos = min(max(distance - traversed, 0.0), dist)
            frac = 0.0 if dist == 0 else seg_pos / dist
            x = p1[0] + frac * (p2[0] - p1[0])
            y = p1[1] + frac * (p2[1] - p1[1])
            sub_idx = int(round(seg_pos / GRID_SIZE))
            return (x, y), (n1, n2, sub_idx)
        traversed += dist
    n1, n2, _, p2, last_dist = segments[-1]
    sub_idx = int(round(last_dist / GRID_SIZE))
    return p2, (n1, n2, sub_idx)


def quantize_coord(coord: Tuple[float, float]) -> Tuple[float, float]:
    x = round(round(coord[0] / GRID_SIZE) * GRID_SIZE, 6)
    y = round(round(coord[1] / GRID_SIZE) * GRID_SIZE, 6)
    return (x, y)


def normalize_mag(value: float, mean: float, std: float = 5.0) -> float:
    """Z-score ì •ê·œí™”ë¡œ ì •ë³´ ë³´ì¡´ (ê¸°ì¡´ tanhëŠ” ë¯¸ì„¸í•œ ë³€í™” ì†ì‹¤)"""
    return (value - mean) / std


def normalize_coord(x: float, y: float) -> Tuple[float, float]:
    """ì¢Œí‘œë¥¼ [0, 1] ë²”ìœ„ë¡œ ì •ê·œí™”"""
    norm_x = (x - COORD_BOUNDS["min_x"]) / (COORD_BOUNDS["max_x"] - COORD_BOUNDS["min_x"])
    norm_y = (y - COORD_BOUNDS["min_y"]) / (COORD_BOUNDS["max_y"] - COORD_BOUNDS["min_y"])
    return (norm_x, norm_y)


def denormalize_coord(norm_x: float, norm_y: float) -> Tuple[float, float]:
    """ì •ê·œí™”ëœ ì¢Œí‘œë¥¼ ì›ë˜ ë¯¸í„° ë‹¨ìœ„ë¡œ ë³µì›"""
    x = norm_x * (COORD_BOUNDS["max_x"] - COORD_BOUNDS["min_x"]) + COORD_BOUNDS["min_x"]
    y = norm_y * (COORD_BOUNDS["max_y"] - COORD_BOUNDS["min_y"]) + COORD_BOUNDS["min_y"]
    return (x, y)


def angle_to_feature(value: float) -> float:
    return math.sin(math.radians(value))


def wavelet_denoise(
    signal: np.ndarray,
    wavelet: str = "db4",
    level: int = 3,
    mode: str = "soft",
) -> np.ndarray:
    """
    ì›¨ì´ë¸”ë › ê¸°ë°˜ ì‹ í˜¸ ë””ë…¸ì´ì§• (Wavelet Denoising)

    Args:
        signal: 1D ì‹ í˜¸ (ì˜ˆ: MagX ì‹œê³„ì—´)
        wavelet: ì›¨ì´ë¸”ë › ì¢…ë¥˜ (db4=Daubechies 4, ë¶€ë“œëŸ¬ìš´ ì‹ í˜¸ì— ì í•©)
        level: ë¶„í•´ ë ˆë²¨ (3-5 ì¶”ì²œ, ë†’ì„ìˆ˜ë¡ ë” ë§ì€ ì£¼íŒŒìˆ˜ ëŒ€ì—­ ë¶„ë¦¬)
        mode: threshold ëª¨ë“œ ('soft'=ë¶€ë“œëŸ½ê²Œ, 'hard'=ê°•í•˜ê²Œ)

    Returns:
        ë””ë…¸ì´ì§•ëœ ì‹ í˜¸
    """
    # ì‹ í˜¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if len(signal) < 2 ** (level + 1):
        return signal

    # 1. ì›¨ì´ë¸”ë › ë¶„í•´ (Decomposition)
    coeffs = pywt.wavedec(signal, wavelet, level=level)

    # 2. ë…¸ì´ì¦ˆ ì¶”ì • (MAD - Median Absolute Deviation)
    # ê°€ì¥ ê³ ì£¼íŒŒ ì„±ë¶„(detail coefficients)ì—ì„œ ë…¸ì´ì¦ˆ ë ˆë²¨ ì¶”ì •
    sigma = np.median(np.abs(coeffs[-1])) / 0.6745

    # 3. Threshold ê³„ì‚° (Universal threshold)
    threshold = sigma * np.sqrt(2 * np.log(len(signal)))

    # 4. Thresholding (ë…¸ì´ì¦ˆ ì œê±°)
    # ì‘ì€ ê³„ìˆ˜ëŠ” ë…¸ì´ì¦ˆë¡œ ê°„ì£¼í•˜ì—¬ ì œê±°
    coeffs_thresh = [pywt.threshold(c, threshold, mode=mode) for c in coeffs]

    # 5. ì¬êµ¬ì„± (Reconstruction)
    denoised = pywt.waverec(coeffs_thresh, wavelet)

    # ê²½ê³„ íš¨ê³¼ë¡œ ê¸¸ì´ê°€ ì•½ê°„ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ
    return denoised[:len(signal)]


def wavelet_denoise_multivariate(
    signals: List[np.ndarray],
    wavelet: str = "db4",
    level: int = 3,
) -> List[np.ndarray]:
    """
    ì—¬ëŸ¬ ì„¼ì„œ ì‹ í˜¸ë¥¼ ë™ì‹œì— ë””ë…¸ì´ì§• (MagX, MagY, MagZ ë“±)
    """
    return [wavelet_denoise(sig, wavelet, level) for sig in signals]


def extract_button_distances(rows: List[List[str]], col_idx: Dict[str, int]) -> List[float] | None:
    if not any(col in col_idx for col in BUTTON_COLUMNS):
        return None

    def is_true(val: str) -> bool:
        return str(val).strip().lower() in ("1", "true")

    distances = []
    last_state = False
    steps = 0
    for row in rows:
        state = False
        for col in BUTTON_COLUMNS:
            if col in col_idx and is_true(row[col_idx[col]]):
                state = True
                break
        if state and not last_state:
            steps += 1
        distances.append(steps * STEP_DISTANCE)
        last_state = state

    if steps == 0:
        return None
    return distances


def process_csv(
    file_path: Path,
    positions: Dict[int, Tuple[float, float]],
    graph: Dict[int, List[Tuple[int, float]]],
    feature_mode: str = "full",
) -> Tuple[List[List[float]], List[Tuple[float, float]], List[str]]:
    with file_path.open() as f:
        reader = csv.reader(f)
        rows = list(reader)
    if len(rows) <= 1:
        return [], [], []
    header = rows[0]
    data = rows[1:]
    col_idx = {name: idx for idx, name in enumerate(header)}

    start, end, *_ = file_path.stem.split("_") + [None, None]
    start_node = int(start)
    end_node = int(end)
    path_nodes = shortest_path(graph, start_node, end_node)
    if path_nodes is None:
        print(f"âš ï¸  skip (ê¸¸ ì—†ìŒ): {file_path}")
        return [], [], []

    segments, total_dist = build_segments(path_nodes, positions)
    distances = extract_button_distances(data, col_idx)

    coords: List[Tuple[float, float]] = []
    tags: List[str] = []

    if len(segments) == 0 or total_dist == 0.0:
        base_coord = positions[path_nodes[0]]
        coords = [base_coord] * len(data)
        tags = [f"{start_node}->{end_node}"] * len(data)
    else:
        if distances is None or len(distances) != len(data):
            distances = [
                (i / max(1, len(data) - 1)) * total_dist for i in range(len(data))
            ]
        for dist in distances:
            coord, (n1, n2, _) = coordinate_at_distance(segments, dist)
            coords.append(coord)
            tags.append(f"{n1}->{n2}")

    # 1. ë¨¼ì € raw ì„¼ì„œ ë°ì´í„° ì¶”ì¶œ
    raw_magx = np.array([float(row[col_idx["MagX"]]) for row in data])
    raw_magy = np.array([float(row[col_idx["MagY"]]) for row in data])
    raw_magz = np.array([float(row[col_idx["MagZ"]]) for row in data])

    # 2. ì›¨ì´ë¸”ë › ë””ë…¸ì´ì§• ì ìš© (ë…¸ì´ì¦ˆ ì œê±°)
    clean_magx = wavelet_denoise(raw_magx, wavelet="db4", level=3)
    clean_magy = wavelet_denoise(raw_magy, wavelet="db4", level=3)
    clean_magz = wavelet_denoise(raw_magz, wavelet="db4", level=3)

    # 3. ì •ê·œí™” ë° íŠ¹ì§• ë³€í™˜
    features = []

    if feature_mode == "mag4":
        # ì§€ìê¸° 4ê°œ: MagX, MagY, MagZ, Magnitude
        for i in range(len(data)):
            magx = normalize_mag(clean_magx[i], BASE_MAG[0])
            magy = normalize_mag(clean_magy[i], BASE_MAG[1])
            magz = normalize_mag(clean_magz[i], BASE_MAG[2])
            # Magnitude ê³„ì‚° (ì›ë³¸ ê°’ ê¸°ì¤€)
            mag_magnitude = math.sqrt(clean_magx[i]**2 + clean_magy[i]**2 + clean_magz[i]**2)
            # Magnitudeë„ ì •ê·œí™” (í‰ê·  50 ê¸°ì¤€)
            mag_magnitude_norm = (mag_magnitude - 50.0) / 10.0
            feat = [magx, magy, magz, mag_magnitude_norm]
            features.append(feat)

    elif feature_mode == "full":
        # ê¸°ì¡´ 6ê°œ: MagX, MagY, MagZ, Pitch, Roll, Yaw
        raw_pitch = np.array([float(row[col_idx["Pitch"]]) for row in data])
        raw_roll = np.array([float(row[col_idx["Roll"]]) for row in data])
        raw_yaw = np.array([float(row[col_idx["Yaw"]]) for row in data])

        clean_pitch = wavelet_denoise(raw_pitch, wavelet="db4", level=2)
        clean_roll = wavelet_denoise(raw_roll, wavelet="db4", level=2)
        clean_yaw = wavelet_denoise(raw_yaw, wavelet="db4", level=2)

        for i in range(len(data)):
            magx = normalize_mag(clean_magx[i], BASE_MAG[0])
            magy = normalize_mag(clean_magy[i], BASE_MAG[1])
            magz = normalize_mag(clean_magz[i], BASE_MAG[2])
            pitch = angle_to_feature(clean_pitch[i])
            roll = angle_to_feature(clean_roll[i])
            yaw = angle_to_feature(clean_yaw[i])
            feat = [magx, magy, magz, pitch, roll, yaw]
            features.append(feat)

    else:
        raise ValueError(f"Unknown feature_mode: {feature_mode}")

    return features, coords, tags


def create_full_sequence(
    features: List[List[float]],
    coords: List[Tuple[float, float]],
    tags: List[str],
    csv_path: Path,
) -> Dict | None:
    """ì „ì²´ ê²½ë¡œë¥¼ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë¡œ ìƒì„± (seq2seq)"""
    if len(features) < 50:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
        return None

    # ì „ì²´ trajectoryì˜ ê³ ìœ  edge path (ë°©í–¥ì„± í¬í•¨)
    # ì˜ˆ: "1->2->3->...->11"
    start_end = csv_path.stem.split("_")[:2]
    edge_path = f"{start_end[0]}->{start_end[1]}"

    # ëª¨ë“  ì¢Œí‘œë¥¼ ì–‘ìí™” í›„ ì •ê·œí™”
    quantized_coords = [quantize_coord(c) for c in coords]
    normalized_coords = [normalize_coord(x, y) for x, y in quantized_coords]

    return {
        "features": features,  # (seq_len, 6)
        "targets": normalized_coords,  # (seq_len, 2) - ì •ê·œí™”ëœ ì¢Œí‘œ [0, 1]
        "edge_path": edge_path,  # "start->end"
        "seq_len": len(features),
    }


def generate_virtual_sequence(
    edge_path: str,
    positions: Dict[int, Tuple[float, float]],
    graph: Dict[int, List[Tuple[int, float]]],
    min_len: int = 500,
    max_len: int = 3000,
    feature_mode: str = "full",
) -> Dict | None:
    """ê°€ìƒì˜ ì „ì²´ ê²½ë¡œ ì‹œí€€ìŠ¤ ìƒì„± (seq2seq)"""
    start_node, end_node = map(int, edge_path.split("->"))
    if start_node not in positions or end_node not in positions:
        return None

    # ìµœë‹¨ ê²½ë¡œ ì°¾ê¸°
    path_nodes = shortest_path(graph, start_node, end_node)
    if path_nodes is None or len(path_nodes) < 2:
        return None

    # ê²½ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
    segments, total_dist = build_segments(path_nodes, positions)
    if total_dist == 0:
        return None

    # ëœë¤ ì‹œí€€ìŠ¤ ê¸¸ì´ (ì‹¤ì œ ì¸¡ì • ë°ì´í„°ì™€ ìœ ì‚¬í•˜ê²Œ)
    seq_len = random.randint(min_len, max_len)

    features = []
    coords = []

    # ì „ì²´ ê²½ë¡œë¥¼ ë”°ë¼ ì‹œë®¬ë ˆì´ì…˜
    for step in range(seq_len):
        frac = step / max(1, seq_len - 1)
        dist = frac * total_dist
        coord, _ = coordinate_at_distance(segments, dist)
        coords.append(coord)

        # í˜„ì¬ ìœ„ì¹˜ì—ì„œì˜ ë°©í–¥ ê³„ì‚° (ì´ì „ stepê³¼ ë¹„êµ)
        if step > 0:
            dx = coords[step][0] - coords[step - 1][0]
            dy = coords[step][1] - coords[step - 1][1]
            heading = math.atan2(dy, dx) if (dx != 0 or dy != 0) else 0
        else:
            # ì²« ìŠ¤í…ì€ ê²½ë¡œ ì „ì²´ ë°©í–¥ ì‚¬ìš©
            dx = positions[end_node][0] - positions[start_node][0]
            dy = positions[end_node][1] - positions[start_node][1]
            heading = math.atan2(dy, dx)

        heading_deg = math.degrees(heading)

        # ì„¼ì„œ ì‹œë®¬ë ˆì´ì…˜
        raw_magx = BASE_MAG[0] + 3 * math.cos(heading) + random.gauss(0, 1.5)
        raw_magy = BASE_MAG[1] + 3 * math.sin(heading) + random.gauss(0, 1.5)
        raw_magz = BASE_MAG[2] + random.gauss(0, 0.8)
        magx = normalize_mag(raw_magx, BASE_MAG[0])
        magy = normalize_mag(raw_magy, BASE_MAG[1])
        magz = normalize_mag(raw_magz, BASE_MAG[2])

        if feature_mode == "mag4":
            mag_magnitude = math.sqrt(raw_magx**2 + raw_magy**2 + raw_magz**2)
            mag_magnitude_norm = (mag_magnitude - 50.0) / 10.0
            features.append([magx, magy, magz, mag_magnitude_norm])
        elif feature_mode == "full":
            pitch = angle_to_feature(random.gauss(0, 3.0))
            roll = angle_to_feature(random.gauss(0, 3.0))
            yaw = angle_to_feature(heading_deg + random.gauss(0, 5.0))
            features.append([magx, magy, magz, pitch, roll, yaw])

    quantized_coords = [quantize_coord(c) for c in coords]
    normalized_coords = [normalize_coord(x, y) for x, y in quantized_coords]

    return {
        "features": features,
        "targets": normalized_coords,  # ì •ê·œí™”ëœ ì¢Œí‘œ [0, 1]
        "edge_path": edge_path,
        "seq_len": seq_len,
    }


def balance_with_virtual(
    samples: List[Dict],
    positions: Dict[int, Tuple[float, float]],
    graph: Dict[int, List[Tuple[int, float]]],
    min_samples_per_path: int = 3,
    feature_mode: str = "full",
) -> List[Dict]:
    """ë¶€ì¡±í•œ ê²½ë¡œì— ê°€ìƒ ì‹œí€€ìŠ¤ ì¶”ê°€ (seq2seq ë°©ì‹)"""
    counts = defaultdict(int)
    for sample in samples:
        counts[sample["edge_path"]] += 1

    # ëª¨ë“  ê°€ëŠ¥í•œ ê²½ë¡œ ìˆ˜ì§‘
    all_paths = set(counts.keys())
    for node, neighbors in graph.items():
        for neigh, _ in neighbors:
            all_paths.add(f"{node}->{neigh}")

    # ë¶€ì¡±í•œ ê²½ë¡œì— ê°€ìƒ ë°ì´í„° ì¶”ê°€
    for path in all_paths:
        needed = max(0, min_samples_per_path - counts[path])
        for _ in range(needed):
            synthetic = generate_virtual_sequence(path, positions, graph, feature_mode=feature_mode)
            if synthetic is None:
                break
            samples.append(synthetic)
            counts[path] += 1

    return samples


def stratified_split(samples: List[Dict]):
    """ê²½ë¡œë³„ë¡œ stratified split (seq2seq)"""
    buckets = defaultdict(list)
    for sample in samples:
        buckets[sample["edge_path"]].append(sample)

    train_set, val_set, test_set = [], [], []
    rng = random.Random(42)

    # ë°ì´í„° ë¶€ì¡± ê²½ë¡œ ì¶”ì 
    insufficient_paths = []

    for path, items in buckets.items():
        rng.shuffle(items)
        total = len(items)

        # ë°ì´í„° ë¶€ì¡± ì²´í¬ (5ê°œ ë¯¸ë§Œì€ ì œëŒ€ë¡œ ë¶„í•  ë¶ˆê°€)
        if total < 5:
            insufficient_paths.append((path, total))

        train_n = max(1, int(round(total * TRAIN_RATIO)))
        val_n = max(1, int(round(total * VAL_RATIO)))
        test_n = total - train_n - val_n

        # ìµœì†Œ 1ê°œì”©ì€ ë³´ì¥ (í•„ìˆ˜)
        if test_n <= 0:
            test_n = 1
            if val_n > 1:
                val_n -= 1
            else:
                train_n = max(1, train_n - 1)

        train_set.extend(items[:train_n])
        val_set.extend(items[train_n : train_n + val_n])
        test_set.extend(items[train_n + val_n : train_n + val_n + test_n])

    # ë°ì´í„° ë¶€ì¡± ê²½ê³ 
    if insufficient_paths:
        print(f"\nâš ï¸  ë°ì´í„° ë¶€ì¡± ê²½ë¡œ ë°œê²¬: {len(insufficient_paths)}ê°œ")
        print("ë‹¤ìŒ ê²½ë¡œë“¤ì€ ì¶”ê°€ ì¸¡ì •ì´ í•„ìš”í•©ë‹ˆë‹¤ (ê¶Œì¥: 5ê°œ ì´ìƒ):\n")
        for path, count in sorted(insufficient_paths, key=lambda x: x[1]):
            print(f"  - {path}: {count}ê°œ (ë¶€ì¡±: {5-count}ê°œ)")
        print()

    return train_set, val_set, test_set


def save_jsonl(path: Path, samples: Iterable[Dict]):
    """Seq2seq ìƒ˜í”Œ ì €ì¥"""
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w") as f:
        for sample in samples:
            f.write(
                json.dumps(
                    {
                        "features": sample["features"],
                        "targets": sample["targets"],
                        "edge_path": sample["edge_path"],
                        "seq_len": sample["seq_len"],
                    }
                )
            )
            f.write("\n")


def preprocess(args):
    """Seq2seq ë°©ì‹ìœ¼ë¡œ ë°ì´í„° ì „ì²˜ë¦¬"""
    law_dir = Path(args.law_dir)
    output_dir = Path(args.output)
    nodes_path = Path(args.nodes)
    min_samples_per_path = getattr(args, "min_samples_per_path", 3)
    feature_mode = getattr(args, "feature_mode", "full")

    positions = read_nodes(nodes_path)
    graph = build_graph(positions)

    all_samples = []
    for csv_file in sorted(law_dir.glob("*.csv")):
        feats, coords, tags = process_csv(csv_file, positions, graph, feature_mode=feature_mode)
        if not feats:
            continue

        # ì „ì²´ ê²½ë¡œë¥¼ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë¡œ ìƒì„±
        sample = create_full_sequence(feats, coords, tags, csv_file)
        if sample is not None:
            all_samples.append(sample)

    if not all_samples:
        raise RuntimeError("ìƒì„±ëœ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤. ì…ë ¥ ë°ì´í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    print(f"ğŸ“Š ì‹¤ì œ ë°ì´í„°: {len(all_samples)}ê°œ ê²½ë¡œ (feature_mode={feature_mode})")

    # ë¶€ì¡±í•œ ê²½ë¡œì— ê°€ìƒ ë°ì´í„° ì¶”ê°€
    if min_samples_per_path > 0:
        all_samples = balance_with_virtual(all_samples, positions, graph, min_samples_per_path, feature_mode=feature_mode)

    # ê²½ë¡œë³„ stratified split
    train_set, val_set, test_set = stratified_split(all_samples)

    # ì €ì¥
    save_jsonl(output_dir / "train.jsonl", train_set)
    save_jsonl(output_dir / "val.jsonl", val_set)
    save_jsonl(output_dir / "test.jsonl", test_set)

    # Summary
    summary_path = output_dir / "summary.csv"
    with summary_path.open("w") as f:
        f.write("split,count,avg_seq_len\n")
        train_avg = sum(s["seq_len"] for s in train_set) / len(train_set) if train_set else 0
        val_avg = sum(s["seq_len"] for s in val_set) / len(val_set) if val_set else 0
        test_avg = sum(s["seq_len"] for s in test_set) / len(test_set) if test_set else 0
        f.write(f"train,{len(train_set)},{train_avg:.0f}\n")
        f.write(f"val,{len(val_set)},{val_avg:.0f}\n")
        f.write(f"test,{len(test_set)},{test_avg:.0f}\n")

    print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ (seq2seq)")
    print(f"   Train: {len(train_set)}ê°œ ê²½ë¡œ (í‰ê·  {train_avg:.0f} íƒ€ì„ìŠ¤í…)")
    print(f"   Val:   {len(val_set)}ê°œ ê²½ë¡œ (í‰ê·  {val_avg:.0f} íƒ€ì„ìŠ¤í…)")
    print(f"   Test:  {len(test_set)}ê°œ ê²½ë¡œ (í‰ê·  {test_avg:.0f} íƒ€ì„ìŠ¤í…)")


# -----------------------------
# í•™ìŠµ íŒŒì´í”„ë¼ì¸ (Hyena)
# -----------------------------


def time_warp_tensor(x: torch.Tensor, scale: float = 0.2) -> torch.Tensor:
    if x.size(0) < 2 or scale <= 0.0:
        return x
    warp = random.uniform(1 - scale, 1 + scale)
    new_len = max(2, int(x.size(0) * warp))
    interp = F.interpolate(
        x.unsqueeze(0).transpose(1, 2),
        size=new_len,
        mode="linear",
        align_corners=False,
    ).transpose(1, 2).squeeze(0)
    if new_len >= x.size(0):
        return interp[: x.size(0)]
    pad = x.size(0) - new_len
    return torch.cat([interp, interp[-1:].repeat(pad, 1)], dim=0)


def time_mask_tensor(x: torch.Tensor, ratio: float = 0.05) -> torch.Tensor:
    length = x.size(0)
    mask_len = max(1, int(length * ratio))
    start = random.randint(0, max(0, length - mask_len))
    x[start : start + mask_len] = 0
    return x


def apply_sequential_augments(x: torch.Tensor) -> torch.Tensor:
    x = x + torch.randn_like(x) * 0.01
    if random.random() < 0.5:
        x = time_warp_tensor(x)
    if random.random() < 0.5:
        x = time_mask_tensor(x)
    dropout_mask = torch.rand(x.shape[0], 1, device=x.device) < 0.02
    x = torch.where(dropout_mask, torch.zeros_like(x), x)
    return x


class Seq2SeqDataset(Dataset):
    """Seq2seq ë°©ì‹ ë°ì´í„°ì…‹: ì „ì²´ ê²½ë¡œ ë¡œë“œ"""

    def __init__(self, path: Path, augment: bool = False):
        self.samples = []
        self.edge_to_id = {}  # edge_path â†’ ID ë§¤í•‘

        with path.open() as f:
            for line in f:
                if line.strip():
                    sample = json.loads(line)
                    self.samples.append(sample)

                    # edge_pathë¥¼ IDë¡œ ë³€í™˜
                    edge_path = sample["edge_path"]
                    if edge_path not in self.edge_to_id:
                        self.edge_to_id[edge_path] = len(self.edge_to_id)

        if not self.samples:
            raise RuntimeError(f"{path} ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        self.augment = augment

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]

        x = torch.tensor(sample["features"], dtype=torch.float32)  # (seq_len, 6)
        y = torch.tensor(sample["targets"], dtype=torch.float32)  # (seq_len, 2)
        edge_id = self.edge_to_id[sample["edge_path"]]

        if self.augment:
            x = apply_sequential_augments(x)

        return x, y, edge_id


def collate_seq2seq(batch):
    """ê°€ë³€ ê¸¸ì´ ì‹œí€€ìŠ¤ë¥¼ ë°°ì¹˜ë¡œ ë¬¶ê¸° (padding ì‚¬ìš©)"""
    xs, ys, edge_ids = zip(*batch)

    # ìµœëŒ€ ê¸¸ì´ ì°¾ê¸°
    max_len = max(x.size(0) for x in xs)

    # Padding
    xs_padded = []
    ys_padded = []
    masks = []

    for x, y in zip(xs, ys):
        seq_len = x.size(0)
        pad_len = max_len - seq_len

        # Padding (0ìœ¼ë¡œ)
        x_pad = F.pad(x, (0, 0, 0, pad_len))  # (max_len, 6)
        y_pad = F.pad(y, (0, 0, 0, pad_len))  # (max_len, 2)

        # Mask (True = valid, False = padding)
        mask = torch.cat([torch.ones(seq_len, dtype=torch.bool), torch.zeros(pad_len, dtype=torch.bool)])

        xs_padded.append(x_pad)
        ys_padded.append(y_pad)
        masks.append(mask)

    xs_batch = torch.stack(xs_padded)  # (batch, max_len, 6)
    ys_batch = torch.stack(ys_padded)  # (batch, max_len, 2)
    masks_batch = torch.stack(masks)  # (batch, max_len)
    edge_ids_batch = torch.tensor(edge_ids, dtype=torch.long)  # (batch,)

    return xs_batch, ys_batch, edge_ids_batch, masks_batch


class PositionalEncoding(nn.Module):
    """Sinusoidal positional encoding"""

    def __init__(self, dim: int, max_len: int = 5000):
        super().__init__()
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, dim, 2) * (-math.log(10000.0) / dim))
        pe = torch.zeros(max_len, dim)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer("pe", pe)

    def forward(self, seq_len: int) -> torch.Tensor:
        return self.pe[:seq_len]  # (seq_len, dim)


class ImplicitFilter(nn.Module):
    """ì‘ì€ MLPë¡œ ê¸´ í•„í„° ìƒì„± (Hyenaì˜ í•µì‹¬)"""

    def __init__(self, dim: int, hidden_dim: int = 64):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(1, hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, dim),
        )

    def forward(self, seq_len: int) -> torch.Tensor:
        # ìœ„ì¹˜ [0, 1, ..., seq_len-1] ìƒì„±
        positions = torch.linspace(0, 1, seq_len, device=next(self.parameters()).device)
        positions = positions.unsqueeze(-1)  # (seq_len, 1)
        filter_weights = self.mlp(positions)  # (seq_len, dim)
        return filter_weights


class HyenaOperator(nn.Module):
    """ì§„ì§œ Hyena: Implicit filter + Short conv + FFT long conv + Multiple gates"""

    def __init__(self, dim: int, order: int = 2):
        super().__init__()
        self.dim = dim
        self.order = order  # gating paths ê°œìˆ˜

        # Implicit long filter
        self.implicit_filter = ImplicitFilter(dim)

        # Short convolution (data-controlled)
        self.short_conv = nn.Conv1d(
            dim, dim, kernel_size=3, padding=1, groups=dim  # depthwise conv
        )

        # Projections for multiple paths (v, u, z, ...)
        self.in_proj = nn.Linear(dim, dim * (order + 1))
        self.out_proj = nn.Linear(dim, dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: (batch, seq_len, dim)
        """
        batch, seq_len, dim = x.shape

        # Multiple paths projection
        proj = self.in_proj(x)  # (batch, seq_len, dim * (order+1))
        paths = proj.chunk(self.order + 1, dim=-1)  # List of (batch, seq_len, dim)

        v = paths[0]  # ê°’ ê²½ë¡œ

        # Implicit filter ìƒì„±
        filt = self.implicit_filter(seq_len)  # (seq_len, dim)

        # Short convolution (data-controlled)
        u_input = paths[1].transpose(1, 2)  # (batch, dim, seq_len)
        u_short = self.short_conv(u_input).transpose(1, 2)  # (batch, seq_len, dim)

        # FFT long convolution
        # filtì™€ u_shortë¥¼ element-wise ê³±í•œ ë’¤ FFT conv
        U = torch.fft.rfft(u_short, dim=1)  # (batch, freq, dim)
        Filt = torch.fft.rfft(filt.unsqueeze(0), n=seq_len, dim=1)  # (1, freq, dim)
        filtered = torch.fft.irfft(U * Filt, n=seq_len, dim=1)  # (batch, seq_len, dim)

        # Multiple gating: v * filtered * z (if order >= 2)
        output = v * filtered

        if self.order >= 2:
            z = paths[2]
            output = output * torch.sigmoid(z)

        return self.out_proj(output)


class HyenaBlock(nn.Module):
    """Hyena Block with normalization and residual"""

    def __init__(self, dim: int, order: int = 2, dropout: float = 0.1):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.hyena = HyenaOperator(dim, order=order)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: (batch, seq_len, dim)
        """
        h = self.norm(x)
        out = self.hyena(h)
        out = self.dropout(out)
        return out + x  # residual connection


class HyenaSeq2SeqModel(nn.Module):
    """Seq2seq Hyena: ê°€ë³€ ê¸¸ì´ ì…ë ¥ â†’ ì „ì²´ trajectory ì¶œë ¥"""

    def __init__(
        self,
        input_dim: int = 6,
        hidden_dim: int = 128,
        depth: int = 4,
        order: int = 2,
        dropout: float = 0.1,
        num_edge_types: int = 100,  # ë°©í–¥ì„± ì¸ì½”ë”©ì„ ìœ„í•œ edge íƒ€ì… ìˆ˜
    ):
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim

        # ì…ë ¥ projection
        self.input_proj = nn.Linear(input_dim, hidden_dim)

        # Positional encoding
        self.pos_encoding = PositionalEncoding(hidden_dim)

        # Edge path embedding (ë°©í–¥ì„± ì¸ì½”ë”©)
        self.edge_embedding = nn.Embedding(num_edge_types, hidden_dim)

        # Hyena blocks
        self.blocks = nn.ModuleList(
            [HyenaBlock(hidden_dim, order=order, dropout=dropout) for _ in range(depth)]
        )

        # Output head (ê° íƒ€ì„ìŠ¤í…ë§ˆë‹¤ ì¢Œí‘œ ì˜ˆì¸¡)
        self.norm = nn.LayerNorm(hidden_dim)
        self.head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 2),  # (x, y)
        )

    def forward(
        self, x: torch.Tensor, edge_ids: torch.Tensor | None = None
    ) -> torch.Tensor:
        """
        x: (batch, seq_len, input_dim)
        edge_ids: (batch,) - edge pathì˜ ID (ë°©í–¥ì„± êµ¬ë¶„)
        returns: (batch, seq_len, 2)
        """
        batch, seq_len, _ = x.shape

        # Input projection
        h = self.input_proj(x)  # (batch, seq_len, hidden_dim)

        # Add positional encoding
        pos = self.pos_encoding(seq_len)  # (seq_len, hidden_dim)
        h = h + pos.unsqueeze(0)  # broadcast

        # Add edge path embedding (ë°©í–¥ì„± ì •ë³´)
        if edge_ids is not None:
            edge_emb = self.edge_embedding(edge_ids)  # (batch, hidden_dim)
            h = h + edge_emb.unsqueeze(1)  # broadcast to all timesteps

        # Hyena blocks
        for block in self.blocks:
            h = block(h)

        # Output head (ê° íƒ€ì„ìŠ¤í…ë§ˆë‹¤ ì¢Œí‘œ ì˜ˆì¸¡)
        h = self.norm(h)
        coords = self.head(h)  # (batch, seq_len, 2)

        return coords


def seq2seq_loss(
    pred: torch.Tensor,
    target: torch.Tensor,
    mask: torch.Tensor,
    huber_delta: float = 1.0,
    l2_weight: float = 0.1,
    penalty_weight: float = 0.5,
):
    """
    Seq2seq ì†ì‹¤ í•¨ìˆ˜ (mask ì ìš©)
    pred: (batch, seq_len, 2)
    target: (batch, seq_len, 2)
    mask: (batch, seq_len) - True = valid, False = padding
    """
    # ìœ íš¨í•œ íƒ€ì„ìŠ¤í…ë§Œ ì„ íƒ
    pred_valid = pred[mask]  # (num_valid, 2)
    target_valid = target[mask]  # (num_valid, 2)

    if pred_valid.numel() == 0:
        return torch.tensor(0.0, device=pred.device)

    # Huber loss
    huber = F.huber_loss(pred_valid, target_valid, delta=huber_delta)

    # L2 loss
    l2 = F.mse_loss(pred_valid, target_valid)

    return huber + l2_weight * l2


def load_nodes_tensor(nodes_path: Path, device: torch.device) -> torch.Tensor:
    positions = read_nodes(nodes_path)
    coords = torch.tensor(list(positions.values()), dtype=torch.float32, device=device)
    return coords


def compute_metrics(
    pred: torch.Tensor,
    target: torch.Tensor,
    node_positions: torch.Tensor,
    thresholds=(0.9, 1.35, 2.0),
    topk: int = 5,
):
    diff = torch.norm(pred - target, dim=1)
    rmse = torch.sqrt(F.mse_loss(pred, target))

    metrics = {}
    for thr in thresholds:
        metrics[f"top1_{thr}m"] = (diff <= thr).float().mean().item()

    if node_positions is not None:
        # í›„ë³´ ë…¸ë“œ ìƒìœ„ Kê°œì˜ ê±°ë¦¬
        pred_exp = pred.unsqueeze(1) - node_positions.unsqueeze(0)
        pred_nodes = torch.norm(pred_exp, dim=2)
        _, pred_idx = torch.topk(
            pred_nodes, k=min(topk, node_positions.shape[0]), largest=False
        )

        target_exp = target.unsqueeze(1) - node_positions.unsqueeze(0)
        target_dist, target_idx = torch.min(target_exp.norm(dim=2), dim=1, keepdim=True)

        for thr in thresholds:
            within_thr = target_dist.squeeze(1) <= thr
            match = (pred_idx == target_idx).any(dim=1)
            metrics[f"top5_{thr}m"] = (within_thr & match).float().mean().item()
    else:
        for thr in thresholds:
            metrics[f"top5_{thr}m"] = float("nan")

    metrics["rmse"] = rmse.item()
    metrics["avg_dist"] = diff.mean().item()
    return metrics


def run_epoch_seq2seq(model, loader, optimizer, device, train=True):
    """Seq2seq í•™ìŠµ/í‰ê°€ ë£¨í”„"""
    if train:
        model.train()
    else:
        model.eval()

    total_loss = 0.0
    total_samples = 0

    # í‰ê°€ ë©”íŠ¸ë¦­ ëˆ„ì 
    all_distances = []

    for batch in loader:
        x, y, edge_ids, mask = batch
        x = x.to(device)
        y = y.to(device)
        edge_ids = edge_ids.to(device)
        mask = mask.to(device)

        if train:
            optimizer.zero_grad()

        with torch.set_grad_enabled(train):
            pred = model(x, edge_ids)  # (batch, seq_len, 2)
            loss = seq2seq_loss(pred, y, mask)

            if train:
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()

        total_loss += loss.item() * x.size(0)
        total_samples += x.size(0)

        # ê±°ë¦¬ ë©”íŠ¸ë¦­ ê³„ì‚° (ìœ íš¨í•œ íƒ€ì„ìŠ¤í…ë§Œ) - ì—­ì •ê·œí™” í›„ ë¯¸í„° ë‹¨ìœ„ë¡œ
        pred_valid = pred[mask]
        target_valid = y[mask]

        # ì—­ì •ê·œí™”í•˜ì—¬ ì‹¤ì œ ë¯¸í„° ë‹¨ìœ„ ê±°ë¦¬ ê³„ì‚°
        pred_denorm = torch.zeros_like(pred_valid)
        target_denorm = torch.zeros_like(target_valid)
        for i in range(pred_valid.size(0)):
            x_pred, y_pred = denormalize_coord(pred_valid[i, 0].item(), pred_valid[i, 1].item())
            x_tgt, y_tgt = denormalize_coord(target_valid[i, 0].item(), target_valid[i, 1].item())
            pred_denorm[i] = torch.tensor([x_pred, y_pred])
            target_denorm[i] = torch.tensor([x_tgt, y_tgt])

        distances = torch.norm(pred_denorm - target_denorm, dim=1)
        all_distances.extend(distances.detach().cpu().tolist())

    avg_loss = total_loss / total_samples
    all_distances = torch.tensor(all_distances)

    metrics = {
        "loss": avg_loss,
        "rmse": torch.sqrt(torch.mean(all_distances ** 2)).item(),
        "mae": torch.mean(all_distances).item(),
        "median": torch.median(all_distances).item(),
        "p90": torch.quantile(all_distances, 0.9).item(),
    }

    return metrics


def evaluate_with_sliding_window(model, dataset, device, window_size=250, stride=50):
    """ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ í‰ê°€ (validation/test ëª¨ë‘ ì‚¬ìš©)

    Args:
        stride: ìŠ¬ë¼ì´ë”© ê°„ê²© (ì‘ì„ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)
                - Validation: 100-150 ê¶Œì¥ (ì†ë„)
                - Test: 50 ê¶Œì¥ (ì •í™•ë„)
    """
    model.eval()
    all_distances = []

    with torch.no_grad():
        for sample in dataset.samples:
            feats = sample["features"]
            targets = sample["targets"]  # ì •ê·œí™”ëœ ì¢Œí‘œ
            edge_path = sample["edge_path"]
            edge_id = dataset.edge_to_id.get(edge_path, 0)

            if len(feats) < window_size:
                continue

            # ìŠ¬ë¼ì´ë”© ìœˆë„ìš°
            for i in range(0, len(feats) - window_size + 1, stride):
                window_feat = feats[i : i + window_size]
                window_target = targets[i : i + window_size]

                # í…ì„œ ë³€í™˜
                x = torch.tensor(window_feat, dtype=torch.float32).unsqueeze(0).to(device)
                edge_tensor = torch.tensor([edge_id], dtype=torch.long).to(device)

                # ì˜ˆì¸¡ (ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ë§Œ ì‚¬ìš©)
                pred = model(x, edge_tensor)  # (1, window_size, 2)
                pred_last_norm = pred[0, -1, :].cpu().numpy()

                # ì—­ì •ê·œí™”
                pred_last = denormalize_coord(pred_last_norm[0], pred_last_norm[1])
                target_last_norm = window_target[-1]
                target_last = denormalize_coord(target_last_norm[0], target_last_norm[1])

                # ê±°ë¦¬ ê³„ì‚°
                dist = math.hypot(pred_last[0] - target_last[0], pred_last[1] - target_last[1])
                all_distances.append(dist)

    if not all_distances:
        return {"rmse": float("inf"), "mae": float("inf"), "median": float("inf"), "p90": float("inf")}

    all_distances = torch.tensor(all_distances)
    metrics = {
        "rmse": torch.sqrt(torch.mean(all_distances ** 2)).item(),
        "mae": torch.mean(all_distances).item(),
        "median": torch.median(all_distances).item(),
        "p90": torch.quantile(all_distances, 0.9).item(),
    }

    return metrics


def train(args):
    """Seq2seq Hyena ëª¨ë¸ í•™ìŠµ"""
    data_dir = Path(args.data_dir)
    nodes_path = Path(args.nodes)
    train_path = data_dir / "train.jsonl"
    val_path = data_dir / "val.jsonl"
    test_path = data_dir / "test.jsonl"

    for path in (train_path, val_path, test_path):
        if not path.exists():
            raise FileNotFoundError(f"{path} ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € preprocessë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

    device = torch.device("cuda" if torch.cuda.is_available() and not args.cpu else "cpu")

    # Seq2seq ë°ì´í„°ì…‹ ë¡œë“œ (augment ì œê±°)
    train_ds = Seq2SeqDataset(train_path, augment=False)
    val_ds = Seq2SeqDataset(val_path, augment=False)
    test_ds = Seq2SeqDataset(test_path, augment=False)

    # Edge íƒ€ì… ìˆ˜ ê³„ì‚° (ëª¨ë“  splitì˜ edge í•©ì¹˜ê¸°)
    all_edges = set()
    all_edges.update(train_ds.edge_to_id.keys())
    all_edges.update(val_ds.edge_to_id.keys())
    all_edges.update(test_ds.edge_to_id.keys())
    num_edge_types = len(all_edges)

    # Input dimension ìë™ ê°ì§€
    sample_features = train_ds.samples[0]["features"]
    input_dim = len(sample_features[0])

    print(f"ğŸ“Š ë°ì´í„° ë¡œë“œ ì™„ë£Œ:")
    print(f"   Train: {len(train_ds)}ê°œ ê²½ë¡œ")
    print(f"   Val:   {len(val_ds)}ê°œ ê²½ë¡œ")
    print(f"   Test:  {len(test_ds)}ê°œ ê²½ë¡œ")
    print(f"   Edge types: {num_edge_types}ê°œ (ë°©í–¥ì„± í¬í•¨)")
    print(f"   Input dim: {input_dim}ê°œ íŠ¹ì§•")

    # DataLoader (collate_fn ì‚¬ìš©)
    train_loader = DataLoader(
        train_ds,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=collate_seq2seq,
        drop_last=True,
    )
    val_loader = DataLoader(val_ds, batch_size=args.batch_size, collate_fn=collate_seq2seq)
    test_loader = DataLoader(test_ds, batch_size=args.batch_size, collate_fn=collate_seq2seq)

    # Seq2seq Hyena ëª¨ë¸
    model = HyenaSeq2SeqModel(
        input_dim=input_dim,
        hidden_dim=args.hidden_dim,
        depth=args.depth,
        order=getattr(args, "hyena_order", 2),
        dropout=args.dropout,
        num_edge_types=num_edge_types,
    ).to(device)

    print(f"ğŸ§  ëª¨ë¸: Hyena Seq2seq")
    print(f"   Input dim: {input_dim}")
    print(f"   Hidden dim: {args.hidden_dim}")
    print(f"   Depth: {args.depth}")
    print(f"   Parameters: {sum(p.numel() for p in model.parameters()):,}")

    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)

    best_val = float("inf")
    no_improve = 0
    ckpt_dir = Path(args.checkpoint_dir)
    ckpt_dir.mkdir(parents=True, exist_ok=True)
    best_path = ckpt_dir / "best.pt"

    print(f"\nğŸš€ í•™ìŠµ ì‹œì‘ (Epochs: {args.epochs})\n")

    for epoch in range(1, args.epochs + 1):
        train_metrics = run_epoch_seq2seq(model, train_loader, optimizer, device, train=True)
        val_metrics = run_epoch_seq2seq(model, val_loader, optimizer, device, train=False)
        scheduler.step()

        print(
            f"[Epoch {epoch:03d}] "
            f"TrainLoss={train_metrics['loss']:.4f} "
            f"ValLoss={val_metrics['loss']:.4f} | "
            f"RMSE={val_metrics['rmse']:.3f}m "
            f"MAE={val_metrics['mae']:.3f}m "
            f"Median={val_metrics['median']:.3f}m "
            f"P90={val_metrics['p90']:.3f}m"
        )

        if val_metrics["loss"] + 1e-4 < best_val:
            best_val = val_metrics["loss"]
            no_improve = 0
            torch.save(
                {
                    "model_state": model.state_dict(),
                    "optimizer_state": optimizer.state_dict(),
                    "epoch": epoch,
                    "val_metrics": val_metrics,
                    "edge_to_id": train_ds.edge_to_id,  # ì €ì¥
                },
                best_path,
            )
            print(f"   ğŸ’¾ Best model saved (loss={best_val:.4f})")
        else:
            no_improve += 1
            if no_improve >= args.patience:
                print(f"\nâ¹ï¸  Early stopping at epoch {epoch} (patience {args.patience})")
                break

    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ. ë² ìŠ¤íŠ¸ ì²´í¬í¬ì¸íŠ¸: {best_path}")

    # Best model ë¡œë“œ
    if best_path.exists():
        checkpoint = torch.load(best_path, map_location=device)
        model.load_state_dict(checkpoint["model_state"])

    # Test í‰ê°€ (ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹)
    print(f"\nğŸ“ˆ Test í‰ê°€ ì¤‘ (ìŠ¬ë¼ì´ë”© ìœˆë„ìš°: window=250, stride=50)...")
    test_metrics = evaluate_with_sliding_window(model, test_ds, device, window_size=250, stride=50)

    print(
        f"\n[Test Results - Sliding Window]\n"
        f"  RMSE:   {test_metrics['rmse']:.3f}m\n"
        f"  MAE:    {test_metrics['mae']:.3f}m\n"
        f"  Median: {test_metrics['median']:.3f}m\n"
        f"  P90:    {test_metrics['p90']:.3f}m\n"
    )


def inference(args):
    """250 ìœˆë„ìš° ìŠ¬ë¼ì´ë”© ì¶”ë¡ """
    import matplotlib.pyplot as plt
    import matplotlib.font_manager as fm

    # í•œêµ­ì–´ í°íŠ¸ ì„¤ì • (macOS/Linux/Windows ìë™ ê°ì§€)
    try:
        import platform
        system = platform.system()
        if system == "Darwin":  # macOS
            plt.rc("font", family="AppleGothic")
        elif system == "Windows":
            plt.rc("font", family="Malgun Gothic")
        else:  # Linux
            plt.rc("font", family="NanumGothic")
        plt.rc("axes", unicode_minus=False)  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
    except Exception:
        print("âš ï¸  í•œêµ­ì–´ í°íŠ¸ ì„¤ì • ì‹¤íŒ¨. ì˜ë¬¸ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.")

    checkpoint_path = Path(args.checkpoint)
    csv_path = Path(args.csv)
    nodes_path = Path(args.nodes)
    window_size = args.window_size

    if not checkpoint_path.exists():
        raise FileNotFoundError(f"ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: {checkpoint_path}")
    if not csv_path.exists():
        raise FileNotFoundError(f"CSV íŒŒì¼ ì—†ìŒ: {csv_path}")

    device = torch.device("cuda" if torch.cuda.is_available() and not args.cpu else "cpu")

    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    print(f"ğŸ“¦ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location=device)

    # ë…¸ë“œ ë° ê·¸ë˜í”„
    positions = read_nodes(nodes_path)
    graph = build_graph(positions)

    # Edge to ID ë§¤í•‘ ë¡œë“œ
    edge_to_id = checkpoint.get("edge_to_id", {})
    num_edge_types = len(edge_to_id)

    # ëª¨ë¸ ë¡œë“œ
    model = HyenaSeq2SeqModel(
        input_dim=6,
        hidden_dim=args.hidden_dim,
        depth=args.depth,
        order=2,
        dropout=0.0,  # ì¶”ë¡  ì‹œ dropout ì—†ìŒ
        num_edge_types=num_edge_types,
    ).to(device)
    model.load_state_dict(checkpoint["model_state"])
    model.eval()

    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    # CSV ì²˜ë¦¬
    print(f"ğŸ“Š CSV ì²˜ë¦¬: {csv_path}")
    feats, coords, tags = process_csv(csv_path, positions, graph)

    if len(feats) < window_size:
        raise ValueError(f"ë°ì´í„° ê¸¸ì´({len(feats)})ê°€ ìœˆë„ìš° í¬ê¸°({window_size})ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤.")

    # Edge path ì¶”ë¡  (íŒŒì¼ëª…ì—ì„œ)
    start_end = csv_path.stem.split("_")[:2]
    edge_path = f"{start_end[0]}->{start_end[1]}"
    edge_id = edge_to_id.get(edge_path, 0)

    print(f"ğŸ” ì¶”ë¡  ì‹œì‘ (ìœˆë„ìš°={window_size}, ì „ì²´ ê¸¸ì´={len(feats)})")

    # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì¶”ë¡ 
    predictions = []
    ground_truths = []

    with torch.no_grad():
        for i in range(len(feats) - window_size + 1):
            window_feat = feats[i : i + window_size]
            window_coords = coords[i : i + window_size]

            # í…ì„œ ë³€í™˜
            x = torch.tensor(window_feat, dtype=torch.float32).unsqueeze(0).to(device)  # (1, window_size, 6)
            edge_tensor = torch.tensor([edge_id], dtype=torch.long).to(device)

            # ì˜ˆì¸¡ (ì •ê·œí™”ëœ ì¢Œí‘œ)
            pred = model(x, edge_tensor)  # (1, window_size, 2)
            pred_last_norm = pred[0, -1, :].cpu().numpy()  # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ë§Œ (ì •ê·œí™”ë¨)

            # ì—­ì •ê·œí™”í•˜ì—¬ ë¯¸í„° ë‹¨ìœ„ë¡œ ë³€í™˜
            pred_last = denormalize_coord(pred_last_norm[0], pred_last_norm[1])

            predictions.append(pred_last)
            ground_truths.append(window_coords[-1])

    predictions = np.array(predictions)
    ground_truths = np.array(ground_truths)

    # í‰ê°€
    distances = np.linalg.norm(predictions - ground_truths, axis=1)
    rmse = np.sqrt(np.mean(distances ** 2))
    mae = np.mean(distances)
    median = np.median(distances)
    p90 = np.percentile(distances, 90)

    print(f"\nğŸ“ˆ ì¶”ë¡  ê²°ê³¼:")
    print(f"  ìƒ˜í”Œ ìˆ˜: {len(predictions)}")
    print(f"  RMSE:    {rmse:.3f}m")
    print(f"  MAE:     {mae:.3f}m")
    print(f"  Median:  {median:.3f}m")
    print(f"  P90:     {p90:.3f}m")

    # ì‹œê°í™”
    if not args.no_plot:
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))

        # 1. Trajectory ë¹„êµ
        ax = axes[0, 0]
        ax.plot(ground_truths[:, 0], ground_truths[:, 1], "b-", label="Ground Truth", alpha=0.7)
        ax.plot(predictions[:, 0], predictions[:, 1], "r--", label="Prediction", alpha=0.7)
        ax.set_xlabel("X (m)")
        ax.set_ylabel("Y (m)")
        ax.set_title("Trajectory Comparison")
        ax.legend()
        ax.grid(True)
        ax.axis("equal")

        # 2. ì‹œê°„ì— ë”°ë¥¸ ì˜¤ì°¨
        ax = axes[0, 1]
        ax.plot(distances, label="Distance Error")
        ax.axhline(y=rmse, color="r", linestyle="--", label=f"RMSE={rmse:.2f}m")
        ax.axhline(y=1.35, color="orange", linestyle=":", label="Target=1.35m")
        ax.set_xlabel("Timestep")
        ax.set_ylabel("Error (m)")
        ax.set_title("Error over Time")
        ax.legend()
        ax.grid(True)

        # 3. ì˜¤ì°¨ íˆìŠ¤í† ê·¸ë¨
        ax = axes[1, 0]
        ax.hist(distances, bins=50, alpha=0.7, edgecolor="black")
        ax.axvline(x=mae, color="r", linestyle="--", label=f"MAE={mae:.2f}m")
        ax.axvline(x=median, color="g", linestyle="--", label=f"Median={median:.2f}m")
        ax.set_xlabel("Error (m)")
        ax.set_ylabel("Frequency")
        ax.set_title("Error Distribution")
        ax.legend()
        ax.grid(True)

        # 4. CDF
        ax = axes[1, 1]
        sorted_distances = np.sort(distances)
        cdf = np.arange(1, len(sorted_distances) + 1) / len(sorted_distances)
        ax.plot(sorted_distances, cdf * 100)
        ax.axvline(x=1.35, color="orange", linestyle=":", label="Target=1.35m")
        ax.axhline(y=90, color="r", linestyle="--", alpha=0.5)
        ax.set_xlabel("Error (m)")
        ax.set_ylabel("Cumulative Percentage (%)")
        ax.set_title("Cumulative Error Distribution")
        ax.legend()
        ax.grid(True)

        plt.tight_layout()
        output_path = checkpoint_path.parent / f"inference_{csv_path.stem}.png"
        plt.savefig(output_path, dpi=150)
        print(f"\nğŸ’¾ ê·¸ë˜í”„ ì €ì¥: {output_path}")

        if not args.no_show:
            plt.show()


# -----------------------------
# CLI
# -----------------------------

def main():
    parser = argparse.ArgumentParser(description="ì§€ìê¸° ê¸°ë°˜ ì‹¤ë‚´ì¸¡ìœ„ íŒŒì´í”„ë¼ì¸")
    subparsers = parser.add_subparsers(dest="command", required=True)

    prep_parser = subparsers.add_parser("preprocess", help="CSV -> ìœˆë„ìš° ë°ì´í„° ìƒì„±")
    prep_parser.add_argument("--law-dir", default="law_data")
    prep_parser.add_argument("--nodes", default="nodes_final.csv")
    prep_parser.add_argument("--output", default="data")
    prep_parser.add_argument("--min-samples-per-path", type=int, default=3)
    prep_parser.add_argument("--feature-mode", default="full", choices=["full", "mag4"],
                             help="Feature mode: full (6 features) or mag4 (4 features)")

    train_parser = subparsers.add_parser("train", help="Hyena ëª¨ë¸ í•™ìŠµ")
    train_parser.add_argument("--data-dir", default="data")
    train_parser.add_argument("--nodes", default="nodes_final.csv")
    train_parser.add_argument("--epochs", type=int, default=50)
    train_parser.add_argument("--batch-size", type=int, default=32)
    train_parser.add_argument("--lr", type=float, default=2e-4)
    train_parser.add_argument("--hidden-dim", type=int, default=256)
    train_parser.add_argument("--depth", type=int, default=8)
    train_parser.add_argument("--dropout", type=float, default=0.1)
    train_parser.add_argument("--checkpoint-dir", default="checkpoints")
    train_parser.add_argument("--patience", type=int, default=10)
    train_parser.add_argument("--cpu", action="store_true")

    infer_parser = subparsers.add_parser("infer", help="250 ìœˆë„ìš° ìŠ¬ë¼ì´ë”© ì¶”ë¡ ")
    infer_parser.add_argument("--checkpoint", required=True, help="ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ")
    infer_parser.add_argument("--csv", required=True, help="ì¶”ë¡ í•  CSV íŒŒì¼ ê²½ë¡œ")
    infer_parser.add_argument("--nodes", default="nodes_final.csv")
    infer_parser.add_argument("--window-size", type=int, default=250)
    infer_parser.add_argument("--hidden-dim", type=int, default=256)
    infer_parser.add_argument("--depth", type=int, default=8)
    infer_parser.add_argument("--no-plot", action="store_true", help="ê·¸ë˜í”„ ìƒì„± ì•ˆ í•¨")
    infer_parser.add_argument("--no-show", action="store_true", help="ê·¸ë˜í”„ í‘œì‹œ ì•ˆ í•¨")
    infer_parser.add_argument("--cpu", action="store_true")

    args = parser.parse_args()
    random.seed(42)
    torch.manual_seed(42)

    if args.command == "preprocess":
        preprocess(args)
    elif args.command == "train":
        train(args)
    elif args.command == "infer":
        inference(args)


if __name__ == "__main__":
    main()
