#!/usr/bin/env python3
"""
Flow Matching preprocessing using data/processed_data CSVs.

This script converts the pre-aligned dataset (with x/y coordinates already
assigned) into the numpy tensors consumed by the Flow Matching training code.

Input  : data/processed_data/*.csv (columns: x, y, Mag*, Pitch/Roll/Yaw, etc.)
Output : flow_matching/processed_data_flow_matching/*.npy + metadata.pkl
"""

from __future__ import annotations

import pickle
from collections import defaultdict
from pathlib import Path

import numpy as np
import pandas as pd
from tqdm import tqdm

# ============================================================================
# Configuration
# ============================================================================
WINDOW_SIZE = 250          # ì‹œí€€ìŠ¤ ê¸¸ì´ (ìƒ˜í”Œë‹¹ ì•½ 5ì´ˆ)
STRIDE = 50                # ìœˆë„ìš° ì´ë™ ê°„ê²© (ìƒ˜í”Œ ì¤‘ì²©)
GRID_SIZE_M = 0.45         # 0.45m ê°„ê²© (ë…¸ë“œ ê°„ ê±°ë¦¬)
SENSOR_COLS = ['MagX', 'MagY', 'MagZ', 'Pitch', 'Roll', 'Yaw']
POSITION_COLS = ['x', 'y']
RNG_SEED = 42

print("=" * 70)
print("ğŸ”§ Flow Matching ì „ì²˜ë¦¬ (data/processed_data â†’ numpy tensors)")
print("=" * 70)

# Paths
REPO_ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = REPO_ROOT / 'data' / 'processed_data'
OUTPUT_DIR = Path(__file__).parent / 'processed_data_flow_matching'

if not DATA_DIR.exists():
    raise FileNotFoundError(f"ë°ì´í„° ë””ë ‰í† ë¦¬ ì—†ìŒ: {DATA_DIR}")

OUTPUT_DIR.mkdir(exist_ok=True)


# ============================================================================
# Helpers
# ============================================================================
def parse_filename(path: Path) -> tuple[int, int, str]:
    """íŒŒì¼ëª… â†’ (start, end, trial)"""
    name = path.stem  # e.g. 1_23_6
    parts = name.split('_')
    if len(parts) != 3:
        raise ValueError(f"Unexpected filename format: {name}")
    start_node = int(parts[0])
    end_node = int(parts[1])
    trial = parts[2]
    return start_node, end_node, trial


def slide_windows(df: pd.DataFrame) -> dict[str, np.ndarray] | None:
    """ìœˆë„ìš° ë‹¨ìœ„ë¡œ (ì„¼ì„œ, ê¶¤ì , ìµœì¢…ì¢Œí‘œ) ìƒì„±"""
    if len(df) < WINDOW_SIZE:
        return None

    seq_list, coord_list, traj_list = [], [], []

    sensor_values = df[SENSOR_COLS].values.astype(np.float32)
    pos_values = df[POSITION_COLS].values.astype(np.float32)

    for start in range(0, len(df) - WINDOW_SIZE + 1, STRIDE):
        end = start + WINDOW_SIZE

        seq = sensor_values[start:end]
        traj = pos_values[start:end]

        if np.isnan(seq).any() or np.isnan(traj).any():
            continue  # ê²°ì¸¡ê°’ ìƒ˜í”Œì€ ì œê±°

        seq_list.append(seq)
        traj_list.append(traj)
        coord_list.append(traj[-1])  # ìœˆë„ìš° ë§ˆì§€ë§‰ ìœ„ì¹˜

    if not seq_list:
        return None

    return {
        'sequences': np.stack(seq_list),
        'trajectories': np.stack(traj_list),
        'coords': np.stack(coord_list),
    }


def normalize_sequences(sequences: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
    """ì„¼ì„œ ë°ì´í„°ë¥¼ í‘œì¤€í™”í•˜ê³  (ì •ê·œí™”ëœ ì‹œí€€ìŠ¤, mean, std) ë°˜í™˜"""
    mean = sequences.mean(axis=(0, 1), keepdims=False)
    std = sequences.std(axis=(0, 1), keepdims=False)
    std[std < 1e-6] = 1.0  # ë¶„ì‚° 0 ë°©ì§€
    normalized = (sequences - mean) / std
    return normalized.astype(np.float32), mean.astype(np.float32), std.astype(np.float32)


def normalize_positions(coords: np.ndarray, x_min: float, x_max: float,
                        y_min: float, y_max: float) -> np.ndarray:
    """ì ˆëŒ€ ì¢Œí‘œ(x/y)ë¥¼ -1~1 ë²”ìœ„ë¡œ ì •ê·œí™”"""
    x_range = max(x_max - x_min, 1e-6)
    y_range = max(y_max - y_min, 1e-6)
    coords_norm = np.empty_like(coords, dtype=np.float32)
    coords_norm[:, 0] = (coords[:, 0] - x_min) / x_range * 2.0 - 1.0
    coords_norm[:, 1] = (coords[:, 1] - y_min) / y_range * 2.0 - 1.0
    return coords_norm


def normalize_trajectories(traj: np.ndarray, x_min: float, x_max: float,
                           y_min: float, y_max: float) -> np.ndarray:
    """ì „ì²´ ê¶¤ì ì„ -1~1ë¡œ ì •ê·œí™”"""
    x_range = max(x_max - x_min, 1e-6)
    y_range = max(y_max - y_min, 1e-6)
    traj_norm = np.empty_like(traj, dtype=np.float32)
    traj_norm[..., 0] = (traj[..., 0] - x_min) / x_range * 2.0 - 1.0
    traj_norm[..., 1] = (traj[..., 1] - y_min) / y_range * 2.0 - 1.0
    return traj_norm


def coord_to_grid_id(coords: np.ndarray, x_min: float, y_min: float,
                     x_max: float, y_max: float) -> np.ndarray:
    """ì¢Œí‘œë¥¼ GRID_SIZE_M ë‹¨ìœ„ì˜ grid idë¡œ ë³€í™˜"""
    num_x_grids = int(np.ceil((x_max - x_min) / GRID_SIZE_M)) + 1
    num_y_grids = int(np.ceil((y_max - y_min) / GRID_SIZE_M)) + 1

    rel_x = np.round((coords[:, 0] - x_min) / GRID_SIZE_M).astype(int)
    rel_y = np.round((coords[:, 1] - y_min) / GRID_SIZE_M).astype(int)

    rel_x = np.clip(rel_x, 0, num_x_grids - 1)
    rel_y = np.clip(rel_y, 0, num_y_grids - 1)

    return (rel_y * num_x_grids + rel_x).astype(np.int32)


# ============================================================================
# 1) íŒŒì¼ë³„ ìœˆë„ìš° ìƒì„±
# ============================================================================
print("\n[1/5] CSV ìŠ¤ìº” ë° ìœˆë„ìš° ìƒì„±...")
files = sorted(DATA_DIR.glob('*.csv'))
if not files:
    raise RuntimeError(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {DATA_DIR}")

all_sequences = []
all_coords = []
all_trajectories = []
file_to_indices = defaultdict(list)  # íŒŒì¼ â†’ ì „ì²´ ì¸ë±ìŠ¤
global_index = 0

for csv_path in tqdm(files, desc="Processing CSV"):
    try:
        start_node, end_node, trial = parse_filename(csv_path)
    except ValueError:
        print(f"  âš ï¸  íŒŒì¼ëª… í˜•ì‹ ë¬´ì‹œ: {csv_path.name}")
        continue

    df = pd.read_csv(csv_path)
    required_cols = set(SENSOR_COLS + POSITION_COLS)
    if not required_cols.issubset(df.columns):
        print(f"  âš ï¸  ëˆ„ë½ ì»¬ëŸ¼ìœ¼ë¡œ ìŠ¤í‚µ: {csv_path.name}")
        continue

    window_data = slide_windows(df)
    if window_data is None:
        continue

    seqs = window_data['sequences']
    coords = window_data['coords']
    trajs = window_data['trajectories']

    all_sequences.append(seqs)
    all_coords.append(coords)
    all_trajectories.append(trajs)

    file_key = f"{start_node}_{end_node}_{trial}"
    file_to_indices[file_key].extend(range(global_index, global_index + len(seqs)))
    global_index += len(seqs)

total_samples = global_index
if total_samples == 0:
    raise RuntimeError("ìƒì„±ëœ ìœˆë„ìš°ê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°/ìœˆë„ìš° ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")

print(f"  ì´ ìœˆë„ìš° ìƒ˜í”Œ: {total_samples:,}ê°œ")

# ìŠ¤íƒ
all_sequences = np.vstack(all_sequences)
all_coords = np.vstack(all_coords)
all_trajectories = np.vstack(all_trajectories)

# ê±´ë¬¼ ë²”ìœ„
x_min, x_max = float(all_coords[:, 0].min()), float(all_coords[:, 0].max())
y_min, y_max = float(all_coords[:, 1].min()), float(all_coords[:, 1].max())
print(f"  ì¢Œí‘œ ë²”ìœ„: x=[{x_min:.2f}, {x_max:.2f}], y=[{y_min:.2f}, {y_max:.2f}]")

# ============================================================================
# 2) ì •ê·œí™”
# ============================================================================
print("\n[2/5] ì„¼ì„œ/ì¢Œí‘œ ì •ê·œí™”...")
states_norm, sensor_mean, sensor_std = normalize_sequences(all_sequences)
coords_norm = normalize_positions(all_coords, x_min, x_max, y_min, y_max)
traj_norm = normalize_trajectories(all_trajectories, x_min, x_max, y_min, y_max)
labels = coord_to_grid_id(all_coords, x_min, y_min, x_max, y_max)

# ============================================================================
# 3) Train/Val/Test split (íŒŒì¼ ë‹¨ìœ„)
# ============================================================================
print("\n[3/5] Train/Val/Test ë¶„í•  (íŒŒì¼ ë‹¨ìœ„)...")
file_keys = list(file_to_indices.keys())
rng = np.random.default_rng(RNG_SEED)
rng.shuffle(file_keys)

n_files = len(file_keys)
n_train = int(n_files * 0.7)
n_val = int(n_files * 0.15)

train_files = file_keys[:n_train]
val_files = file_keys[n_train:n_train + n_val]
test_files = file_keys[n_train + n_val:]

def gather_indices(keys):
    idx = []
    for key in keys:
        idx.extend(file_to_indices[key])
    return np.array(idx, dtype=np.int32)

train_idx = gather_indices(train_files)
val_idx = gather_indices(val_files)
test_idx = gather_indices(test_files)

print(f"  íŒŒì¼ ê¸°ì¤€ ë¶„í• : Train {len(train_files)}, Val {len(val_files)}, Test {len(test_files)}")
print(f"  ìƒ˜í”Œ ìˆ˜: Train {len(train_idx):,}, Val {len(val_idx):,}, Test {len(test_idx):,}")


def subset(arr: np.ndarray, indices: np.ndarray) -> np.ndarray:
    return arr[indices]


states_train = subset(states_norm, train_idx)
coords_train = subset(coords_norm, train_idx)
traj_train = subset(traj_norm, train_idx)
labels_train = subset(labels, train_idx)

states_val = subset(states_norm, val_idx)
coords_val = subset(coords_norm, val_idx)
traj_val = subset(traj_norm, val_idx)
labels_val = subset(labels, val_idx)

states_test = subset(states_norm, test_idx)
coords_test = subset(coords_norm, test_idx)
traj_test = subset(traj_norm, test_idx)
labels_test = subset(labels, test_idx)

# ============================================================================
# 4) ì €ì¥
# ============================================================================
print("\n[4/5] numpy ì €ì¥...")
np.save(OUTPUT_DIR / 'states_train.npy', states_train)
np.save(OUTPUT_DIR / 'coords_train.npy', coords_train)
np.save(OUTPUT_DIR / 'trajectories_train.npy', traj_train)
np.save(OUTPUT_DIR / 'labels_train.npy', labels_train)

np.save(OUTPUT_DIR / 'states_val.npy', states_val)
np.save(OUTPUT_DIR / 'coords_val.npy', coords_val)
np.save(OUTPUT_DIR / 'trajectories_val.npy', traj_val)
np.save(OUTPUT_DIR / 'labels_val.npy', labels_val)

np.save(OUTPUT_DIR / 'states_test.npy', states_test)
np.save(OUTPUT_DIR / 'coords_test.npy', coords_test)
np.save(OUTPUT_DIR / 'trajectories_test.npy', traj_test)
np.save(OUTPUT_DIR / 'labels_test.npy', labels_test)

metadata = {
    'window_size': WINDOW_SIZE,
    'stride': STRIDE,
    'grid_size': GRID_SIZE_M,
    'sensor_cols': SENSOR_COLS,
    'position_cols': POSITION_COLS,
    'num_samples': {
        'train': int(len(train_idx)),
        'val': int(len(val_idx)),
        'test': int(len(test_idx)),
        'total': int(total_samples),
    },
    'sensor_normalization': {
        'mean': sensor_mean.tolist(),
        'std': sensor_std.tolist(),
    },
    'normalization': {  # ê¸°ì¡´ ìŠ¤í¬ë¦½íŠ¸ì™€ í˜¸í™˜
        'sensor_mean': sensor_mean.tolist(),
        'sensor_std': sensor_std.tolist(),
        'x_min': x_min,
        'x_max': x_max,
        'y_min': y_min,
        'y_max': y_max,
    },
    'position_bounds': {
        'x_min': x_min,
        'x_max': x_max,
        'y_min': y_min,
        'y_max': y_max,
    },
    'file_splits': {
        'train': train_files,
        'val': val_files,
        'test': test_files,
    },
}

with open(OUTPUT_DIR / 'metadata.pkl', 'wb') as f:
    pickle.dump(metadata, f)

print("\n[5/5] ì™„ë£Œ ìš”ì•½")
print(f"  states_train.npy shape: {states_train.shape}")
print(f"  states_val.npy   shape: {states_val.shape}")
print(f"  states_test.npy  shape: {states_test.shape}")
print(f"  metadata: {OUTPUT_DIR / 'metadata.pkl'}")
print("\nâœ… data/processed_data â†’ flow_matching/processed_data_flow_matching ë³€í™˜ ì™„ë£Œ!")
