#!/usr/bin/env python3
"""
Flow Matching ì§„ë‹¨ ìŠ¤í¬ë¦½íŠ¸
"""
import torch
import numpy as np
from pathlib import Path
from train_flow_matching import FlowMatchingDataset
from model import FlowMatchingLocalization

print("="*70)
print("ğŸ” Flow Matching ì§„ë‹¨")
print("="*70)

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
BASE_DIR = Path(__file__).resolve().parent

# ë°ì´í„° ë¡œë“œ
data_dir = BASE_DIR / 'processed_data_flow_matching'
states_test = np.load(data_dir / 'states_test.npy')
traj_test = np.load(data_dir / 'trajectories_test.npy')

print(f"\në°ì´í„°:")
print(f"  Test: {states_test.shape}")
print(f"  ì„¼ì„œ ë²”ìœ„: [{states_test.min():.2f}, {states_test.max():.2f}]")
print(f"  ìœ„ì¹˜ ë²”ìœ„: [{traj_test[:, -1, :].min():.2f}, {traj_test[:, -1, :].max():.2f}]")

# ëª¨ë¸ ë¡œë“œ
model = FlowMatchingLocalization(
    sensor_dim=6, position_dim=2, d_model=256,
    encoder_layers=4, velocity_layers=4, n_heads=8, dropout=0.1
).to(DEVICE)

checkpoint = torch.load(BASE_DIR.parent / 'models' / 'flow_matching_best.pt')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# ìƒ˜í”Œ 10ê°œë¡œ í…ŒìŠ¤íŠ¸
test_dataset = FlowMatchingDataset(states_test[:100], traj_test[:100], augment=False)

sensor_data = torch.stack([test_dataset[i]['sensor_data'] for i in range(10)]).to(DEVICE)
true_pos = torch.stack([test_dataset[i]['position'] for i in range(10)]).to(DEVICE)

with torch.no_grad():
    pred_pos = model.sample(sensor_data, n_steps=10)

print(f"\nì˜ˆì¸¡ ê²°ê³¼ (10ê°œ ìƒ˜í”Œ):")
print(f"{'idx':<5} {'True X':<10} {'True Y':<10} {'Pred X':<10} {'Pred Y':<10} {'Error':<10}")
print("-"*70)
for i in range(10):
    tx, ty = true_pos[i].cpu().numpy()
    px, py = pred_pos[i].cpu().numpy()
    error = np.linalg.norm([tx-px, ty-py])
    print(f"{i:<5} {tx:<10.4f} {ty:<10.4f} {px:<10.4f} {py:<10.4f} {error:<10.4f}")

print(f"\ní†µê³„:")
errors = torch.norm(pred_pos - true_pos, dim=1).cpu().numpy()
print(f"  í‰ê·  ì˜¤ì°¨: {errors.mean():.4f}")
print(f"  ìµœì†Œ ì˜¤ì°¨: {errors.min():.4f}")
print(f"  ìµœëŒ€ ì˜¤ì°¨: {errors.max():.4f}")

# Grid í¬ê¸°
grid_size = 0.9 / 85.5 * 2  # ì •ê·œí™”ëœ grid
print(f"\n1 Grid í¬ê¸° (ì •ê·œí™”): {grid_size:.4f}")
print(f"1 Grid ì´ë‚´: {(errors <= grid_size).sum()}/10")

print("\nğŸ’¡ ë¶„ì„:")
if errors.mean() > 1.0:
    print("  âš ï¸ í‰ê·  ì˜¤ì°¨ê°€ ë§¤ìš° í¼ â†’ ëª¨ë¸ì´ ìœ„ì¹˜ í•™ìŠµ ì‹¤íŒ¨")
if errors.std() < 0.1:
    print("  âš ï¸ ë¶„ì‚°ì´ ë„ˆë¬´ ì‘ìŒ â†’ ëª¨ë¸ì´ í•­ìƒ ë¹„ìŠ·í•œ ìœ„ì¹˜ ì˜ˆì¸¡")
if np.abs(pred_pos.cpu().numpy()).mean() > 2.0:
    print("  âš ï¸ ì˜ˆì¸¡ ìœ„ì¹˜ê°€ ì •ê·œí™” ë²”ìœ„ ë²—ì–´ë‚¨ â†’ ì¢Œí‘œê³„ ë¬¸ì œ")
