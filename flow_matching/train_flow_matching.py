#!/usr/bin/env python3
"""
Flow Matching for Magnetic Field Indoor Positioning - Training Script

í•µì‹¬:
- Conditional Flow Matchingìœ¼ë¡œ ì„¼ì„œ â†’ ìœ„ì¹˜ í•™ìŠµ
- 1-2 step inferenceë¡œ ì‹¤ì‹œê°„ ê°€ëŠ¥
- ì™„ì „íˆ ìƒˆë¡œìš´ ì ‘ê·¼ë²• (ë…¼ë¬¸ 0ê°œ)
"""
import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
import math
import pickle
from tqdm import tqdm

from model import FlowMatchingLocalization, compute_flow_matching_loss

print("=" * 70)
print("ğŸš€ Flow Matching for Indoor Positioning - Training")
print("=" * 70)

# ============================================================================
# Configuration
# ============================================================================
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\nì‚¬ìš© ë””ë°”ì´ìŠ¤: {DEVICE}")

CONFIG = {
    # Model
    'sensor_dim': 6,
    'position_dim': 2,
    'd_model': 384,          # 30x ë°ì´í„°ì— ë§ì¶° ëª¨ë¸ í™•ì¥
    'encoder_layers': 6,
    'velocity_layers': 6,
    'n_heads': 8,
    'dropout': 0.15,

    # Data (5ê±¸ìŒ ê¸°ì¤€: 250 timesteps)
    'sequence_length': 250,

    # Training
    'batch_size': 128,
    'learning_rate': 5e-5,
    'weight_decay': 1e-4,
    'epochs': 200,
    'warmup_steps': 8000,
    'early_stopping_patience': 15,

    # Top-k Loss (Hard Example Mining)
    'use_topk_loss': True,
    'topk_ratio': 0.5,

    # Data Augmentation (ì „ì²˜ë¦¬ ì‹œ ì ìš©ë¨, Train only)
    'augment_train': False,  # ì „ì²˜ë¦¬ì—ì„œ ì´ë¯¸ ì ìš©ë¨
    'mag_noise_std': 0.8,      # (ì‚¬ìš© ì•ˆ í•¨)
    'orient_noise_std': 1.5,   # (ì‚¬ìš© ì•ˆ í•¨)

    # Inference
    'inference_steps': 10,  # Can use 1-2 for real-time
    'topk_samples': 10,  # Top-k sampling: ì´ ìƒ˜í”Œ ìˆ˜
    'topk_k': 5,  # Top-k sampling: ì„ íƒí•  ê°œìˆ˜ (10ê°œ ì¤‘ ìƒìœ„ 5ê°œ)
}

print("\nì„¤ì •:")
for key, value in CONFIG.items():
    print(f"  {key}: {value}")

# ============================================================================
# Dataset with Real-time Augmentation
# ============================================================================
def augment_sensor_data(sensor_data, mag_noise_std=0.8, orient_noise_std=1.5):
    """
    ì„¼ì„œ ë°ì´í„° ì‹¤ì‹œê°„ ì¦ê°• (Train only)

    ì‹œí€€ì…œ ë°ì´í„° íŠ¹ì„±ì„ ê³ ë ¤í•œ ì¦ê°•:
    1. Drift (ì „ì²´ ì‹œí€€ìŠ¤ ë°”ì´ì–´ìŠ¤) - ì‹œê°„ ë¶ˆë³€
    2. Smooth noise (ì‹œê°„ì ìœ¼ë¡œ ì—°ì†ì ì¸ ë…¸ì´ì¦ˆ)

    ì¦ê°• ë°©ë²•:
    1. ì§€ìê¸° ì„¼ì„œ ë…¸ì´ì¦ˆ (MagX, MagY, MagZ)
       - 70% drift: ì „ì²´ ì‹œí€€ìŠ¤ì— ë™ì¼í•œ ë°”ì´ì–´ìŠ¤ (ì„¼ì„œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì˜¤ì°¨)
       - 30% smooth noise: ì‹œê°„ì ìœ¼ë¡œ ì—°ì†ì ì¸ ë…¸ì´ì¦ˆ (ì¸¡ì • ì˜¤ì°¨)

    2. ë°©í–¥ ì„¼ì„œ ë…¸ì´ì¦ˆ (Pitch, Roll, Yaw)
       - 70% drift: ì „ì²´ ì‹œí€€ìŠ¤ì— ë™ì¼í•œ ë°”ì´ì–´ìŠ¤ (ìì„¸ ì¶”ì • ì˜¤ì°¨)
       - 30% smooth noise: ì‹œê°„ì ìœ¼ë¡œ ì—°ì†ì ì¸ ë…¸ì´ì¦ˆ (ê°ì†ë„ ëˆ„ì  ì˜¤ì°¨)

    ì¦ê°• ë¹„ìœ¨:
    - Train: ë§¤ epochë§ˆë‹¤ 100% ìƒ˜í”Œì— ì‹¤ì‹œê°„ ì¦ê°• ì ìš©
    - Val/Test: ì¦ê°• ì—†ìŒ (ì›ë³¸ ë°ì´í„°ë§Œ)

    Args:
        sensor_data: (100, 6) - [MagX, MagY, MagZ, Pitch, Roll, Yaw]
        mag_noise_std: ì§€ìê¸° ë…¸ì´ì¦ˆ í‘œì¤€í¸ì°¨ (Î¼T)
        orient_noise_std: ë°©í–¥ ë…¸ì´ì¦ˆ í‘œì¤€í¸ì°¨ (ë„)

    Returns:
        augmented sensor_data: (100, 6)
    """
    sensor_data = sensor_data.clone()
    T = sensor_data.shape[0]

    # ì§€ìê¸° ì„¼ì„œ ë…¸ì´ì¦ˆ (MagX, MagY, MagZ)
    # 70% drift + 30% smooth noise
    mag_drift = torch.randn(3) * mag_noise_std * 0.7  # (3,)
    mag_smooth = torch.randn(T, 3) * mag_noise_std * 0.3  # (T, 3)
    sensor_data[:, 0:3] += mag_drift + mag_smooth

    # ë°©í–¥ ì„¼ì„œ ë…¸ì´ì¦ˆ (Pitch, Roll, Yaw)
    # 70% drift + 30% smooth noise
    orient_drift = torch.randn(3) * orient_noise_std * 0.7  # (3,)
    orient_smooth = torch.randn(T, 3) * orient_noise_std * 0.3  # (T, 3)
    sensor_data[:, 3:6] += orient_drift + orient_smooth

    return sensor_data


class FlowMatchingDataset(Dataset):
    """
    Flow Matchingìš© ë°ì´í„°ì…‹ (ì‹¤ì‹œê°„ ì¦ê°• ì§€ì›)

    Input: ì„¼ì„œ ì‹œí€€ìŠ¤ (T, 6) - ê¸°ë³¸ T=250
    Target: ë§ˆì§€ë§‰ ìœ„ì¹˜ (2,)
    """
    def __init__(self, states, trajectories, augment=False,
                 mag_noise_std=0.8, orient_noise_std=1.5):
        """
        Args:
            states: (N, 100, 6) - ì„¼ì„œ ë°ì´í„°
            trajectories: (N, 100, 2) - ê° timestepì˜ ìœ„ì¹˜
            augment: Train ì‹œì—ë§Œ True
            mag_noise_std: ì§€ìê¸° ë…¸ì´ì¦ˆ í‘œì¤€í¸ì°¨
            orient_noise_std: ë°©í–¥ ë…¸ì´ì¦ˆ í‘œì¤€í¸ì°¨
        """
        self.states = torch.FloatTensor(states)
        self.positions = torch.FloatTensor(trajectories[:, -1, :])  # ë§ˆì§€ë§‰ ìœ„ì¹˜ë§Œ
        self.augment = augment
        self.mag_noise_std = mag_noise_std
        self.orient_noise_std = orient_noise_std

    def __len__(self):
        return len(self.states)

    def __getitem__(self, idx):
        sensor_data = self.states[idx]  # (100, 6)

        # Train ì‹œì—ë§Œ ì¦ê°• ì ìš©
        if self.augment:
            sensor_data = augment_sensor_data(
                sensor_data,
                self.mag_noise_std,
                self.orient_noise_std
            )

        return {
            'sensor_data': sensor_data,           # (T, 6)
            'position': self.positions[idx],       # (2,)
        }


# ============================================================================
# Utility helpers
# ============================================================================
def load_metadata(data_dir: Path):
    """Load metadata.pkl if available"""
    metadata_path = data_dir / 'metadata.pkl'
    if not metadata_path.exists():
        print("  âš ï¸ metadata.pklì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì •ê·œí™” ì¢Œí‘œë¡œ í‰ê°€í•©ë‹ˆë‹¤.")
        return None

    with open(metadata_path, 'rb') as f:
        metadata = pickle.load(f)
    return metadata


def extract_position_bounds(metadata):
    """
    metadataì—ì„œ x/y ìµœì†Œ/ìµœëŒ€ ë²”ìœ„ë¥¼ ì¶”ì¶œí•œë‹¤.
    ìƒˆë¡œìš´ ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ëŠ” position_boundsë¥¼ ì œê³µí•˜ê³ ,
    ê¸°ì¡´ ìŠ¤í¬ë¦½íŠ¸ëŠ” normalizationì— í•´ë‹¹ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤.
    """
    if metadata is None:
        return None

    if 'position_bounds' in metadata:
        return metadata['position_bounds']

    norm = metadata.get('normalization')
    if norm and all(k in norm for k in ('x_min', 'x_max', 'y_min', 'y_max')):
        return {
            'x_min': norm['x_min'],
            'x_max': norm['x_max'],
            'y_min': norm['y_min'],
            'y_max': norm['y_max'],
        }
    return None


def denormalize_positions_tensor(pos_tensor, bounds):
    """(-1, 1) ì •ê·œí™” ì¢Œí‘œë¥¼ ì‹¤ì œ (x, y)ë¡œ ë˜ëŒë¦°ë‹¤."""
    x = (pos_tensor[..., 0] + 1.0) * 0.5 * (bounds['x_max'] - bounds['x_min']) + bounds['x_min']
    y = (pos_tensor[..., 1] + 1.0) * 0.5 * (bounds['y_max'] - bounds['y_min']) + bounds['y_min']
    return torch.stack([x, y], dim=-1)


def maybe_denormalize(pos_tensor, bounds):
    """boundsê°€ ìˆì„ ë•Œë§Œ denormalize"""
    if bounds is None:
        return pos_tensor
    return denormalize_positions_tensor(pos_tensor, bounds)

# ============================================================================
# Training
# ============================================================================
def train():
    print("\n[1/6] ë°ì´í„° ë¡œë“œ...")

    # ë°ì´í„° ë””ë ‰í† ë¦¬ ìë™ ì„ íƒ (í•©ì„± ë°ì´í„° ìš°ì„ )
    base_dir = Path(__file__).parent
    synth_dir = base_dir / 'processed_data_flow_matching_synth'
    default_dir = base_dir / 'processed_data_flow_matching'

    if synth_dir.exists():
        data_dir = synth_dir
        print("  ğŸ“¦ Using synthetic dataset: processed_data_flow_matching_synth")
    else:
        data_dir = default_dir
        print("  ğŸ“¦ Using default dataset: processed_data_flow_matching")
    metadata = load_metadata(data_dir)
    position_bounds = extract_position_bounds(metadata)
    grid_threshold = metadata.get('grid_size') if metadata else None
    grid_metrics_enabled = position_bounds is not None and grid_threshold is not None
    unit_label = "m" if position_bounds is not None else "normalized units"

    states_train = np.load(data_dir / 'states_train.npy', allow_pickle=True)
    traj_train = np.load(data_dir / 'trajectories_train.npy', allow_pickle=True)

    states_val = np.load(data_dir / 'states_val.npy', allow_pickle=True)
    traj_val = np.load(data_dir / 'trajectories_val.npy', allow_pickle=True)

    print(f"  Train: {states_train.shape}")
    print(f"  Val:   {states_val.shape}")

    print("\n[2/6] ë°ì´í„°ì…‹ ìƒì„±...")
    # Train: ì‹¤ì‹œê°„ ì¦ê°• ON
    train_dataset = FlowMatchingDataset(
        states_train, traj_train,
        augment=CONFIG['augment_train'],
        mag_noise_std=CONFIG['mag_noise_std'],
        orient_noise_std=CONFIG['orient_noise_std']
    )
    # Val: ì¦ê°• OFF (ì›ë³¸ ë°ì´í„°ë§Œ)
    val_dataset = FlowMatchingDataset(
        states_val, traj_val,
        augment=False
    )

    print(f"  âœ… Train: ì‹¤ì‹œê°„ ì¦ê°• {'í™œì„±í™”' if CONFIG['augment_train'] else 'ë¹„í™œì„±í™”'}")
    if CONFIG['augment_train']:
        print(f"     - ì§€ìê¸° ë…¸ì´ì¦ˆ: std={CONFIG['mag_noise_std']}Î¼T")
        print(f"     - ë°©í–¥ ë…¸ì´ì¦ˆ: std={CONFIG['orient_noise_std']}Â°")
        print(f"     - ì¦ê°• ë¹„ìœ¨: ë§¤ epoch 100% ìƒ˜í”Œ")
    print(f"  âœ… Val: ì›ë³¸ ë°ì´í„°ë§Œ ì‚¬ìš©")

    train_loader = DataLoader(
        train_dataset,
        batch_size=CONFIG['batch_size'],
        shuffle=True,
        num_workers=0,
        pin_memory=True if DEVICE.type == 'cuda' else False
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=CONFIG['batch_size'],
        shuffle=False,
        num_workers=0,
        pin_memory=True if DEVICE.type == 'cuda' else False
    )

    print(f"  Train batches: {len(train_loader)}")
    print(f"  Val batches: {len(val_loader)}")

    print("\n[3/6] ëª¨ë¸ ì´ˆê¸°í™”...")
    model = FlowMatchingLocalization(
        sensor_dim=CONFIG['sensor_dim'],
        position_dim=CONFIG['position_dim'],
        d_model=CONFIG['d_model'],
        encoder_layers=CONFIG['encoder_layers'],
        velocity_layers=CONFIG['velocity_layers'],
        n_heads=CONFIG['n_heads'],
        dropout=CONFIG['dropout']
    ).to(DEVICE)

    n_params = sum(p.numel() for p in model.parameters())
    print(f"  ì´ íŒŒë¼ë¯¸í„°: {n_params:,}")

    print("\n[4/6] Optimizer & Scheduler...")
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=CONFIG['learning_rate'],
        weight_decay=CONFIG['weight_decay']
    )

    # Cosine annealing with warmup
    def lr_lambda(step):
        if step < CONFIG['warmup_steps']:
            return step / CONFIG['warmup_steps']
        else:
            progress = (step - CONFIG['warmup_steps']) / (CONFIG['epochs'] * len(train_loader) - CONFIG['warmup_steps'])
            return 0.5 * (1 + math.cos(math.pi * progress))

    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

    print("\n[5/6] í•™ìŠµ ì‹œì‘...")
    print(f"  Epochs: {CONFIG['epochs']}")
    print(f"  Batch size: {CONFIG['batch_size']}")
    print(f"  Early stopping patience: {CONFIG['early_stopping_patience']}")

    best_val_loss = float('inf')
    patience_counter = 0
    best_epoch = 0

    for epoch in range(CONFIG['epochs']):
        # ========== Training ==========
        model.train()
        train_loss = 0.0
        train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{CONFIG['epochs']} [Train]", leave=False)

        for batch in train_pbar:
            sensor_data = batch['sensor_data'].to(DEVICE)
            positions = batch['position'].to(DEVICE)

            # Flow Matching loss (with Top-k)
            loss = compute_flow_matching_loss(
                model, sensor_data, positions,
                use_topk=CONFIG['use_topk_loss'],
                k_ratio=CONFIG['topk_ratio']
            )

            # Backward
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()

            train_loss += loss.item()
            train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})

        train_loss /= len(train_loader)

        # ========== Validation ==========
        model.eval()
        val_loss = 0.0
        val_position_error = 0.0
        val_position_error_topk = 0.0

        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f"Epoch {epoch+1}/{CONFIG['epochs']} [Val]", leave=False):
                sensor_data = batch['sensor_data'].to(DEVICE)
                positions = batch['position'].to(DEVICE)

                # Flow Matching loss
                loss = compute_flow_matching_loss(
                    model, sensor_data, positions,
                    use_topk=CONFIG['use_topk_loss'],
                    k_ratio=CONFIG['topk_ratio']
                )
                val_loss += loss.item()

                # Position error (using sampling)
                pred_positions = model.sample(sensor_data, n_steps=CONFIG['inference_steps'])
                pred_eval = maybe_denormalize(pred_positions, position_bounds)
                target_eval = maybe_denormalize(positions, position_bounds)
                error = torch.norm(pred_eval - target_eval, dim=1).mean()
                val_position_error += error.item()

                # Position error with Top-k sampling
                best_pos, topk_positions, topk_scores = model.sample_topk(
                    sensor_data,
                    n_samples=CONFIG['topk_samples'],
                    k=CONFIG['topk_k'],
                    n_steps=CONFIG['inference_steps']
                )
                # ìµœê³  ì‹ ë¢°ë„ ìœ„ì¹˜ ì˜¤ì°¨
                best_eval = maybe_denormalize(best_pos, position_bounds)
                error_topk = torch.norm(best_eval - target_eval, dim=1).mean()
                val_position_error_topk += error_topk.item()

        val_loss /= len(val_loader)
        val_position_error /= len(val_loader)
        val_position_error_topk /= len(val_loader)

        print(f"Epoch {epoch+1:3d} | "
              f"Train Loss: {train_loss:.4f} | "
              f"Val Loss: {val_loss:.4f} | "
              f"Val Pos Error ({unit_label}): {val_position_error:.4f} | "
              f"Val Pos Error (Top-k, {unit_label}): {val_position_error_topk:.4f}")

        # Save best model & Early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_epoch = epoch + 1
            patience_counter = 0

            model_dir = Path(__file__).parent.parent / 'models'
            model_dir.mkdir(exist_ok=True)

            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'val_loss': val_loss,
                'val_position_error': val_position_error,
                'val_position_error_topk': val_position_error_topk,
                'config': CONFIG,
            }, model_dir / 'flow_matching_best.pt')
            print(f"  âœ… Best model saved! (Val Loss: {val_loss:.4f})")
        else:
            patience_counter += 1
            print(f"  â³ No improvement for {patience_counter} epoch(s)")

            if patience_counter >= CONFIG['early_stopping_patience']:
                print(f"\nâ›” Early stopping triggered! No improvement for {CONFIG['early_stopping_patience']} epochs.")
                print(f"  Best epoch: {best_epoch}")
                print(f"  Best val loss: {best_val_loss:.4f}")
                break

    print("\n[6/6] í•™ìŠµ ì™„ë£Œ!")
    print(f"  ìµœê³  Val Loss: {best_val_loss:.4f}")
    print(f"  ëª¨ë¸ ì €ì¥: models/flow_matching_best.pt")

    # ========== Test Evaluation ==========
    print("\n" + "=" * 70)
    print("ğŸ“Š Test ë°ì´í„° í‰ê°€")
    print("=" * 70)

    # Test ë°ì´í„° ë¡œë“œ
    states_test = np.load(data_dir / 'states_test.npy', allow_pickle=True)
    traj_test = np.load(data_dir / 'trajectories_test.npy', allow_pickle=True)

    test_dataset = FlowMatchingDataset(states_test, traj_test, augment=False)
    test_loader = DataLoader(
        test_dataset,
        batch_size=CONFIG['batch_size'],
        shuffle=False,
        num_workers=0,
        pin_memory=True if DEVICE.type == 'cuda' else False
    )

    print(f"\nTest ìƒ˜í”Œ: {len(test_dataset):,}ê°œ")

    # Best model ë¡œë“œ
    checkpoint = torch.load(model_dir / 'flow_matching_best.pt')
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    # Test í‰ê°€
    test_position_error = 0.0
    test_position_error_topk = 0.0
    test_within_1grid = 0
    test_within_1grid_topk = 0
    total_samples = 0

    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Test Evaluation"):
            sensor_data = batch['sensor_data'].to(DEVICE)
            positions = batch['position'].to(DEVICE)

            # ì¼ë°˜ sampling
            pred_positions = model.sample(sensor_data, n_steps=CONFIG['inference_steps'])
            pred_eval = maybe_denormalize(pred_positions, position_bounds)
            target_eval = maybe_denormalize(positions, position_bounds)
            error = torch.norm(pred_eval - target_eval, dim=1)
            test_position_error += error.sum().item()

            if grid_metrics_enabled:
                test_within_1grid += (error <= grid_threshold).sum().item()

            # Top-k sampling (5ê°œ í›„ë³´ ëª¨ë‘ í‰ê°€)
            best_pos, topk_positions, topk_scores = model.sample_topk(
                sensor_data,
                n_samples=CONFIG['topk_samples'],
                k=CONFIG['topk_k'],
                n_steps=CONFIG['inference_steps']
            )

            # ìµœê³  ì‹ ë¢°ë„ ìœ„ì¹˜ ì˜¤ì°¨
            best_eval = maybe_denormalize(best_pos, position_bounds)
            error_topk = torch.norm(best_eval - target_eval, dim=1)
            test_position_error_topk += error_topk.sum().item()

            if grid_metrics_enabled:
                candidates_eval = maybe_denormalize(topk_positions, position_bounds)
                candidate_errors = torch.norm(candidates_eval - target_eval.unsqueeze(1), dim=2)
                test_within_1grid_topk += (candidate_errors <= grid_threshold).any(dim=1).sum().item()

            total_samples += len(positions)

    test_position_error /= total_samples
    test_position_error_topk /= total_samples
    if grid_metrics_enabled:
        test_acc_1grid = test_within_1grid / total_samples * 100
        test_acc_1grid_topk = test_within_1grid_topk / total_samples * 100

    print(f"\nğŸ“Š Test ê²°ê³¼:")
    print(f"  í‰ê·  ìœ„ì¹˜ ì˜¤ì°¨ (ì¼ë°˜): {test_position_error:.4f} {unit_label}")
    print(f"  í‰ê·  ìœ„ì¹˜ ì˜¤ì°¨ (Top-k ìµœê³  ì‹ ë¢°ë„): {test_position_error_topk:.4f} {unit_label}")

    if grid_metrics_enabled:
        print(f"\n  ğŸ¯ 1 Grid({grid_threshold:.2f}m) ì´ë‚´ ì •í™•ë„:")
        print(f"    ì¼ë°˜ ìƒ˜í”Œë§ (1ê°œ): {test_acc_1grid:.2f}% ({test_within_1grid}/{total_samples})")
        print(f"    Top-5 í›„ë³´ (5ê°œ ì¤‘ í•˜ë‚˜ë¼ë„): {test_acc_1grid_topk:.2f}% ({test_within_1grid_topk}/{total_samples})")
    else:
        print("\n  ğŸ¯ Grid ì •í™•ë„: metadata ì •ë³´ê°€ ì—†ì–´ ìƒëµë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ========== Test Sampling Speed ==========
    print("\n" + "=" * 70)
    print("âš¡ Inference Speed Test")
    print("=" * 70)

    model.eval()
    test_batch = next(iter(val_loader))
    sensor_data = test_batch['sensor_data'][:4].to(DEVICE)

    import time

    for n_steps in [1, 2, 5, 10]:
        start = time.time()
        with torch.no_grad():
            for _ in range(100):
                _ = model.sample(sensor_data, n_steps=n_steps)
        end = time.time()

        avg_time = (end - start) / 100 * 1000  # ms
        print(f"  {n_steps} steps: {avg_time:.2f} ms/batch (4 samples)")

    print("\n" + "=" * 70)
    print("âœ… Flow Matching í•™ìŠµ ì™„ë£Œ!")
    print("=" * 70)
    print(f"""
ğŸ“Š ìµœì¢… ê²°ê³¼:
  Val Loss: {best_val_loss:.4f}
  ëª¨ë¸: models/flow_matching_best.pt

ğŸ”¥ ë…ì°½ì„±:
  âœ… ì§€ìê¸° ê¸°ë°˜ ì¸ë„ì–´ í¬ì§€ì…”ë‹ì— Flow Matching ì²« ì ìš©
  âœ… Top-k Lossë¡œ ì–´ë ¤ìš´ ìƒ˜í”Œì— ì§‘ì¤‘ í•™ìŠµ
  âœ… Top-k Samplingìœ¼ë¡œ ì•ˆì •ì ì¸ ìœ„ì¹˜ ì˜ˆì¸¡
  âœ… 1-2 step inferenceë¡œ ì‹¤ì‹œê°„ ê°€ëŠ¥
  âœ… Conditional generationìœ¼ë¡œ ì„¼ì„œ â†’ ìœ„ì¹˜ ë§¤í•‘

ğŸ¯ ë‹¤ìŒ ë‹¨ê³„:
  1. í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ë¡œ ì„±ëŠ¥ ì¸¡ì •
  2. LSTM/Transformer ë² ì´ìŠ¤ë¼ì¸ê³¼ ë¹„êµ
  3. ë…¼ë¬¸ ì‘ì„± (ì™„ì „íˆ ìƒˆë¡œìš´ ì ‘ê·¼ë²•!)
""")

if __name__ == '__main__':
    train()
