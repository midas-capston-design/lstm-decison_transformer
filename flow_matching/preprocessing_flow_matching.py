#!/usr/bin/env python3
"""
Flow Matchingìš© ë°ì´í„° ì „ì²˜ë¦¬
- ì¦ê°• ì—†ì´ ì›ë³¸ ë°ì´í„°ë§Œ ì²˜ë¦¬
- Train/Val/Test split í›„
- Trainì—ë§Œ ì‹œí€€ì…œ ì¦ê°• ì ìš©
"""
import pandas as pd
import numpy as np
import os
from pathlib import Path
import pickle
from tqdm import tqdm
from collections import defaultdict
from scipy.ndimage import gaussian_filter1d

# ì„¤ì •
WINDOW_SIZE = 100  # ìƒ˜í”Œ
STRIDE = 5         # Sliding window stride
GRID_SIZE = 0.9   # m (ë” í° ê·¸ë¦¬ë“œë¡œ ìœ„ì¹˜ë‹¹ ìƒ˜í”Œ ìˆ˜ ì¦ê°€)
SENSOR_COLS = ['MagX', 'MagY', 'MagZ', 'Pitch', 'Roll', 'Yaw']

# ì¦ê°• ì„¤ì • (Train only)
AUGMENT_RATIO = 0.5  # Train ë°ì´í„°ì˜ 50%ì— ì¦ê°• ì ìš©
MAG_DRIFT_STD = 0.4    # ì§€ìê¸° drift í‘œì¤€í¸ì°¨ (Î¼T)
MAG_NOISE_STD = 0.1    # ì§€ìê¸° smooth noise í‘œì¤€í¸ì°¨ (Î¼T)
ORIENT_DRIFT_STD = 0.8  # ë°©í–¥ drift í‘œì¤€í¸ì°¨ (ë„)
ORIENT_NOISE_STD = 0.2  # ë°©í–¥ smooth noise í‘œì¤€í¸ì°¨ (ë„)

print("="*70)
print("ğŸ”§ Flow Matchingìš© ë°ì´í„° ì „ì²˜ë¦¬")
print("="*70)
print(f"  Window: {WINDOW_SIZE} ìƒ˜í”Œ (2ì´ˆ @ 50Hz)")
print(f"  Stride: {STRIDE} ìƒ˜í”Œ (0.1ì´ˆ)")
print(f"  Grid: {GRID_SIZE}m")
print(f"\n  ì¦ê°• ì„¤ì • (Train only):")
print(f"    ì¦ê°• ë¹„ìœ¨: {AUGMENT_RATIO*100:.0f}%")
print(f"    ì§€ìê¸° drift: std={MAG_DRIFT_STD}Î¼T")
print(f"    ì§€ìê¸° noise: std={MAG_NOISE_STD}Î¼T (smooth)")
print(f"    ë°©í–¥ drift: std={ORIENT_DRIFT_STD}Â°")
print(f"    ë°©í–¥ noise: std={ORIENT_NOISE_STD}Â° (smooth)")

# ============================================================================
# ì¦ê°• í•¨ìˆ˜ (ì‹œí€€ì…œ íŠ¹ì„± ìœ ì§€)
# ============================================================================

def augment_sequence_sequential(seq):
    """
    ì‹œí€€ì…œ íŠ¹ì„±ì„ ìœ ì§€í•˜ëŠ” ì¦ê°•

    1. Sensor Drift (90%): ì „ì²´ ì‹œí€€ìŠ¤ì— ë™ì¼í•œ ë°”ì´ì–´ìŠ¤
       â†’ íŒ¨í„´ ì™„ì „íˆ ìœ ì§€, ì„¼ì„œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì˜¤ì°¨ ëª¨ì‚¬

    2. Smooth Noise (10%): ì‹œê°„ì ìœ¼ë¡œ ì—°ì†ì ì¸ ë…¸ì´ì¦ˆ
       â†’ ì¸¡ì • ë…¸ì´ì¦ˆ ëª¨ì‚¬, Gaussian filterë¡œ smooth

    Args:
        seq: (100, 6) numpy array [MagX, MagY, MagZ, Pitch, Roll, Yaw]

    Returns:
        augmented seq: (100, 6)
    """
    seq_aug = seq.copy()

    # 1. Sensor Drift (ì „ì²´ ì‹œí€€ìŠ¤ì— ë™ì¼)
    drift_mag = np.random.randn(3) * MAG_DRIFT_STD
    drift_orient = np.random.randn(3) * ORIENT_DRIFT_STD
    seq_aug[:, 0:3] += drift_mag
    seq_aug[:, 3:6] += drift_orient

    # 2. Smooth Noise (ì‹œê°„ì ìœ¼ë¡œ ì—°ì†)
    noise_mag = np.random.randn(seq.shape[0], 3) * MAG_NOISE_STD
    noise_orient = np.random.randn(seq.shape[0], 3) * ORIENT_NOISE_STD

    # Gaussian filterë¡œ smoothí•˜ê²Œ
    for i in range(3):
        noise_mag[:, i] = gaussian_filter1d(noise_mag[:, i], sigma=5)
        noise_orient[:, i] = gaussian_filter1d(noise_orient[:, i], sigma=5)

    seq_aug[:, 0:3] += noise_mag
    seq_aug[:, 3:6] += noise_orient

    return seq_aug


# ============================================================================
# 1ë‹¨ê³„: ë…¸ë“œ ì •ë³´ ë¡œë“œ
# ============================================================================
print("\n[1/8] ë…¸ë“œ ì •ë³´ ë¡œë“œ...")
nodes_df = pd.read_csv('nodes_final.csv')
node_positions = {row['id']: (row['x_m'], row['y_m'])
                  for _, row in nodes_df.iterrows()}

# ê±´ë¬¼ ë²”ìœ„ ê³„ì‚°
x_coords = [pos[0] for pos in node_positions.values()]
y_coords = [pos[1] for pos in node_positions.values()]
x_min, x_max = min(x_coords), max(x_coords)
y_min, y_max = min(y_coords), max(y_coords)

print(f"  ê±´ë¬¼ ë²”ìœ„: x=[{x_min}, {x_max}], y=[{y_min}, {y_max}]")

# ============================================================================
# 2ë‹¨ê³„: ê·¸ë¦¬ë“œ ë§¤í•‘ í•¨ìˆ˜
# ============================================================================

def coord_to_grid(x, y):
    """ì ˆëŒ€ ì¢Œí‘œë¥¼ ê·¸ë¦¬ë“œ IDë¡œ ë³€í™˜"""
    grid_x = int(round((x - x_min) / GRID_SIZE))
    grid_y = int(round((y - y_min) / GRID_SIZE))

    num_x_grids = int(np.ceil((x_max - x_min) / GRID_SIZE)) + 1
    num_y_grids = int(np.ceil((y_max - y_min) / GRID_SIZE)) + 1

    grid_x = max(0, min(grid_x, num_x_grids - 1))
    grid_y = max(0, min(grid_y, num_y_grids - 1))

    grid_id = grid_y * num_x_grids + grid_x
    return grid_id


def calculate_marker_coordinates(start_pos, end_pos, num_markers):
    """
    ì‹œì‘ì ì—ì„œ ëì ê¹Œì§€ num_markers ê°œì˜ ì¢Œí‘œ ìƒì„±

    **ì¤‘ìš”**: Xì¶• ë˜ëŠ” Yì¶•ìœ¼ë¡œë§Œ ì´ë™ (ëŒ€ê°ì„  X)
    - dxì™€ dy ì¤‘ í•˜ë‚˜ë§Œ 0ì´ ì•„ë‹ˆì–´ì•¼ í•¨
    - ë‘˜ ë‹¤ ë³€í•˜ë©´ â†’ Xì¶• ë¨¼ì € ì´ë™ í›„ Yì¶• ì´ë™
    """
    coords = []
    dx = end_pos[0] - start_pos[0]
    dy = end_pos[1] - start_pos[1]

    # ì§ì„  ì´ë™ì¸ì§€ í™•ì¸
    is_straight_x = (abs(dy) < 0.01)  # Y ê³ ì •, Xë§Œ ë³€í™”
    is_straight_y = (abs(dx) < 0.01)  # X ê³ ì •, Yë§Œ ë³€í™”

    if is_straight_x or is_straight_y:
        # ì§ì„  ì´ë™: ë‹¨ìˆœ ë³´ê°„
        for i in range(num_markers):
            progress = i / (num_markers - 1) if num_markers > 1 else 0
            x = start_pos[0] + dx * progress
            y = start_pos[1] + dy * progress
            coords.append((x, y))
    else:
        # ëŒ€ê°ì„  ê²½ë¡œ: Xì¶• ë¨¼ì € â†’ Yì¶• ì´ë™
        # ì¤‘ê°„ì  ê³„ì‚°
        mid_pos = (end_pos[0], start_pos[1])

        # Xì¶• ì´ë™ êµ¬ê°„ì˜ ë§ˆì»¤ ìˆ˜ (ê±°ë¦¬ ë¹„ìœ¨ë¡œ ê²°ì •)
        total_dist = abs(dx) + abs(dy)
        x_dist = abs(dx)
        num_x_markers = max(1, int(num_markers * x_dist / total_dist))
        num_y_markers = num_markers - num_x_markers

        # Xì¶• ì´ë™ (start â†’ mid)
        for i in range(num_x_markers):
            progress = i / num_x_markers if num_x_markers > 1 else 0
            x = start_pos[0] + dx * progress
            y = start_pos[1]
            coords.append((x, y))

        # Yì¶• ì´ë™ (mid â†’ end)
        for i in range(num_y_markers):
            progress = i / (num_y_markers - 1) if num_y_markers > 1 else 1
            x = end_pos[0]
            y = mid_pos[1] + dy * progress
            coords.append((x, y))

    return coords


def interpolate_position(pos1, pos2, t1, t2, t_current):
    """ë‘ ë§ˆì»¤ ì‚¬ì´ì—ì„œ í˜„ì¬ ìƒ˜í”Œì˜ ìœ„ì¹˜ë¥¼ ì„ í˜• ë³´ê°„"""
    if t2 == t1:
        return pos1

    progress = (t_current - t1) / (t2 - t1)
    progress = max(0, min(1, progress))

    x = pos1[0] + (pos2[0] - pos1[0]) * progress
    y = pos1[1] + (pos2[1] - pos1[1]) * progress

    return (x, y)


# ============================================================================
# 3ë‹¨ê³„: íŒŒì¼ë³„ ì²˜ë¦¬ (ì¦ê°• ì—†ì´ ì›ë³¸ë§Œ)
# ============================================================================

def process_file_no_augment(filepath):
    """
    í•˜ë‚˜ì˜ ê²½ë¡œ íŒŒì¼ ì²˜ë¦¬ (ì¦ê°• ì—†ìŒ)

    Returns:
        sequences: (N, 100, 6)
        labels: (N,)
        coords: (N, 2)
    """
    filename = os.path.basename(filepath)
    parts = filename.replace('.csv', '').split('_')

    if len(parts) != 3:
        return None

    start_node = int(parts[0])
    end_node = int(parts[1])

    if start_node not in node_positions or end_node not in node_positions:
        return None

    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(filepath)

    # ì„¼ì„œ ì»¬ëŸ¼ í™•ì¸
    if not all(col in df.columns for col in SENSOR_COLS):
        return None

    # Highlighted ë§ˆì»¤ ì¸ë±ìŠ¤
    highlighted_indices = df[df['Highlighted'] == True].index.tolist()
    num_markers = len(highlighted_indices)

    if num_markers < 2:
        return None

    # ë§ˆì»¤ ì¢Œí‘œ ê³„ì‚°
    start_pos = node_positions[start_node]
    end_pos = node_positions[end_node]
    marker_coords = calculate_marker_coordinates(start_pos, end_pos, num_markers)

    sequences = []
    labels = []
    coords_list = []
    trajectories = []  # ì „ì²´ ê¶¤ì  ì €ì¥

    # ì—°ì†ëœ ë§ˆì»¤ ìŒ ìˆœíšŒ
    for i in range(len(highlighted_indices) - 1):
        marker_idx_A = highlighted_indices[i]
        marker_idx_B = highlighted_indices[i + 1]
        coord_A = marker_coords[i]
        coord_B = marker_coords[i + 1]

        # ë§ˆì»¤ Aì™€ B ì‚¬ì´ì˜ ëª¨ë“  ìƒ˜í”Œ ìˆœíšŒ
        for center_idx in range(marker_idx_A, marker_idx_B, STRIDE):
            start_idx = center_idx - WINDOW_SIZE
            end_idx = center_idx

            if start_idx < 0:
                continue
            if end_idx > len(df):
                break

            # ì„¼ì„œ ë°ì´í„° ì¶”ì¶œ
            seq = df.iloc[start_idx:end_idx][SENSOR_COLS].values

            if len(seq) != WINDOW_SIZE:
                continue

            # ê° timestepì˜ ìœ„ì¹˜ ê³„ì‚° (ì„ í˜• ë³´ê°„)
            traj = []
            for t_idx in range(start_idx, end_idx):
                if t_idx <= marker_idx_A:
                    pos = coord_A
                elif t_idx >= marker_idx_B:
                    pos = coord_B
                else:
                    pos = interpolate_position(coord_A, coord_B,
                                              marker_idx_A, marker_idx_B, t_idx)
                traj.append(pos)

            traj = np.array(traj)  # (100, 2)

            # ë§ˆì§€ë§‰ ìœ„ì¹˜
            x, y = traj[-1]
            grid_id = coord_to_grid(x, y)

            # ì›ë³¸ ë°ì´í„°ë§Œ ì¶”ê°€
            sequences.append(seq)
            labels.append(grid_id)
            coords_list.append((x, y))
            trajectories.append(traj)

    if len(sequences) == 0:
        return None

    return {
        'sequences': np.array(sequences),
        'labels': np.array(labels),
        'coords': np.array(coords_list),
        'trajectories': np.array(trajectories),  # (N, 100, 2)
        'route': f"{start_node}â†’{end_node}",
    }


# ============================================================================
# 4ë‹¨ê³„: ì „ì²´ ë°ì´í„°ì…‹ ìƒì„± (ì¦ê°• ì—†ìŒ)
# ============================================================================

print("\n[2/8] ë°ì´í„° íŒŒì¼ ì²˜ë¦¬ (ì¦ê°• ì—†ìŒ)...")

data_dir = Path('law_data')
files = sorted(list(data_dir.glob('*.csv')))

all_data = []
grid_stats = defaultdict(int)

np.random.seed(42)

for filepath in tqdm(files, desc="Processing files"):
    result = process_file_no_augment(filepath)
    if result is not None:
        all_data.append(result)

        for label in result['labels']:
            grid_stats[label] += 1

total_samples = sum(len(d['sequences']) for d in all_data)

print(f"\n  ì²˜ë¦¬ëœ íŒŒì¼: {len(all_data)}/{len(files)}")
print(f"  ê³ ìœ  ê·¸ë¦¬ë“œ ì…€: {len(grid_stats)}")
print(f"  ì´ ìƒ˜í”Œ ìˆ˜ (ì›ë³¸): {total_samples:,}")

# ============================================================================
# 5ë‹¨ê³„: ë°ì´í„° í†µí•© ë° ì •ê·œí™”
# ============================================================================

print("\n[3/8] ë°ì´í„° í†µí•© ë° ì •ê·œí™”...")

all_sequences = np.vstack([d['sequences'] for d in all_data])
all_labels = np.concatenate([d['labels'] for d in all_data])
all_coords = np.vstack([d['coords'] for d in all_data])
all_trajectories = np.vstack([d['trajectories'] for d in all_data])

print(f"  í†µí•© ë°ì´í„° shape: {all_sequences.shape}")
print(f"  ê¶¤ì  shape: {all_trajectories.shape}")

# ì •ê·œí™”
mean = all_sequences.mean(axis=(0, 1))
std = all_sequences.std(axis=(0, 1))

print(f"\n  ì •ê·œí™” íŒŒë¼ë¯¸í„°:")
for i, col in enumerate(SENSOR_COLS):
    print(f"    {col}: mean={mean[i]:.4f}, std={std[i]:.4f}")

all_sequences_norm = (all_sequences - mean) / (std + 1e-8)

# ì¢Œí‘œ ì •ê·œí™” (-1, 1)
coords_min = all_coords.min(axis=0)
coords_max = all_coords.max(axis=0)
coords_range = coords_max - coords_min
all_coords_norm = 2 * (all_coords - coords_min) / (coords_range + 1e-8) - 1

# ê¶¤ì ë„ ë™ì¼í•˜ê²Œ ì •ê·œí™”
all_trajectories_norm = 2 * (all_trajectories - coords_min) / (coords_range + 1e-8) - 1

# ============================================================================
# 6ë‹¨ê³„: Train/Val/Test ë¶„í•  (ê²½ë¡œ ê¸°ì¤€)
# ============================================================================

print("\n[4/8] Train/Val/Test ë¶„í•  (ê²½ë¡œ ê¸°ì¤€)...")

# ê²½ë¡œë³„ ê·¸ë£¹í™”
route_groups = defaultdict(list)
for i, data in enumerate(all_data):
    route_groups[data['route']].append(i)

routes = list(route_groups.keys())
np.random.seed(42)
np.random.shuffle(routes)

n_routes = len(routes)
n_train = int(0.7 * n_routes)
n_val = int(0.15 * n_routes)

train_routes = routes[:n_train]
val_routes = routes[n_train:n_train + n_val]
test_routes = routes[n_train + n_val:]

print(f"  Train ê²½ë¡œ: {len(train_routes)}")
print(f"  Val ê²½ë¡œ: {len(val_routes)}")
print(f"  Test ê²½ë¡œ: {len(test_routes)}")

# ì¸ë±ìŠ¤ ë§¤í•‘
route_to_indices = {}
current_idx = 0
for data in all_data:
    route = data['route']
    n_samples = len(data['sequences'])
    route_to_indices[route] = list(range(current_idx, current_idx + n_samples))
    current_idx += n_samples

# Split
train_indices = []
for route in train_routes:
    train_indices.extend(route_to_indices[route])

val_indices = []
for route in val_routes:
    val_indices.extend(route_to_indices[route])

test_indices = []
for route in test_routes:
    test_indices.extend(route_to_indices[route])

states_train = all_sequences_norm[train_indices]
states_val = all_sequences_norm[val_indices]
states_test = all_sequences_norm[test_indices]

traj_train = all_trajectories_norm[train_indices]
traj_val = all_trajectories_norm[val_indices]
traj_test = all_trajectories_norm[test_indices]

labels_train = all_labels[train_indices]
labels_val = all_labels[val_indices]
labels_test = all_labels[test_indices]

coords_train = all_coords_norm[train_indices]
coords_val = all_coords_norm[val_indices]
coords_test = all_coords_norm[test_indices]

print(f"\n  Train: {len(train_indices):,} ìƒ˜í”Œ")
print(f"  Val:   {len(val_indices):,} ìƒ˜í”Œ")
print(f"  Test:  {len(test_indices):,} ìƒ˜í”Œ")

# ============================================================================
# 7ë‹¨ê³„: Trainì—ë§Œ ì¦ê°• ì ìš©
# ============================================================================

print(f"\n[5/8] Train ë°ì´í„° ì¦ê°• (ì‹œí€€ì…œ ìœ ì§€)...")
print(f"  ì¦ê°• ë¹„ìœ¨: {AUGMENT_RATIO*100:.0f}%")

n_train_samples = len(states_train)
n_augment = int(n_train_samples * AUGMENT_RATIO)

print(f"  ì›ë³¸: {n_train_samples:,}ê°œ")
print(f"  ì¦ê°•: {n_augment:,}ê°œ")
print(f"  ìµœì¢…: {n_train_samples + n_augment:,}ê°œ")

# ì¦ê°•í•  ìƒ˜í”Œ ëœë¤ ì„ íƒ
augment_indices = np.random.choice(n_train_samples, n_augment, replace=False)

augmented_states = []
augmented_traj = []
augmented_labels = []
augmented_coords = []

for idx in tqdm(augment_indices, desc="Augmenting"):
    # ì„¼ì„œ ë°ì´í„° ì¦ê°•
    seq_aug = augment_sequence_sequential(states_train[idx])
    augmented_states.append(seq_aug)

    # ìœ„ì¹˜ëŠ” ê·¸ëŒ€ë¡œ (ì„¼ì„œë§Œ ì¦ê°•)
    augmented_traj.append(traj_train[idx])
    augmented_labels.append(labels_train[idx])
    augmented_coords.append(coords_train[idx])

# Train ë°ì´í„°ì— ì¦ê°• ì¶”ê°€
states_train_final = np.vstack([states_train, np.array(augmented_states)])
traj_train_final = np.vstack([traj_train, np.array(augmented_traj)])
labels_train_final = np.concatenate([labels_train, np.array(augmented_labels)])
coords_train_final = np.vstack([coords_train, np.array(augmented_coords)])

print(f"\n  âœ… Train ìµœì¢…: {len(states_train_final):,}ê°œ (ì¦ê°• í¬í•¨)")
print(f"  âœ… Val: {len(states_val):,}ê°œ (ì›ë³¸ë§Œ)")
print(f"  âœ… Test: {len(states_test):,}ê°œ (ì›ë³¸ë§Œ)")

# ============================================================================
# 8ë‹¨ê³„: ì €ì¥
# ============================================================================

print("\n[6/8] ë°ì´í„° ì €ì¥...")

output_dir = Path(__file__).parent / 'processed_data_flow_matching'
output_dir.mkdir(exist_ok=True)

# Train
np.save(output_dir / 'states_train.npy', states_train_final)
np.save(output_dir / 'trajectories_train.npy', traj_train_final)
np.save(output_dir / 'labels_train.npy', labels_train_final)
np.save(output_dir / 'coords_train.npy', coords_train_final)

# Val
np.save(output_dir / 'states_val.npy', states_val)
np.save(output_dir / 'trajectories_val.npy', traj_val)
np.save(output_dir / 'labels_val.npy', labels_val)
np.save(output_dir / 'coords_val.npy', coords_val)

# Test
np.save(output_dir / 'states_test.npy', states_test)
np.save(output_dir / 'trajectories_test.npy', traj_test)
np.save(output_dir / 'labels_test.npy', labels_test)
np.save(output_dir / 'coords_test.npy', coords_test)

# ë©”íƒ€ ì •ë³´
metadata = {
    'window_size': WINDOW_SIZE,
    'stride': STRIDE,
    'grid_size': GRID_SIZE,
    'sensor_cols': SENSOR_COLS,
    'normalization': {
        'sensor_mean': mean.tolist(),
        'sensor_std': std.tolist(),
        'coords_min': coords_min.tolist(),
        'coords_max': coords_max.tolist(),
    },
    'augmentation': {
        'train_only': True,
        'augment_ratio': AUGMENT_RATIO,
        'mag_drift_std': MAG_DRIFT_STD,
        'mag_noise_std': MAG_NOISE_STD,
        'orient_drift_std': ORIENT_DRIFT_STD,
        'orient_noise_std': ORIENT_NOISE_STD,
    },
    'splits': {
        'train': len(states_train_final),
        'val': len(states_val),
        'test': len(states_test),
    }
}

with open(output_dir / 'metadata.pkl', 'wb') as f:
    pickle.dump(metadata, f)

print(f"  ì €ì¥ ìœ„ì¹˜: {output_dir}")

print("\n" + "="*70)
print("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
print("="*70)
print(f"""
ğŸ“Š ìµœì¢… ë°ì´í„°ì…‹:
  Train: {len(states_train_final):,}ê°œ (ì›ë³¸ {n_train_samples:,} + ì¦ê°• {n_augment:,})
  Val:   {len(states_val):,}ê°œ (ì›ë³¸ë§Œ)
  Test:  {len(states_test):,}ê°œ (ì›ë³¸ë§Œ)

ğŸ”¥ ì¦ê°• ë°©ì‹:
  âœ… Trainì—ë§Œ ì ìš©
  âœ… Sensor Drift (ì „ì²´ ì‹œí€€ìŠ¤ ë™ì¼) - íŒ¨í„´ ìœ ì§€
  âœ… Smooth Noise (ì‹œê°„ì  ì—°ì†) - ì¸¡ì • ì˜¤ì°¨ ëª¨ì‚¬
  âœ… Val/TestëŠ” ì›ë³¸ ë°ì´í„°ë§Œ

ğŸ“ ì¶œë ¥:
  {output_dir}/
    â”œâ”€â”€ states_train.npy
    â”œâ”€â”€ trajectories_train.npy
    â”œâ”€â”€ states_val.npy
    â”œâ”€â”€ trajectories_val.npy
    â”œâ”€â”€ states_test.npy
    â”œâ”€â”€ trajectories_test.npy
    â””â”€â”€ metadata.pkl
""")
